var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"homepage.html","title":"Overview","text":"<p>GenJAX: a probabilistic programming library designed to scale probabilistic modeling and inference into high performance settings. (1)</p> <ol> <li> <p>Here, high performance means massively parallel, either cores or devices.</p> <p>For those whom this overview page may be irrelevant: the value proposition is about putting expressive models and customizable Bayesian inference on GPUs, TPUs, etc - without sacrificing abstraction or modularity.</p> </li> </ol> <p>Gen is a multi-paradigm (generative, differentiable, incremental) system for probabilistic programming. GenJAX is an implementation of Gen on top of JAX (2) - exposing the ability to programmatically construct and manipulate generative functions (1) (computational objects which represent probability measures over structured sample spaces), with compilation to native devices, accelerators, and other parallel fabrics. </p> <ol> <li> <p>By design, generative functions expose a concise interface for expressing approximate and differentiable inference algorithms. </p> <p>The set of generative functions is extensible! You can implement your own - allowing advanced users to performance optimize their critical modeling/inference code paths.</p> <p>You can (and we, at the MIT Probabilistic Computing Project, do!) use these objects for machine learning - including robotics, natural language processing, reasoning about agents, and modelling / creating systems which exhibit human-like reasoning.</p> <p>A precise mathematical formulation of generative functions is given in Marco Cusumano-Towner's PhD thesis.</p> </li> <li> <p>If the usage of JAX is not a dead giveaway, GenJAX is written in Python.</p> </li> </ol> Model codeInference code <p><p> Defining a beta-bernoulli process model as a generative function in GenJAX. </p></p> <pre><code>@genjax.gen\ndef model():\np = beta(0, 1) @ \"p\"\nv = bernoulli(p) @ \"v\"\nreturn v\n</code></pre> <p><p> This works for any generative function, not just the beta-bernoulli model. </p></p> <pre><code>def importance_sampling(\nkey: PRNGKey,\ngen_fn: GenerativeFunction,\nmodel_args: Tuple,\nobs: ChoiceMap,\nn_samples: Int,\n): # (1)!\nkey, sub_keys = genjax.slash(key, n_samples)  # split keys\n_, (lws, trs) = jax.vmap(\ngen_fn.importance, # (2)!\nin_axes=(0, None, None),\n)(sub_keys, obs, args)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (trs, log_normalized_weights, log_ml_estimate)\n</code></pre> <ol> <li> <p>Here's a few notes about the signature:</p> <ul> <li><code>PRNGKey</code> is the type of <code>jax.random.PRNGKey</code>. In GenJAX, we pass keys into generative code, and generative code returns a changed key.</li> <li><code>GenerativeFunction</code> refers to generative functions, objects which expose Gen's probabilistic interface.</li> <li>For now, think of <code>ChoiceMap</code> as the type of object which Gen uses to express conditioning.</li> </ul> </li> <li> <p><code>gen_fn.importance</code> is a generative function interface method. Generative functions are responsible for implementing this method, to support conditional sampling and conditional density estimation. You can learn a lot more about this method in the generative function interface.</p> </li> </ol>"},{"location":"homepage.html#what-sort-of-things-do-you-use-genjax-for","title":"What sort of things do you use GenJAX for?","text":"Real time object tracking <p>Real time tracking of objects in 3D using probabilistic rendering. (Left) Ground truth, (center) depth mask, (right) inference overlaid on ground truth.</p> <p><p> </p></p>"},{"location":"homepage.html#why-gen","title":"Why Gen?","text":"<p>GenJAX is a Gen implementation. If you're considering using GenJAX - it's worth starting by understanding what problems Gen purports to solve.</p>"},{"location":"homepage.html#the-evolution-of-probabilistic-programming-languages","title":"The evolution of probabilistic programming languages","text":"<p>Probabilistic modeling and inference is hard: understanding a domain well enough to construct a probabilistic model in the Bayesian paradigm is challenging, and that's half the battle - the other half is designing effective inference algorithms to probe the implications of the model (1).</p> <ol> <li> <p>Some probabilistic programming languages restrict the set of allowable models, providing (in return) efficient (often, exact) inference. </p> <p>Gen considers a wide class of models - include Bayesian nonparametrics, open-universe models, and models over rich structures (like programs!) - which don't natively support efficient exact inference.</p> </li> </ol> <p>Model writers have historically considered the following design loop.</p> <pre><code>graph LR\n  A[Design model.] --&gt; B[Implement inference by hand.];\n  B --&gt; C[Model + inference okay?];\n  C --&gt; D[Happy.];\n  C --&gt; A;</code></pre> <p>The first generation (1) of probabilistic programming systems introduced inference engines which could operate abstractly over many different models, without requiring the programmer to return and tweak their inference code. The utopia envisioned by these systems is shown below.</p> <ol> <li> <p>Here, the definition of \"first generation\" includes systems like JAGS, BUGS, BLOG, IBAL, Church, Infer.NET, Figaro, Stan, amongst others.</p> <p>But more precisely, many systems preceded the DARPA PPAML project - which gave rise to several novel systems, including the predecessors of Gen.</p> </li> </ol> <pre><code>graph LR\n  A[Design model.] --&gt; D[Model + inference okay?];\n  B[Inference engine.] ---&gt; D;\n  D --&gt; E[Happy.];\n  D ---&gt; A;</code></pre> <p>The problem with this utopia is that we often need to customize our inference algorithms (1) to achieve maximum performance, with respect to accuracy as well as runtime (2). First generation systems were not designed with this in mind.</p> <ol> <li>Here, programmable inference denotes using a custom proposal distribution in importance sampling, or a custom variational family for variational inference, or even a custom kernel in Markov chain Monte Carlo.</li> <li>Composition of inference programs can also be highly desirable when performing inference in complex models, or designing a probabilistic application from several modeling and inference components. The first examples of universal inference engines ignored this design problem.</li> </ol>"},{"location":"homepage.html#programmable-inference","title":"Programmable inference","text":"<p>A worthy design goal is to allow users to customize when required, while retaining the rapid model/inference iteration properties explored by first generation systems.</p> <p>Gen addresses this goal by introducing a separation between modeling and inference code: the generative function interface.</p> <p> </p> <p>The interface provides an abstraction layer that inference algorithms can call to compute the necessary (and hard to get right!) math (1). Probabilistic application developers can also extend the interface to new modeling languages - and immediately gain access to advanced inference procedures.</p> <ol> <li> <p>Examples of hard-to-get-right math: importance weights, accept reject ratios, and gradient estimators. </p> <p>For simple models and inference, one might painlessly derive these quantities. As soon as the model/inference gets complicated, however, you might find yourself thanking the interface.</p> </li> </ol>"},{"location":"homepage.html#whose-using-gen","title":"Whose using Gen?","text":"<p>Gen supports a growing list of users, with collaboration across academic research labs and industry affiliates.</p> <p> </p> <p>We're looking to expand our user base! If you're interested, please contact us to get involved.</p>"},{"location":"genjax/diff_jl.html","title":"Comparisons with Gen.jl","text":"<p>GenJAX implements concepts from Gen, and implements several inference algorithms with reference to implementations from <code>Gen.jl</code>. In general, you should find that the programming patterns and interface idioms should match closely with <code>Gen.jl</code>.</p> <p>However, there are a few necessary design deviations between <code>genjax</code> and <code>Gen.jl</code> that stem from restrictions arising from JAX's compilation model. In this section, we describe several of these differences and try to highlight workarounds or discuss the reason for the discrepancy.</p>"},{"location":"genjax/diff_jl.html#turing-universality","title":"Turing universality","text":"<p><code>Gen.jl</code> is Turing universal - it can encode any computable distribution, including those expressed by forms of unbounded recursion.</p> <p>Arguing from a practical perspective, <code>genjax</code> also falls into this category, but several things are harder to encode for GPUs(1). Additionally, JAX does not feature mechanisms for dynamic shape allocations, but it does feature mechanisms for unbounded recursion.</p> <ol> <li>We expect that GPU/TPU deployment to be the dominant usage pattern for <code>genjax</code> - and defer optimized CPU deployment to other implementations of Gen.</li> </ol> <p>Lack of dynamic allocations provides a technical barrier to implementing Gen's trace machinery for generative functions which feature recursive calls to other generative functions. While JAX allows for unbounded recursion, to generally support recording trace data - we also need the ability to dynamically allocate choice data. This requirement is currently at tension with XLA's requirements of knowing the static shape of everything. Nonetheless, one might imagine pre-allocating large arrays - passing them into <code>jax.lax.while_op</code> implementations of certain types of recursion, etc. Painful and impractical - yes, but theoretical possible.</p> <p><code>genjax</code> supports generative function combinators with bounded recursion / unfold chain length. Ahead of time, these combinators can be directed to pre-allocate arrays with enough size to handle recursion/looping within the bounds that the programmer sets. If these bounds are exceeded, a Python runtime error will be thrown (both on and off JAX device).</p> <p>In practice, this means that some performance engineering (space vs. expressivity) is required of the programmer. It's certainly feasible to express bounded recursive computations which terminate with probability 1 - but you'll need to ahead of time allocate space for it.</p>"},{"location":"genjax/diff_jl.html#mutation","title":"Mutation","text":"<p>Just like JAX, GenJAX disallows mutation - expressing a mutating operation on an array must be done through special JAX interfaces. Outside of JIT compilation, those interfaces often fully copy array data. Inside of JIT compilation, there are special circumstances where these operations will be performed in place.</p>"},{"location":"genjax/diff_jl.html#to-jit-or-not-to-jit","title":"To JIT or not to JIT","text":"<p><code>Gen.jl</code> is written in Julia, which automatically JITs everything. <code>genjax</code>, by virtue of being constructed on top of JAX, allows us to JIT JAX compatible code - but the JIT process is user directed. Thus, the idioms that are used to express and optimize inference code are necessarily different compared to <code>Gen.jl</code>. In the inference standard library, you'll typically find algorithms implemented as dataclasses which inherit (and implement) the <code>jax.Pytree</code> interfaces. Implementing these interfaces allow usage of inference dataclasses and methods in jittable code - and, as a bonus, allow us to be specific about trace vs. runtime known values.</p> <p>In general, it's productive to enclose as much of a computation as possible in a <code>jax.jit</code> block. This can sometimes lead to long trace times. If trace times are ballooning, a common source is explicit for-loops (with known bounds, else JAX will complain). In these cases, you might look at Advice on speeding up compilation time. We've taken care to optimize (by e.g. using XLA primitives) the code which we expose from GenJAX - but if you find something out of the ordinary, file an issue!</p>"},{"location":"genjax/language_aperitifs.html","title":"Language ap\u00e9ritifs","text":"<p>This page assumes that the reader has familiarity with trace-based probabilistic programming systems.</p> <p>The implementation of GenJAX adhers to commonly accepted JAX idioms (1) and modern functional programming patterns (2).</p> <ol> <li>One example: everything is a Pytree. Implies another: everything is JAX traceable by default.</li> <li>Modern here meaning patterns concerning the composition of effectful computations via effect handling abstractions.</li> </ol> <p>GenJAX consists of a set of languages based around transforming pure functions to apply semantic transformations. On this page, we'll provide a taste of some of these languages.</p>"},{"location":"genjax/language_aperitifs.html#the-builtin-language","title":"The builtin language","text":"<p>GenJAX provides a builtin language which supports a <code>trace</code> primitive and the ability to invoke other generative functions as callees:</p> <pre><code>@genjax.gen\ndef submodel():\nx = trace(\"x\", normal)(0.0, 1.0) # explicit\nreturn x\n@genjax.gen\ndef model():\nx = submodel() @ \"sub\" # sugared\nreturn x\n</code></pre> <p>The <code>trace</code> call is a JAX primitive which is given semantics by transformations which implement the semantics of inference interfaces described in Generative functions.</p> <p>Addresses (here, <code>\"x\"</code> and <code>\"sub\"</code>) are important - addressed random choices within <code>trace</code> allow us to structure the address hierarchy for the measure over choice maps which generative functions in this language define.</p> <p>Because convenient idioms for working with addresses is so important in Gen, the generative functions from the builtin language also support a form of \"splatting\" addresses into a caller.</p> <pre><code>@genjax.gen\ndef model():\nx = submodel.inline()\nreturn x\n</code></pre> <p>Invoking the <code>submodel</code> via the <code>inline</code> interface here means that the addresses in <code>submodel</code> are flattened into the address level for the <code>model</code>. If there's overlap, that's a problem! But GenJAX will yell at you for that.</p>"},{"location":"genjax/language_aperitifs.html#structured-control-flow-with-combinators","title":"Structured control flow with combinators","text":"<p>The base modeling language is the <code>BuiltinGenerativeFunction</code> language shown above. The builtin language is based on pure functions, with the interface semantics implemented using program transformations. But we'd also like to take advantage of structured control flow in our generative computations. </p> <p>Users gain access to structured control flow via combinators, other generative function mini-languages which implement the interfaces in control flow compatible ways.</p> <pre><code>@functools.partial(genjax.Map, in_axes=(0, 0))\n@genjax.gen\ndef kernel(x, y):\nz = normal(x + y, 1.0) @ \"z\"\nreturn z\n</code></pre> <p>This defines a <code>MapCombinator</code> generative function - a generative function whose interfaces take care of applying <code>vmap</code> in the appropriate ways (1).</p> <ol> <li>Read: compatible with JIT, gradients, and incremental computation.</li> </ol> <p><code>MapCombinator</code> has a vectorial friend named <code>UnfoldCombinator</code> which implements a <code>scan</code>-like pattern of generative computation.</p> <pre><code>@functools.partial(genjax.Unfold, max_length = 10)\n@genjax.gen\ndef scanner(prev, static_args):\nsigma, = static_args\nnew = normal(prev, sigma) @ \"z\"\nreturn new\n</code></pre> <p><code>UnfoldCombinator</code> allows the expression of general state space models - modeled as a generative function which supports a dependent-for (1) control flow pattern.</p> <ol> <li>Dependent-for means that each iteration may depend on the output from the previous iteration. Think of <code>jax.lax.scan</code> here.</li> </ol> <p><code>UnfoldCombinator</code> allows uncertainty over the length of the chain:</p> <pre><code>@genjax.gen\ndef top_model(p):\nlength = truncated_geometric(10, p) @ \"l\"\ninitial_state = normal(0.0, 1.0) @ \"init\"\nsigma = normal(0.0, 1.0) @ \"sigma\"\n(v, xs) = scanner(length, initial_state, sigma)\nreturn v\n</code></pre> <p>Here, <code>length</code> is drawn from a truncated geometric distribution, and determines the index range of the chain which participates in the generative computation.</p> <p>Of course, combinators are composable.</p> <pre><code>@functools.partial(genjax.Map, in_axes = (0, ))\n@genjax.gen\ndef top_model(p):\nlength = truncated_geometric(10, p) @ \"l\"\ninitial_state = normal(0.0, 1.0) @ \"init\"\nsigma = normal(0.0, 1.0) @ \"sigma\"\n(v, xs) = scanner(length, initial_state, sigma)\nreturn v\n</code></pre> <p>Now we're describing a broadcastable generative function whose internal choices include a chain-like generative structure with dynamic truncation using padding. And we could go on!</p>"},{"location":"genjax/notebooks.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"genjax/concepts/generative_functions.html","title":"Generative functions","text":"<p>Gen is all about generative functions: computational objects which support an interface that helps automate the tricky math involved in programming Bayesian inference algorithms. In this section, we'll unpack the generative function interface and explain the mathematics behind generative functions (1).</p> <ol> <li>For a deeper dive, enjoy Marco Cusumano-Towner's PhD thesis.</li> </ol>"},{"location":"genjax/library/index.html","title":"Library reference","text":"<p>This is the API documentation for modules and symbols which are exposed publicly from <code>genjax</code>. </p> <p>The <code>genjax</code> package consists of several modules, many of which rely on functionality from the <code>genjax.core</code> module, and build upon datatypes, transforms, and generative datatypes which are documented there. Generative function languages use the core datatypes and transforms to implement the generative function interface. Inference and learning algorithms are then implemented using the interface.</p> <ul> <li>The core documentation discusses key datatypes and transformations, which are used throughout the codebase.</li> <li>The documentation on generative function languages describes the functionality and usage for several generative function implementations, including distributions, a function-like language with primitives that allow callee generative functions, and combinator languages which provide structured patterns of control flow.</li> <li>The inference documentation provides information on the standard inference library algorithms.</li> <li>The differentiable programming documentation describes GenJAX's approach to stateful computation and learning.</li> </ul>"},{"location":"genjax/library/core/index.html","title":"Core","text":"<p>This module provides the core functionality and JAX compatibility layer which <code>GenJAX</code> generative function and inference modules are built on top of. It contains (truncated, and in no particular order):</p> <ul> <li> <p>Core data types for the associated data types of generative functions.</p> </li> <li> <p>Utility abstract data types (mixins) for automatically registering class definitions as valid <code>Pytree</code> method implementors (guaranteeing <code>flatten</code>/<code>unflatten</code> compatibility across JAX transform boundaries). For more information, see Pytrees.</p> </li> <li> <p>Transformation interpreters: interpreter-based transformations which operate on <code>ClosedJaxpr</code> instances, as well as staging functionality for staging out computations to <code>ClosedJaxpr</code> instances. The core interpreters are written in a mixed initial / final style. The application of all interpreters are JAX compatible, meaning that the application of any interpreter can be staged out to eliminate the interpreter overhead.</p> </li> </ul>"},{"location":"genjax/library/core/datatypes.html","title":"Core datatypes","text":"<p>GenJAX features a set of core abstract datatypes which build on JAX's <code>Pytree</code> interface. These datatypes are used as an abstract base mixin (especially <code>Pytree</code>) for basically all of the dataclasses in GenJAX.</p>"},{"location":"genjax/library/core/datatypes.html#pytree","title":"Pytree","text":""},{"location":"genjax/library/core/datatypes.html#genjax.core.Pytree","title":"<code>genjax.core.Pytree</code>","text":"<p>Abstract base class which registers a class with JAX's <code>Pytree</code> system.</p> <p>Users who mixin this ABC for class definitions are required to implement <code>flatten</code> below. In turn, instances of the class gain access to a large set of utility functions for working with <code>Pytree</code> data, as well as the ability to use <code>jax.tree_util</code> Pytree functionality.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>class Pytree(metaclass=abc.ABCMeta):\n\"\"\"\n    &gt; Abstract base class which registers a class with JAX's `Pytree` system.\n    Users who mixin this ABC for class definitions are required to implement `flatten` below. In turn, instances of the class gain access to a large set of utility functions for working with `Pytree` data, as well as the ability to use `jax.tree_util` Pytree functionality.\n    \"\"\"\ndef __init_subclass__(cls, **kwargs):\nsuper().__init_subclass__(**kwargs)\njtu.register_pytree_node(\ncls,\ncls.flatten,\ncls.unflatten,\n)\n@abc.abstractmethod\ndef flatten(self) -&gt; Tuple[Tuple, Tuple]:\n\"\"\"\n        `flatten` must be implemented when a user mixes `Pytree` into the declaration of a new class or dataclass.\n        The implementation of `flatten` assumes the following contract:\n        * must return a 2-tuple of tuples.\n        * the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.\n        * the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for `Pytree` equality comparison.\n        For more information, consider [JAX's documentation on Pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n        Returns:\n            dynamic: Dynamic data which supports JAX tracer values.\n            static: Static data which is JAX trace time constant.\n        Examples:\n            Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the `Pytree` mixin.\n            ```python\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            ```\n            !!! info \"Ordering fields in `Pytree` declarations\"\n                Note that the ordering in the dataclass declaration **does matter** - you should put static fields first. The automatically defined `unflatten` method (c.f. below) assumes this ordering.\n            Now, given the declaration, you can use `jax.tree_util` flattening/unflatten functionality.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import genjax\n            import jax.tree_util as jtu\n            from genjax.core import Pytree\n            from dataclasses import dataclass\n            console = genjax.pretty()\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            f = MyFoo(0, 1.0)\n            leaves, form = jtu.tree_flatten(f)\n            print(console.render(leaves))\n            new = jtu.tree_unflatten(form, leaves)\n            print(console.render(new))\n            ```\n        \"\"\"\n@classmethod\ndef unflatten(cls, data, xs):\n\"\"\"\n        Given an implementation of `flatten` (c.f. above), `unflatten` is automatically defined and registered with JAX's `Pytree` system.\n        `unflatten` allows usage of `jtu.tree_unflatten` to create instances of a declared class that mixes `Pytree` from a `PyTreeDef` for that class and leaf data.\n        Examples:\n            Our example from `flatten` above also applies here - where we use `jtu.tree_unflatten` to create a new instance of `MyFoo` from a `PyTreeDef` and leaf data.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import genjax\n            import jax.tree_util as jtu\n            from genjax.core import Pytree\n            from dataclasses import dataclass\n            console = genjax.pretty()\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            f = MyFoo(0, 1.0)\n            leaves, form = jtu.tree_flatten(f)\n            new = jtu.tree_unflatten(form, leaves)\n            print(console.render(new))\n            ```\n        \"\"\"\nreturn cls(*data, *xs)\n@classmethod\ndef new(cls, *args, **kwargs):\nreturn cls(*args, **kwargs)\n# This exposes slicing the struct-of-array representation,\n# taking leaves and indexing/randing into them on the first index,\n# returning a value with the same `Pytree` structure.\ndef slice(self, index_or_index_array):\n\"\"\"\n        &gt; Utility available to any class which mixes `Pytree` base. This method supports indexing/slicing on indices when leaves are arrays.\n        `obj.slice(index)` will take an instance whose class extends `Pytree`, and return an instance of the same class type, but with leaves indexed into at `index`.\n        Arguments:\n            index_or_index_array: An `Int` index or an array of indices which will be used to index into the leaf arrays of the `Pytree` instance.\n        Returns:\n            new_instance: A `Pytree` instance of the same type, whose leaf values are the results of indexing into the leaf arrays with `index_or_index_array`.\n        \"\"\"\nreturn jtu.tree_map(lambda v: v[index_or_index_array], self)\ndef stack(self, *trees):\nreturn tree_stack([self, *trees])\ndef unstack(self):\nreturn tree_unstack(self)\n# Lift multiple trees into a sum type.\ndef sum(self, *trees):\nreturn Sumtree.new(self, trees)\n# Defines default pretty printing.\ndef __rich_console__(self, console, options):\ntree = gpp.tree_pformat(self)\nyield tree\ndef __rich_repr__(self):\nyield self\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.flatten","title":"<code>flatten()</code>  <code>abstractmethod</code>","text":"<p><code>flatten</code> must be implemented when a user mixes <code>Pytree</code> into the declaration of a new class or dataclass.</p> <p>The implementation of <code>flatten</code> assumes the following contract:</p> <ul> <li>must return a 2-tuple of tuples.</li> <li>the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.</li> <li>the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for <code>Pytree</code> equality comparison.</li> </ul> <p>For more information, consider JAX's documentation on Pytrees.</p> <p>Returns:</p> Name Type Description <code>dynamic</code> <code>Tuple</code> <p>Dynamic data which supports JAX tracer values.</p> <code>static</code> <code>Tuple</code> <p>Static data which is JAX trace time constant.</p> <p>Examples:</p> <p>Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the <code>Pytree</code> mixin.</p> <pre><code>@dataclass\nclass MyFoo(Pytree):\nstatic_field: Any\ndynamic_field: Any\n# Implementing `flatten`\ndef flatten(self):\nreturn (self.dynamic_field, ), (self.static_field, )\n</code></pre> <p>Ordering fields in <code>Pytree</code> declarations</p> <p>Note that the ordering in the dataclass declaration does matter - you should put static fields first. The automatically defined <code>unflatten</code> method (c.f. below) assumes this ordering.</p> <p>Now, given the declaration, you can use <code>jax.tree_util</code> flattening/unflatten functionality.</p> SourceResult <pre><code>import genjax\nimport jax.tree_util as jtu\nfrom genjax.core import Pytree\nfrom dataclasses import dataclass\nconsole = genjax.pretty()\n@dataclass\nclass MyFoo(Pytree):\nstatic_field: Any\ndynamic_field: Any\n# Implementing `flatten`\ndef flatten(self):\nreturn (self.dynamic_field, ), (self.static_field, )\nf = MyFoo(0, 1.0)\nleaves, form = jtu.tree_flatten(f)\nprint(console.render(leaves))\nnew = jtu.tree_unflatten(form, leaves)\nprint(console.render(new))\n</code></pre> <p><pre><code>[1.0]\n</code></pre> <pre><code>MyFoo\n\u251c\u2500\u2500 static_field\n\u2502   \u2514\u2500\u2500 (const) 0\n\u2514\u2500\u2500 dynamic_field\n    \u2514\u2500\u2500 (const) 1.0\n</code></pre></p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@abc.abstractmethod\ndef flatten(self) -&gt; Tuple[Tuple, Tuple]:\n\"\"\"\n    `flatten` must be implemented when a user mixes `Pytree` into the declaration of a new class or dataclass.\n    The implementation of `flatten` assumes the following contract:\n    * must return a 2-tuple of tuples.\n    * the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.\n    * the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for `Pytree` equality comparison.\n    For more information, consider [JAX's documentation on Pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n    Returns:\n        dynamic: Dynamic data which supports JAX tracer values.\n        static: Static data which is JAX trace time constant.\n    Examples:\n        Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the `Pytree` mixin.\n        ```python\n        @dataclass\n        class MyFoo(Pytree):\n            static_field: Any\n            dynamic_field: Any\n            # Implementing `flatten`\n            def flatten(self):\n                return (self.dynamic_field, ), (self.static_field, )\n        ```\n        !!! info \"Ordering fields in `Pytree` declarations\"\n            Note that the ordering in the dataclass declaration **does matter** - you should put static fields first. The automatically defined `unflatten` method (c.f. below) assumes this ordering.\n        Now, given the declaration, you can use `jax.tree_util` flattening/unflatten functionality.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import genjax\n        import jax.tree_util as jtu\n        from genjax.core import Pytree\n        from dataclasses import dataclass\n        console = genjax.pretty()\n        @dataclass\n        class MyFoo(Pytree):\n            static_field: Any\n            dynamic_field: Any\n            # Implementing `flatten`\n            def flatten(self):\n                return (self.dynamic_field, ), (self.static_field, )\n        f = MyFoo(0, 1.0)\n        leaves, form = jtu.tree_flatten(f)\n        print(console.render(leaves))\n        new = jtu.tree_unflatten(form, leaves)\n        print(console.render(new))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.unflatten","title":"<code>unflatten(data, xs)</code>  <code>classmethod</code>","text":"<p>Given an implementation of <code>flatten</code> (c.f. above), <code>unflatten</code> is automatically defined and registered with JAX's <code>Pytree</code> system.</p> <p><code>unflatten</code> allows usage of <code>jtu.tree_unflatten</code> to create instances of a declared class that mixes <code>Pytree</code> from a <code>PyTreeDef</code> for that class and leaf data.</p> <p>Examples:</p> <p>Our example from <code>flatten</code> above also applies here - where we use <code>jtu.tree_unflatten</code> to create a new instance of <code>MyFoo</code> from a <code>PyTreeDef</code> and leaf data.</p> SourceResult <pre><code>import genjax\nimport jax.tree_util as jtu\nfrom genjax.core import Pytree\nfrom dataclasses import dataclass\nconsole = genjax.pretty()\n@dataclass\nclass MyFoo(Pytree):\nstatic_field: Any\ndynamic_field: Any\n# Implementing `flatten`\ndef flatten(self):\nreturn (self.dynamic_field, ), (self.static_field, )\nf = MyFoo(0, 1.0)\nleaves, form = jtu.tree_flatten(f)\nnew = jtu.tree_unflatten(form, leaves)\nprint(console.render(new))\n</code></pre> <pre><code>MyFoo\n\u251c\u2500\u2500 static_field\n\u2502   \u2514\u2500\u2500 (const) 0\n\u2514\u2500\u2500 dynamic_field\n    \u2514\u2500\u2500 (const) 1.0\n</code></pre> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@classmethod\ndef unflatten(cls, data, xs):\n\"\"\"\n    Given an implementation of `flatten` (c.f. above), `unflatten` is automatically defined and registered with JAX's `Pytree` system.\n    `unflatten` allows usage of `jtu.tree_unflatten` to create instances of a declared class that mixes `Pytree` from a `PyTreeDef` for that class and leaf data.\n    Examples:\n        Our example from `flatten` above also applies here - where we use `jtu.tree_unflatten` to create a new instance of `MyFoo` from a `PyTreeDef` and leaf data.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import genjax\n        import jax.tree_util as jtu\n        from genjax.core import Pytree\n        from dataclasses import dataclass\n        console = genjax.pretty()\n        @dataclass\n        class MyFoo(Pytree):\n            static_field: Any\n            dynamic_field: Any\n            # Implementing `flatten`\n            def flatten(self):\n                return (self.dynamic_field, ), (self.static_field, )\n        f = MyFoo(0, 1.0)\n        leaves, form = jtu.tree_flatten(f)\n        new = jtu.tree_unflatten(form, leaves)\n        print(console.render(new))\n        ```\n    \"\"\"\nreturn cls(*data, *xs)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.slice","title":"<code>slice(index_or_index_array)</code>","text":"<p>Utility available to any class which mixes <code>Pytree</code> base. This method supports indexing/slicing on indices when leaves are arrays.</p> <p><code>obj.slice(index)</code> will take an instance whose class extends <code>Pytree</code>, and return an instance of the same class type, but with leaves indexed into at <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index_or_index_array</code> <p>An <code>Int</code> index or an array of indices which will be used to index into the leaf arrays of the <code>Pytree</code> instance.</p> required <p>Returns:</p> Name Type Description <code>new_instance</code> <p>A <code>Pytree</code> instance of the same type, whose leaf values are the results of indexing into the leaf arrays with <code>index_or_index_array</code>.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def slice(self, index_or_index_array):\n\"\"\"\n    &gt; Utility available to any class which mixes `Pytree` base. This method supports indexing/slicing on indices when leaves are arrays.\n    `obj.slice(index)` will take an instance whose class extends `Pytree`, and return an instance of the same class type, but with leaves indexed into at `index`.\n    Arguments:\n        index_or_index_array: An `Int` index or an array of indices which will be used to index into the leaf arrays of the `Pytree` instance.\n    Returns:\n        new_instance: A `Pytree` instance of the same type, whose leaf values are the results of indexing into the leaf arrays with `index_or_index_array`.\n    \"\"\"\nreturn jtu.tree_map(lambda v: v[index_or_index_array], self)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.stack","title":"<code>stack(*trees)</code>","text":"Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def stack(self, *trees):\nreturn tree_stack([self, *trees])\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.unstack","title":"<code>unstack()</code>","text":"Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def unstack(self):\nreturn tree_unstack(self)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#abstract-base-classes-which-extend-pytree","title":"Abstract base classes which extend <code>Pytree</code>","text":""},{"location":"genjax/library/core/datatypes.html#tree","title":"Tree","text":"<p>The <code>Tree</code> class is used to define abstract classes for tree-shaped datatypes. These classes are used to implement trace, choice map, and selection types. <code>Tree</code> mixes in <code>Pytree</code> automatically.</p> <p>One should think of <code>Tree</code> as providing a convenient base for many of the generative datatypes.</p>"},{"location":"genjax/library/core/datatypes.html#genjax.core.Tree","title":"<code>genjax.core.Tree</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@dataclass\nclass Tree(Pytree):\n@abc.abstractmethod\ndef has_subtree(self, addr) -&gt; bool:\npass\n@abc.abstractmethod\ndef get_subtree(self, addr):\npass\n@abc.abstractmethod\ndef get_subtrees_shallow(self):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Tree.has_subtree","title":"<code>has_subtree(addr)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef has_subtree(self, addr) -&gt; bool:\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Tree.get_subtree","title":"<code>get_subtree(addr)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef get_subtree(self, addr):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Tree.get_subtrees_shallow","title":"<code>get_subtrees_shallow()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef get_subtrees_shallow(self):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#leaf","title":"Leaf","text":"<p>A <code>Leaf</code> is a <code>Tree</code> without any internal subtrees. <code>Leaf</code> is a convenient base for generative datatypes which don't keep reference to other <code>Tree</code> instances - things like <code>ValueChoiceMap</code> (whose only choice value is a single value, not a dictionary or other tree-like object).</p> <p><code>Leaf</code> extends <code>Tree</code> with a special extension method <code>get_leaf_value</code>.</p>"},{"location":"genjax/library/core/datatypes.html#genjax.core.Leaf","title":"<code>genjax.core.Leaf</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Tree</code></p> Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@dataclass\nclass Leaf(Tree):\n@abc.abstractmethod\ndef get_leaf_value(self):\npass\n@abc.abstractmethod\ndef set_leaf_value(self, v):\npass\ndef has_subtree(self, addr):\nreturn False\ndef get_subtree(self, addr):\nraise Exception(\nf\"{type(self)} is a Leaf: it does not address any internal choices.\"\n)\ndef get_subtrees_shallow(self):\nraise Exception(f\"{type(self)} is a Leaf: it does not have any subtrees.\")\ndef merge(self, other):\nraise Exception(f\"{type(self)} is a Leaf: can't merge.\")\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.get_leaf_value","title":"<code>get_leaf_value()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef get_leaf_value(self):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.has_subtree","title":"<code>has_subtree(addr)</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>def has_subtree(self, addr):\nreturn False\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.get_subtree","title":"<code>get_subtree(addr)</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>def get_subtree(self, addr):\nraise Exception(\nf\"{type(self)} is a Leaf: it does not address any internal choices.\"\n)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.get_subtrees_shallow","title":"<code>get_subtrees_shallow()</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>def get_subtrees_shallow(self):\nraise Exception(f\"{type(self)} is a Leaf: it does not have any subtrees.\")\n</code></pre>"},{"location":"genjax/library/core/generative.html","title":"Generative datatypes","text":"<p>Key generative datatypes in Gen</p> <p>This documentation page contains the type and interface documentation for the primary generative datatypes used in Gen. The documentation on this page deals with the abstract base classes for these datatypes. </p> <p>Any concrete implementor of these abstract classes should be documented with the language which implements it.</p>"},{"location":"genjax/library/core/generative.html#generative-functions","title":"Generative functions","text":"<p>The main computational objects in Gen are generative functions. These objects support an abstract interface of methods and associated types. The interface is designed to allow inference layers to abstract over implementations.</p> <p>Below, we document the abstract base class, and illustrate example usage using concrete implementors. Full descriptions of concrete generative function languages are described in their own documentation module.</p>"},{"location":"genjax/library/core/generative.html#genjax.core.GenerativeFunction","title":"<code>genjax.core.GenerativeFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Abstract base class for generative functions.</p> <p>Interaction with JAX</p> <p>Concrete implementations of <code>GenerativeFunction</code> will likely interact with the JAX tracing machinery if used with the languages exposed by <code>genjax</code>. Hence, there are specific implementation requirements which are more stringent than the requirements enforced in other Gen implementations (e.g. Gen in Julia).</p> <ul> <li>For broad compatibility, the implementation of the interfaces should be compatible with JAX tracing.</li> <li>If a user wishes to implement a generative function which is not compatible with JAX tracing, that generative function may invoke other JAX compat generative functions, but likely cannot be invoked inside of JAX compat generative functions.</li> </ul> <p>Aside from JAX compatibility, an implementor should match the interface signatures documented below. This is not statically checked - but failure to do so will lead to unintended behavior or errors.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass GenerativeFunction(Pytree):\n\"\"\"\n    &gt; Abstract base class for generative functions.\n    !!! info \"Interaction with JAX\"\n        Concrete implementations of `GenerativeFunction` will likely interact with the JAX tracing machinery if used with the languages exposed by `genjax`. Hence, there are specific implementation requirements which are more stringent than the requirements\n        enforced in other Gen implementations (e.g. Gen in Julia).\n        * For broad compatibility, the implementation of the interfaces *should* be compatible with JAX tracing.\n        * If a user wishes to implement a generative function which is not compatible with JAX tracing, that generative function may invoke other JAX compat generative functions, but likely cannot be invoked inside of JAX compat generative functions.\n    Aside from JAX compatibility, an implementor *should* match the interface signatures documented below. This is not statically checked - but failure to do so\n    will lead to unintended behavior or errors.\n    \"\"\"\n# This is used to support tracing -- the user is not required to provide\n# a PRNGKey, because the value of the key is not important, only\n# the fact that the value has type PRNGKey.\ndef __abstract_call__(self, *args) -&gt; Tuple[PRNGKey, Any]:\nkey = jax.random.PRNGKey(0)\n_, tr = self.simulate(key, args)\nretval = tr.get_retval()\nreturn retval\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\nshape = kwargs.get(\"shape\", ())\nreturn Bottom(shape)\n@abc.abstractmethod\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n\"\"\"\n        &gt; Given a `PRNGKey` and arguments, execute the generative function, returning a new `PRNGKey` and a trace.\n        `simulate` can be informally thought of as forward sampling: given `key: PRNGKey` and arguments `args: Tuple`, the generative function should sample a choice map $c \\sim p(\\cdot; \\\\text{args})$, as well as any untraced randomness $r \\sim p(\\cdot; \\\\text{args}, c)$.\n        The implementation of `simulate` should then create a trace holding the choice map, as well as the score $\\log \\\\frac{p(c; \\\\text{args})}{q(r; \\\\text{args}, c)}$.\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            key: A new (deterministically evolved) `PRNGKey`.\n            tr: A trace capturing the data and inference data associated with the generative function invocation.\n        Examples:\n            Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            print(console.render(tr))\n            ```\n            Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                y = genjax.normal(x, 1.0) @ \"y\"\n                return y\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            print(console.render(tr))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, Trace]]:\n\"\"\"\n        &gt; Given a `PRNGKey`, a choice map (constraints), and arguments, execute the generative function, returning a new `PRNGKey`, a single-sample importance weight estimate of the conditional density evaluated at the non-constrained choices, and a trace whose choice map is consistent with the constraints.\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            key: A new (deterministically evolved) `PRNGKey`.\n            tup: A tuple `(w, tr)` where `w` is an importance weight estimate of the conditional density, and `tr` is a trace capturing the data and inference data associated with the generative function invocation.\n        \"\"\"\n@abc.abstractmethod\ndef update(\nself,\nkey: PRNGKey,\ntrace: Trace,\nnew: ChoiceMap,\ndiffs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, Trace, ChoiceMap]]:\npass\n@abc.abstractmethod\ndef assess(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\npass\ndef unzip(\nself,\nkey: PRNGKey,\nfixed: ChoiceMap,\n) -&gt; Tuple[\nPRNGKey,\nCallable[[ChoiceMap, Tuple], FloatArray],\nCallable[[ChoiceMap, Tuple], Any],\n]:\nkey, sub_key = jax.random.split(key)\ndef score(differentiable: Tuple, nondifferentiable: Tuple) -&gt; FloatArray:\nprovided, args = tree_zipper(differentiable, nondifferentiable)\nmerged = fixed.merge(provided)\n_, (_, score) = self.assess(sub_key, merged, args)\nreturn score\ndef retval(differentiable: Tuple, nondifferentiable: Tuple) -&gt; Any:\nprovided, args = tree_zipper(differentiable, nondifferentiable)\nmerged = fixed.merge(provided)\n_, (retval, _) = self.assess(sub_key, merged, args)\nreturn retval\nreturn key, score, retval\n# A higher-level gradient API - it relies upon `unzip`,\n# but provides convenient access to first-order gradients.\ndef choice_grad(self, key, trace, selection):\nfixed = selection.complement().filter(trace.strip())\nchm = selection.filter(trace.strip())\nkey, scorer, _ = self.unzip(key, fixed)\ngrad, nograd = tree_grad_split(\n(chm, trace.get_args()),\n)\nchoice_gradient_tree, _ = jax.grad(scorer)(grad, nograd)\nreturn key, choice_gradient_tree\n###################\n# ADEV and fusion #\n###################\ndef adev_simulate(self, key: PRNGKey, args: Tuple) -&gt; Tuple[PRNGKey, Any]:\n\"\"\"An opt-in method which expresses forward sampling from the\n        generative function in terms of primitives which are compatible with\n        ADEV's language.\"\"\"\nraise NotImplementedError\ndef prepare_fuse(self, key: PRNGKey, args: Tuple):\n\"\"\"Convert a generative function to a canonical form with ADEV\n        primitives for proposal fusion.\"\"\"\nraise NotImplementedError\ndef fuse(self, _: \"GenerativeFunction\"):\n\"\"\"Fuse a generative function and a proposal to produce a probabilistic\n        computation that returns an ELBO estimate.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.simulate","title":"<code>simulate(key, args)</code>  <code>abstractmethod</code>","text":"<p>Given a <code>PRNGKey</code> and arguments, execute the generative function, returning a new <code>PRNGKey</code> and a trace.</p> <p><code>simulate</code> can be informally thought of as forward sampling: given <code>key: PRNGKey</code> and arguments <code>args: Tuple</code>, the generative function should sample a choice map \\(c \\sim p(\\cdot; \\text{args})\\), as well as any untraced randomness \\(r \\sim p(\\cdot; \\text{args}, c)\\).</p> <p>The implementation of <code>simulate</code> should then create a trace holding the choice map, as well as the score \\(\\log \\frac{p(c; \\text{args})}{q(r; \\text{args}, c)}\\).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>args</code> <code>Tuple</code> <p>Arguments to the generative function.</p> required <p>Returns:</p> Name Type Description <code>key</code> <code>PRNGKey</code> <p>A new (deterministically evolved) <code>PRNGKey</code>.</p> <code>tr</code> <code>Trace</code> <p>A trace capturing the data and inference data associated with the generative function invocation.</p> <p>Examples:</p> <p>Here's an example using a <code>genjax</code> distribution (<code>normal</code>). Distributions are generative functions, so they support the interface.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nprint(console.render(tr))\n</code></pre> <pre><code>DistributionTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 Normal\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u251c\u2500\u2500 (const) 0.0\n\u2502       \u2514\u2500\u2500 (const) 1.0\n\u251c\u2500\u2500 value\n\u2502   \u2514\u2500\u2500  f32[]\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> <p>Here's a slightly more complicated example using the <code>Builtin</code> generative function language. You can find more examples on the <code>Builtin</code> language page.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = genjax.normal(0.0, 1.0) @ \"x\"\ny = genjax.normal(x, 1.0) @ \"y\"\nreturn y\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nprint(console.render(tr))\n</code></pre> <pre><code>BuiltinTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502       \u2514\u2500\u2500 source\n\u2502           \u2514\u2500\u2500 PytreeClosure\n\u2502               \u251c\u2500\u2500 callable\n\u2502               \u2502   \u2514\u2500\u2500 &lt;function model&gt;\n\u2502               \u2514\u2500\u2500 environment\n\u2502                   \u2514\u2500\u2500 list\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u251c\u2500\u2500 retval\n\u2502   \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 choices\n\u2502   \u2514\u2500\u2500 Trie\n\u2502       \u251c\u2500\u2500 :x\n\u2502       \u2502   \u2514\u2500\u2500 DistributionTrace\n\u2502       \u2502       \u251c\u2500\u2500 gen_fn\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 Normal\n\u2502       \u2502       \u251c\u2500\u2500 args\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2502       \u251c\u2500\u2500 (const) 0.0\n\u2502       \u2502       \u2502       \u2514\u2500\u2500 (const) 1.0\n\u2502       \u2502       \u251c\u2500\u2500 value\n\u2502       \u2502       \u2502   \u2514\u2500\u2500  f32[]\n\u2502       \u2502       \u2514\u2500\u2500 score\n\u2502       \u2502           \u2514\u2500\u2500  f32[]\n\u2502       \u2514\u2500\u2500 :y\n\u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502               \u251c\u2500\u2500 gen_fn\n\u2502               \u2502   \u2514\u2500\u2500 Normal\n\u2502               \u251c\u2500\u2500 args\n\u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502               \u2502       \u251c\u2500\u2500  f32[]\n\u2502               \u2502       \u2514\u2500\u2500 (const) 1.0\n\u2502               \u251c\u2500\u2500 value\n\u2502               \u2502   \u2514\u2500\u2500  f32[]\n\u2502               \u2514\u2500\u2500 score\n\u2502                   \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 cache\n\u2502   \u2514\u2500\u2500 Trie\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n\"\"\"\n    &gt; Given a `PRNGKey` and arguments, execute the generative function, returning a new `PRNGKey` and a trace.\n    `simulate` can be informally thought of as forward sampling: given `key: PRNGKey` and arguments `args: Tuple`, the generative function should sample a choice map $c \\sim p(\\cdot; \\\\text{args})$, as well as any untraced randomness $r \\sim p(\\cdot; \\\\text{args}, c)$.\n    The implementation of `simulate` should then create a trace holding the choice map, as well as the score $\\log \\\\frac{p(c; \\\\text{args})}{q(r; \\\\text{args}, c)}$.\n    Arguments:\n        key: A `PRNGKey`.\n        args: Arguments to the generative function.\n    Returns:\n        key: A new (deterministically evolved) `PRNGKey`.\n        tr: A trace capturing the data and inference data associated with the generative function invocation.\n    Examples:\n        Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        print(console.render(tr))\n        ```\n        Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = genjax.normal(0.0, 1.0) @ \"x\"\n            y = genjax.normal(x, 1.0) @ \"y\"\n            return y\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        print(console.render(tr))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.importance","title":"<code>importance(key, chm, args)</code>  <code>abstractmethod</code>","text":"<p>Given a <code>PRNGKey</code>, a choice map (constraints), and arguments, execute the generative function, returning a new <code>PRNGKey</code>, a single-sample importance weight estimate of the conditional density evaluated at the non-constrained choices, and a trace whose choice map is consistent with the constraints.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>args</code> <code>Tuple</code> <p>Arguments to the generative function.</p> required <p>Returns:</p> Name Type Description <code>key</code> <code>PRNGKey</code> <p>A new (deterministically evolved) <code>PRNGKey</code>.</p> <code>tup</code> <code>Tuple[FloatArray, Trace]</code> <p>A tuple <code>(w, tr)</code> where <code>w</code> is an importance weight estimate of the conditional density, and <code>tr</code> is a trace capturing the data and inference data associated with the generative function invocation.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, Trace]]:\n\"\"\"\n    &gt; Given a `PRNGKey`, a choice map (constraints), and arguments, execute the generative function, returning a new `PRNGKey`, a single-sample importance weight estimate of the conditional density evaluated at the non-constrained choices, and a trace whose choice map is consistent with the constraints.\n    Arguments:\n        key: A `PRNGKey`.\n        args: Arguments to the generative function.\n    Returns:\n        key: A new (deterministically evolved) `PRNGKey`.\n        tup: A tuple `(w, tr)` where `w` is an importance weight estimate of the conditional density, and `tr` is a trace capturing the data and inference data associated with the generative function invocation.\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.update","title":"<code>update(key, trace, new, diffs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef update(\nself,\nkey: PRNGKey,\ntrace: Trace,\nnew: ChoiceMap,\ndiffs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, Trace, ChoiceMap]]:\npass\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.assess","title":"<code>assess(key, chm, args)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef assess(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\npass\n</code></pre>"},{"location":"genjax/library/core/generative.html#traces","title":"Traces","text":"<p>Traces are data structures which record (execution and inference) data about the invocation of generative functions.</p> <p>Traces are often specialized to a generative function language, to take advantage of data locality, and other representation optimizations.</p> <p>Traces support a set of accessor method interfaces designed to provide convenient manipulation when handling traces in inference algorithms.</p>"},{"location":"genjax/library/core/generative.html#genjax.core.Trace","title":"<code>genjax.core.Trace</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ChoiceMap</code>, <code>Tree</code></p> <p>Abstract base class for traces of generative functions.</p> <p>A <code>Trace</code> is a data structure used to represent sampled executions of generative functions.</p> <p>Traces track metadata associated with log probabilities of choices, as well as other data associated with the invocation of a generative function, including the arguments it was invoked with, its return value, and the identity of the generative function itself.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass Trace(ChoiceMap, Tree):\n\"\"\"\n    &gt; Abstract base class for traces of generative functions.\n    A `Trace` is a data structure used to represent sampled executions of generative functions.\n    Traces track metadata associated with log probabilities of choices, as well as other data associated with the invocation of a generative function, including the arguments it was invoked with, its return value, and the identity of the generative function itself.\n    \"\"\"\n@abc.abstractmethod\ndef get_retval(self) -&gt; Any:\n\"\"\"\n        Returns the return value from the generative function invocation which created the `Trace`.\n        Examples:\n            Here's an example using `genjax.normal` (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            retval = tr.get_retval()\n            chm = tr.get_choices()\n            v = chm.get_leaf_value()\n            print(console.render((retval, v)))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_score(self) -&gt; FloatArray:\npass\n@abc.abstractmethod\ndef get_args(self) -&gt; Tuple:\npass\n@abc.abstractmethod\ndef get_choices(self) -&gt; ChoiceMap:\npass\n@abc.abstractmethod\ndef get_gen_fn(self) -&gt; \"GenerativeFunction\":\n\"\"\"\n        Returns the generative function whose invocation created the `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            gen_fn = tr.get_gen_fn()\n            print(console.render(gen_fn))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef project(self, selection: \"Selection\") -&gt; FloatArray:\npass\ndef update(self, key, choices, argdiffs):\ngen_fn = self.get_gen_fn()\nreturn gen_fn.update(key, self, choices, argdiffs)\ndef has_subtree(self, addr) -&gt; BoolArray:\nchoices = self.get_choices()\nreturn choices.has_subtree(addr)\ndef get_subtree(self, addr) -&gt; ChoiceMap:\nchoices = self.get_choices()\nreturn choices.get_subtree(addr)\ndef get_subtrees_shallow(self):\nchoices = self.get_choices()\nreturn choices.get_subtrees_shallow()\ndef merge(self, other) -&gt; ChoiceMap:\nreturn self.get_choices().merge(other.get_choices())\ndef get_selection(self):\nreturn self.get_choices().get_selection()\ndef strip(self):\n\"\"\"\n        Remove all `Trace` metadata, and return a choice map.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            chm = tr.strip()\n            print(console.render(chm))\n            ```\n        \"\"\"\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self.get_choices(), is_leaf=_check)\ndef __getitem__(self, addr):\nchoices = self.get_choices()\nchoice = choices.get_subtree(addr)\nif isinstance(choice, BooleanMask):\nif is_concrete(choice.mask):\nif choice.mask:\nreturn choice.unmask()\nelse:\nreturn EmptyChoiceMap()\nelse:\nreturn choice\nelif isinstance(choice, Leaf):\nreturn choice.get_leaf_value()\nelse:\nreturn choice\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_gen_fn","title":"<code>get_gen_fn()</code>  <code>abstractmethod</code>","text":"<p>Returns the generative function whose invocation created the <code>Trace</code>.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\ngen_fn = tr.get_gen_fn()\nprint(console.render(gen_fn))\n</code></pre> <pre><code>Normal\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_gen_fn(self) -&gt; \"GenerativeFunction\":\n\"\"\"\n    Returns the generative function whose invocation created the `Trace`.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        gen_fn = tr.get_gen_fn()\n        print(console.render(gen_fn))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_retval","title":"<code>get_retval()</code>  <code>abstractmethod</code>","text":"<p>Returns the return value from the generative function invocation which created the <code>Trace</code>.</p> <p>Examples:</p> <p>Here's an example using <code>genjax.normal</code> (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nretval = tr.get_retval()\nchm = tr.get_choices()\nv = chm.get_leaf_value()\nprint(console.render((retval, v)))\n</code></pre> <pre><code>(Array(-0.10823099, dtype=float32), Array(-0.10823099, dtype=float32))\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_retval(self) -&gt; Any:\n\"\"\"\n    Returns the return value from the generative function invocation which created the `Trace`.\n    Examples:\n        Here's an example using `genjax.normal` (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        retval = tr.get_retval()\n        chm = tr.get_choices()\n        v = chm.get_leaf_value()\n        print(console.render((retval, v)))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_choices","title":"<code>get_choices()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_choices(self) -&gt; ChoiceMap:\npass\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_score","title":"<code>get_score()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_score(self) -&gt; FloatArray:\npass\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.strip","title":"<code>strip()</code>","text":"<p>Remove all <code>Trace</code> metadata, and return a choice map.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nchm = tr.strip()\nprint(console.render(chm))\n</code></pre> <pre><code>ValueChoiceMap\n\u2514\u2500\u2500 value\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>def strip(self):\n\"\"\"\n    Remove all `Trace` metadata, and return a choice map.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        chm = tr.strip()\n        print(console.render(chm))\n        ```\n    \"\"\"\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self.get_choices(), is_leaf=_check)\n</code></pre>"},{"location":"genjax/library/core/generative.html#choice-maps","title":"Choice maps","text":""},{"location":"genjax/library/core/generative.html#genjax.core.ChoiceMap","title":"<code>genjax.core.ChoiceMap</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Tree</code></p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass ChoiceMap(Tree):\n@abc.abstractmethod\ndef get_selection(self):\n\"\"\"\n        Convert a `ChoiceMap` to a `Selection`.\n        \"\"\"\ndef get_choices(self):\nreturn self\ndef strip(self):\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self, is_leaf=_check)\ndef __eq__(self, other):\nreturn self.flatten() == other.flatten()\ndef __getitem__(self, addr):\nchoice = self.get_subtree(addr)\nif isinstance(choice, Leaf):\nv = choice.get_leaf_value()\n# If the choice is a Leaf, it might participate in masking.\n# Here, we check if the value is masked.\n# Then, we either unwrap the mask - or return it,\n# depending on the concreteness of the mask value.\nif isinstance(v, BooleanMask):\nif is_concrete(v.mask):\nif v.mask:\nreturn v.unmask()\nelse:\nreturn EmptyChoiceMap()\nelse:\nreturn v\nelse:\nreturn v\nelse:\nreturn choice\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.ChoiceMap.get_selection","title":"<code>get_selection()</code>  <code>abstractmethod</code>","text":"<p>Convert a <code>ChoiceMap</code> to a <code>Selection</code>.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_selection(self):\n\"\"\"\n    Convert a `ChoiceMap` to a `Selection`.\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#selections","title":"Selections","text":""},{"location":"genjax/library/core/generative.html#genjax.core.Selection","title":"<code>genjax.core.Selection</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Tree</code></p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass Selection(Tree):\n@abc.abstractmethod\ndef filter(self, chm: ChoiceMap) -&gt; ChoiceMap:\npass\n@abc.abstractmethod\ndef complement(self) -&gt; \"Selection\":\npass\ndef get_selection(self):\nreturn self\ndef __getitem__(self, addr):\nsubselection = self.get_subtree(addr)\nreturn subselection\n</code></pre>"},{"location":"genjax/library/core/interpreters.html","title":"Interpreters","text":"<p>JAX supports transformations of pure, numerical Python programs by staging out interpreters which evaluate <code>Jaxpr</code> representations of programs.</p> <p>The <code>Core</code> module features interpreter infrastructure, and common transforms designed to facilitate certain types of transformations.</p>"},{"location":"genjax/library/core/interpreters.html#contextual-interpreter","title":"Contextual interpreter","text":"<p>A common type of interpreter involves overloading desired primitives with context-specific behavior by inheriting from <code>Trace</code> and define the correct methods to process the primitives.</p> <p>In this module, we provide an interpreter which mixes initial style (e.g. the Python program is immediately staged, and then an interpreter walks the <code>Jaxpr</code> representation) with custom <code>Trace</code> and <code>Tracer</code> overloads. </p> <p>This pattern supports a wide range of program transformations, and allows parametrization over the inner interpreter (e.g. forward evaluation, or CPS).</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context","title":"<code>genjax._src.core.interpreters.context</code>","text":"<p>This module contains a transformation infrastructure based on interpreters with stateful contexts and custom primitive handling lookups.</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context.ContextualTracer","title":"<code>ContextualTracer</code>","text":"<p>         Bases: <code>jc.Tracer</code></p> <p>A <code>ContextualTracer</code> encapsulates a single value.</p> Source code in <code>src/genjax/_src/core/interpreters/context.py</code> <pre><code>class ContextualTracer(jc.Tracer):\n\"\"\"A `ContextualTracer` encapsulates a single value.\"\"\"\ndef __init__(self, trace: \"ContextualTrace\", val: Value):\nself._trace = trace\nself.val = val\n@property\ndef aval(self):\nreturn abstract_arrays.raise_to_shaped(jc.get_aval(self.val))\ndef full_lower(self):\nreturn self\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context.ContextualTrace","title":"<code>ContextualTrace</code>","text":"<p>         Bases: <code>jc.Trace</code></p> <p>An evaluating trace that dispatches to a dynamic context.</p> Source code in <code>src/genjax/_src/core/interpreters/context.py</code> <pre><code>class ContextualTrace(jc.Trace):\n\"\"\"An evaluating trace that dispatches to a dynamic context.\"\"\"\ndef pure(self, val: Value) -&gt; ContextualTracer:\nreturn ContextualTracer(self, val)\ndef sublift(self, tracer: ContextualTracer) -&gt; ContextualTracer:\nreturn self.pure(tracer.val)\ndef lift(self, val: Value) -&gt; ContextualTracer:\nreturn self.pure(val)\ndef process_primitive(\nself,\nprimitive: jc.Primitive,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n) -&gt; Union[ContextualTracer, List[ContextualTracer]]:\ncontext = staging.get_dynamic_context(self)\ncustom_rule = context.get_custom_rule(primitive)\nif custom_rule:\nreturn custom_rule(self, *tracers, **params)\nreturn self.default_process_primitive(primitive, tracers, params)\ndef default_process_primitive(\nself,\nprimitive: jc.Primitive,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n) -&gt; Union[ContextualTracer, List[ContextualTracer]]:\ncontext = staging.get_dynamic_context(self)\nvals = [v.val for v in tracers]\nif context.can_process(primitive):\noutvals = context.process_primitive(primitive, *vals, **params)\nreturn jax_util.safe_map(self.pure, outvals)\noutvals = primitive.bind(*vals, **params)\nif not primitive.multiple_results:\noutvals = [outvals]\nout_tracers = jax_util.safe_map(self.full_raise, outvals)\nif primitive.multiple_results:\nreturn out_tracers\nreturn out_tracers[0]\ndef process_call(\nself,\ncall_primitive: jc.Primitive,\nf: Any,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_higher_order_primitive(\nself, call_primitive, f, tracers, params, False\n)\ndef post_process_call(self, call_primitive, out_tracers, params):\nvals = tuple(t.val for t in out_tracers)\nmaster = self.main\ndef todo(x):\ntrace = ContextualTrace(master, jc.cur_sublevel())\nreturn jax_util.safe_map(functools.partial(ContextualTracer, trace), x)\nreturn vals, todo\ndef process_map(\nself,\ncall_primitive: jc.Primitive,\nf: Any,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_higher_order_primitive(\nself, call_primitive, f, tracers, params, True\n)\npost_process_map = post_process_call\ndef process_custom_jvp_call(self, primitive, fun, jvp, tracers, *, symbolic_zeros):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_custom_jvp_call(\nself, primitive, fun, jvp, tracers, symbolic_zeros=symbolic_zeros\n)\ndef post_process_custom_jvp_call(self, out_tracers, jvp_was_run):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_jvp_call(self, out_tracers, jvp_was_run)\ndef process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers, out_trees):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_custom_vjp_call(\nself, primitive, fun, fwd, bwd, tracers, out_trees\n)\ndef post_process_custom_vjp_call(self, out_tracers, params):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_vjp_call(self, out_tracers, params)\ndef post_process_custom_vjp_call_fwd(self, out_tracers, out_trees):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_vjp_call_fwd(self, out_tracers, out_trees)\n</code></pre>"},{"location":"genjax/library/generative_functions/index.html","title":"Generative function languages","text":"<p>This module contains several standard generative function classes useful for structuring probabilistic programs.</p> <ul> <li>The <code>distributions</code> module exports standard distributions from several sources, including SciPy (<code>scipy</code>), TensorFlow Probability Distributions (<code>tfd</code>), and custom distributions.<ul> <li>The <code>distributions</code> module also contains a small <code>oryx</code>-like language called <code>coryx</code> which implements the generative function interface for programs with inverse log determinant Jacobian (ildj) compatible return value functions of distribution random choices.</li> <li>The <code>distributions</code> module also contains an implementation of <code>gensp</code>, a research language for probabilistic programming with estimated densities.</li> </ul> </li> <li>The <code>builtin</code> module contains a function-like language for defining generative functions from programs.</li> <li>The <code>combinators</code> module contains combinators which support transforming generative functions into new ones with structured control flow patterns of computation.</li> </ul>"},{"location":"genjax/library/generative_functions/builtin.html","title":"Builtin language","text":"<p>This module provides a function-like modeling language. The generative function interfaces are implemented for objects in this language using transformations by JAX interpreters.</p> <p>The language also exposes a set of JAX primitives which allow hierarchical construction of generative programs. These programs can utilize other generative functions inside of a new JAX primitive (<code>trace</code>) to create hierarchical patterns of generative computation.</p>"},{"location":"genjax/library/generative_functions/builtin.html#usage","title":"Usage","text":"<p>The <code>Builtin</code> language is a common foundation for constructing models. It exposes a DSL based on JAX primitives and transformations which allows the programmer to construct generative functions out of Python functions. </p> <p>Below, we illustrate a simple example:</p> <pre><code>from genjax import beta \nfrom genjax import bernoulli \nfrom genjax import uniform \nfrom genjax import gen\n@genjax.gen\ndef beta_bernoulli_process(u):\np = beta(0, u) @ \"p\"\nv = bernoulli(p) @ \"v\"\nreturn v\n@genjax.gen\ndef joint():\nu = uniform() @ \"u\"\nv = beta_bernoulli_process(u) @ \"bbp\"\nreturn v\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#language-primitives","title":"Language primitives","text":"<p>The builtin language exposes custom primitives, which are handled by JAX interpreters to support the semantics of the generative function interface.</p>"},{"location":"genjax/library/generative_functions/builtin.html#trace","title":"<code>trace</code>","text":"<p>The <code>trace</code> primitive provides access to the to invoke another generative function as a callee.</p>"},{"location":"genjax/library/generative_functions/builtin.html#cache","title":"<code>cache</code>","text":"<p>The <code>cache</code> primitive is designed to expose a space vs. time trade-off for incremental computation in Gen's <code>update</code> interface.</p>"},{"location":"genjax/library/generative_functions/builtin.html#generative-datatypes","title":"Generative datatypes","text":"<p>The builtin language implements a trie-like trace, choice map, and selection.</p>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.BuiltinTrace","title":"<code>genjax.generative_functions.builtin.BuiltinTrace</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Trace</code></p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_datatypes.py</code> <pre><code>@dataclass\nclass BuiltinTrace(Trace):\ngen_fn: GenerativeFunction\nargs: Tuple\nretval: Any\nchoices: Trie\ncache: Trie\nscore: FloatArray\ndef flatten(self):\nreturn (\nself.gen_fn,\nself.args,\nself.retval,\nself.choices,\nself.cache,\nself.score,\n), ()\ndef get_gen_fn(self):\nreturn self.gen_fn\ndef get_choices(self):\nreturn BuiltinChoiceMap(self.choices)\ndef get_retval(self):\nreturn self.retval\ndef get_score(self):\nreturn self.score\ndef get_args(self):\nreturn self.args\ndef project(self, selection: Selection):\nweight = 0.0\nfor (k, v) in self.choices.get_subtrees_shallow():\nif selection.has_subtree(k):\nweight += v.project(selection[k])\nreturn weight\ndef has_cached_value(self, addr):\nreturn self.cache.has_subtree(addr)\ndef get_cached_value(self, addr):\nreturn self.cache.get_subtree(addr)\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html","title":"Distributions","text":"<p>This module provides:</p> <ul> <li> <p>Abstract base classes for declaring distributions as <code>GenerativeFunction</code> types. These classes include <code>Distribution</code> and <code>ExactDensity</code>. The latter assumes that the inheritor exposes exact density evaluation, while the former makes no such assumption.</p> </li> <li> <p>Several distributions from JAX's <code>scipy</code> module, as well as TensorFlow Distributions (<code>tfd</code>) from TensorFlow Probability (<code>tfp</code>) using the JAX backend.</p> </li> <li> <p>Custom distributions, including ones with exact posteriors (like discrete HMMs).</p> </li> <li> <p>A language (<code>coryx</code>) based on <code>oryx</code> for defining new distribution objects from inverse log determinant Jacobian transformations on existing distributions.</p> </li> <li> <p>A language (<code>gensp</code>) for defining distributions with estimated densities using inference.</p> </li> </ul> <p>Info</p> <p>On this page, we document the abstract base classes which are used throughout the module. For submodules which implement distributions using the base classes (e.g. <code>scipy</code>, or <code>tfd</code>) - we list the available distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#the-distribution-abstract-base-class","title":"The <code>Distribution</code> abstract base class","text":""},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.Distribution","title":"<code>genjax.generative_functions.distributions.Distribution</code>  <code>dataclass</code>","text":"<p>         Bases: <code>GenerativeFunction</code></p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@dataclass\nclass Distribution(GenerativeFunction):\ndef flatten(self):\nreturn (), ()\n# This overloads the call functionality for this generative function\n# and allows usage of shorthand notation in the builtin DSL.\ndef __call__(self, *args, **kwargs) -&gt; DeferredGenerativeFunctionCall:\nreturn DeferredGenerativeFunctionCall.new(self, args, kwargs)\n# Syntactical overload to define `Product` of distributions.\n# C.f. below.\ndef __mul__(self, other):\np = Product([])\np.append(self)\np.append(other)\nreturn p\n@typecheck\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\n# `get_trace_type` is compile time - the key value\n# doesn't matter, just the type.\nkey = jax.random.PRNGKey(1)\n_, (_, (_, ttype)) = jax.make_jaxpr(self.random_weighted, return_shape=True)(\nkey, *args\n)\nreturn tt_lift(ttype)\n@abc.abstractmethod\ndef random_weighted(self, *args, **kwargs):\npass\n@abc.abstractmethod\ndef estimate_logpdf(self, key, v, *args, **kwargs):\npass\n@typecheck\ndef simulate(\nself, key: PRNGKey, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, DistributionTrace]:\nkey, (w, v) = self.random_weighted(key, *args, **kwargs)\ntr = DistributionTrace(self, args, v, w)\nreturn key, tr\n@typecheck\ndef importance(\nself, key: PRNGKey, chm: ChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, DistributionTrace]]:\nassert isinstance(chm, Leaf)\n# If the choice map is empty, we just simulate\n# and return 0.0 for the log weight.\nif isinstance(chm, EmptyChoiceMap):\nkey, tr = self.simulate(key, args, **kwargs)\nreturn key, (0.0, tr)\n# If it's not empty, we should check if it is a mask.\n# If it is a mask, we need to see if it is active or not,\n# and then unwrap it - and use the active flag to determine\n# what to do at runtime.\nv = chm.get_leaf_value()\nif isinstance(v, BooleanMask):\nactive = v.mask\nv = v.unmask()\ndef _active(key, v, args):\nkey, w = self.estimate_logpdf(key, v, *args)\nreturn key, v, w\ndef _inactive(key, v, _):\nw = 0.0\nreturn key, v, w\nkey, v, w = concrete_cond(active, _active, _inactive, key, v, args)\nscore = w\n# Otherwise, we just estimate the logpdf of the value\n# we got out of the choice map.\nelse:\nkey, w = self.estimate_logpdf(key, v, *args)\nscore = w\nreturn key, (\nw,\nDistributionTrace(self, args, v, score),\n)\n# NOTE: Here's an interesting note about `update`...\n# (really, any of the GFI methods for any generative function)\n# - they should return homogeneous types for any return\n# branch leading out of the call.\n# Because these methods may be invoked in `jax.lax.switch` calls\n# it's important that callers have some knowledge about the\n# consistency of invoking a callee -- most generative function\n# languages ensure this is true by default e.g. if they defer\n# some of their behavior to callees.\n# For `Distribution` this is not true by default - we have to be\n# careful when defining the methods, and this is most true of update\n# below.\n@typecheck\ndef update(\nself,\nkey: PRNGKey,\nprev: DistributionTrace,\nconstraints: ChoiceMap,\nargdiffs: Tuple,\n**kwargs\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, DistributionTrace, Any]]:\nassert isinstance(constraints, Leaf)\nmaybe_discard = mask(False, prev.get_choices())\n# Incremental optimization - if nothing has changed,\n# just return the previous trace.\nif isinstance(constraints, EmptyChoiceMap) and static_check_no_change(argdiffs):\nv = prev.get_retval()\nretval_diff = Diff(v, NoChange)\nreturn key, (retval_diff, 0.0, prev, maybe_discard)\n# Otherwise, we consider the cases.\nargs = tree_diff_primal(argdiffs)\n# First, we have to check if the trace provided\n# is masked or not. It's possible that a trace\n# with a mask is updated.\nprev_v = prev.get_retval()\nactive = True\nif isinstance(prev_v, BooleanMask):\nactive = prev_v.mask\nprev_v = prev_v.unmask()\n# Case 1: the new choice map is empty here.\nif isinstance(constraints, EmptyChoiceMap):\nprev_score = prev.get_score()\nv = prev_v\n# If the value is active, we compute any weight\n# corrections from changing arguments.\ndef _active(key, v, *args):\nkey, fwd = self.estimate_logpdf(key, v, *args)\nreturn key, fwd - prev_score\n# If the value is inactive, we do nothing.\ndef _inactive(key, v, *args):\nreturn key, prev_score\nkey, w = concrete_cond(active, _active, _inactive, key, v, *args)\ndiscard = maybe_discard\nretval_diff = jtu.tree_map(lambda v: Diff(v, NoChange), prev_v)\n# Case 2: the new choice map is not empty here.\nelse:\nprev_score = prev.get_score()\nv = constraints.get_leaf_value()\n# Now, we must check if the choice map has a masked\n# leaf value, and dispatch accordingly.\nactive_chm = True\nif isinstance(v, BooleanMask):\nactive_chm = v.mask\nv = v.unmask()\n# The only time this flag is on is when both leaf values\n# are concrete, or they are both masked with true mask\n# values.\nactive = jnp.all(jnp.logical_and(active_chm, active))\nkey, fwd = self.estimate_logpdf(key, v, *args)\ndef _constraints_active(key, v, *args):\nreturn key, v, fwd - prev_score\ndef _constraints_inactive(key, v, *args):\nreturn key, prev_v, 0.0\nkey, v, w = concrete_cond(\nactive_chm, _constraints_active, _constraints_inactive, key, v, *args\n)\ndiscard = mask(active_chm, ValueChoiceMap(prev.get_leaf_value()))\nretval_diff = Diff(v, UnknownChange)\nreturn key, (\nretval_diff,\nw,\nDistributionTrace(self, args, v, w),\ndiscard,\n)\n@typecheck\ndef assess(\nself, key: PRNGKey, evaluation_point: ValueChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\nv = evaluation_point.get_leaf_value()\nkey, score = self.estimate_logpdf(key, v, *args)\nreturn key, (v, score)\n########\n# ADEV #\n########\n@typecheck\ndef prepare_fuse(self, key: PRNGKey, args: Tuple):\nprob_prim = adev.ADEVPrimitive(self)\nkey, v = adev.sample(prob_prim, key, args)\nreturn key, (v, ValueChoiceMap(v))\n@typecheck\ndef fuse(self, proposal: \"Distribution\") -&gt; adev.ADEVProgram:\ndef wrapper(key, p_args, q_args):\nkey, (v, _) = proposal.prepare_fuse(key, q_args)\nkey, qw = proposal.estimate_logpdf(key, v, *q_args)\nkey, pw = self.estimate_logpdf(key, v, *p_args)\nreturn key, pw - qw\nreturn adev.ADEVProgram(wrapper)\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.Distribution.random_weighted","title":"<code>random_weighted(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef random_weighted(self, *args, **kwargs):\npass\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.Distribution.estimate_logpdf","title":"<code>estimate_logpdf(key, v, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef estimate_logpdf(self, key, v, *args, **kwargs):\npass\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#the-exactdensity-abstract-base-class","title":"The <code>ExactDensity</code> abstract base class","text":"<p>If you are attempting to create a new <code>Distribution</code>, you'll likely want to inherit from <code>ExactDensity</code> - which assumes that you have access to an exact logpdf method (a more restrictive assumption than <code>Distribution</code>). This is most often the case: all of the standard distributions (<code>scipy</code>, <code>tfd</code>) use <code>ExactDensity</code>.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.ExactDensity","title":"<code>genjax.generative_functions.distributions.ExactDensity</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Distribution</code></p> <p>Abstract base class which extends Distribution and assumes that the implementor provides an exact logpdf method (compared to one which returns an estimate of the logpdf).</p> <p>All of the standard distributions inherit from <code>ExactDensity</code>, and if you are looking to implement your own distribution, you should likely use this class.</p> <p><code>Distribution</code> implementors are <code>Pytree</code> implementors</p> <p>As <code>Distribution</code> extends <code>Pytree</code>, if you use this class, you must implement <code>flatten</code> as part of your class declaration.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@dataclass\nclass ExactDensity(Distribution):\n\"\"\"\n    &gt; Abstract base class which extends Distribution and assumes that the implementor provides an exact logpdf method (compared to one which returns _an estimate of the logpdf_).\n    All of the standard distributions inherit from `ExactDensity`, and if you are looking to implement your own distribution, you should likely use this class.\n    !!! info \"`Distribution` implementors are `Pytree` implementors\"\n        As `Distribution` extends `Pytree`, if you use this class, you must implement `flatten` as part of your class declaration.\n    \"\"\"\n@abc.abstractmethod\ndef sample(self, key: PRNGKey, *args, **kwargs) -&gt; Any:\n\"\"\"\n        &gt; Sample from the distribution, returning a value from the event space.\n        Arguments:\n            key: A `PRNGKey`.\n            *args: The arguments to the distribution invocation.\n        Returns:\n            v: A value from the event space of the distribution.\n        !!! info \"Implementations need not return a new `PRNGKey`\"\n            Note that `sample` does not return a new evolved `PRNGKey`. This is for convenience - `ExactDensity` is used often, and the interface for `sample` is simple. `sample` is called by `random_weighted` in the generative function interface implementations, and always gets a fresh `PRNGKey` - `sample` as a callee need not return a new evolved key.\n        Examples:\n            `genjax.normal` is a distribution with an exact density, which supports the `sample` interface. Here's an example of invoking `sample`.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            v = genjax.normal.sample(key, 0.0, 1.0)\n            print(console.render(v))\n            ```\n            Note that you often do want or need to invoke `sample` directly - you'll likely want to use the generative function interface methods instead:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            print(console.render(tr))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef logpdf(self, v, *args, **kwargs):\n\"\"\"\n        &gt; Given a value from the event space, compute the log probability of that value under the distribution.\n        \"\"\"\ndef random_weighted(self, key, *args, **kwargs):\nkey, sub_key = jax.random.split(key)\nv = self.sample(sub_key, *args, **kwargs)\nw = self.logpdf(v, *args, **kwargs)\nreturn key, (w, v)\ndef estimate_logpdf(self, key, v, *args, **kwargs):\nw = self.logpdf(v, *args, **kwargs)\nreturn key, w\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.ExactDensity.sample","title":"<code>sample(key, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Sample from the distribution, returning a value from the event space.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>*args</code> <p>The arguments to the distribution invocation.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>v</code> <code>Any</code> <p>A value from the event space of the distribution.</p> <p>Implementations need not return a new <code>PRNGKey</code></p> <p>Note that <code>sample</code> does not return a new evolved <code>PRNGKey</code>. This is for convenience - <code>ExactDensity</code> is used often, and the interface for <code>sample</code> is simple. <code>sample</code> is called by <code>random_weighted</code> in the generative function interface implementations, and always gets a fresh <code>PRNGKey</code> - <code>sample</code> as a callee need not return a new evolved key.</p> <p>Examples:</p> <p><code>genjax.normal</code> is a distribution with an exact density, which supports the <code>sample</code> interface. Here's an example of invoking <code>sample</code>.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nv = genjax.normal.sample(key, 0.0, 1.0)\nprint(console.render(v))\n</code></pre> <pre><code>-0.62537825\n</code></pre> <p>Note that you often do want or need to invoke <code>sample</code> directly - you'll likely want to use the generative function interface methods instead:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nprint(console.render(tr))\n</code></pre> <pre><code>DistributionTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 Normal\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u251c\u2500\u2500 (const) 0.0\n\u2502       \u2514\u2500\u2500 (const) 1.0\n\u251c\u2500\u2500 value\n\u2502   \u2514\u2500\u2500  f32[]\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef sample(self, key: PRNGKey, *args, **kwargs) -&gt; Any:\n\"\"\"\n    &gt; Sample from the distribution, returning a value from the event space.\n    Arguments:\n        key: A `PRNGKey`.\n        *args: The arguments to the distribution invocation.\n    Returns:\n        v: A value from the event space of the distribution.\n    !!! info \"Implementations need not return a new `PRNGKey`\"\n        Note that `sample` does not return a new evolved `PRNGKey`. This is for convenience - `ExactDensity` is used often, and the interface for `sample` is simple. `sample` is called by `random_weighted` in the generative function interface implementations, and always gets a fresh `PRNGKey` - `sample` as a callee need not return a new evolved key.\n    Examples:\n        `genjax.normal` is a distribution with an exact density, which supports the `sample` interface. Here's an example of invoking `sample`.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        v = genjax.normal.sample(key, 0.0, 1.0)\n        print(console.render(v))\n        ```\n        Note that you often do want or need to invoke `sample` directly - you'll likely want to use the generative function interface methods instead:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        print(console.render(tr))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.ExactDensity.logpdf","title":"<code>logpdf(v, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a value from the event space, compute the log probability of that value under the distribution.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef logpdf(self, v, *args, **kwargs):\n\"\"\"\n    &gt; Given a value from the event space, compute the log probability of that value under the distribution.\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/scipy.html","title":"SciPy","text":"<p>Here, we document the GenJAX distributions which implement the generative function interfaces using <code>scipy</code> distribution methods. </p> <p>Each shorthand name (<code>module-attribute</code> below) refers to an instance of an indicator class which implements the interfaces for <code>ExactDensity</code> using the <code>scipy</code> distribution methods. </p> <p>These indicator classes are instanced when <code>genjax</code> is loaded, for convenience (so that users need not instance their own <code>Beta()</code>, etc).</p>"},{"location":"genjax/library/generative_functions/distributions/scipy.html#beta","title":"Beta","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.beta","title":"<code>genjax.generative_functions.distributions.beta = Beta()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#bernoulli","title":"Bernoulli","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.bernoulli","title":"<code>genjax.generative_functions.distributions.bernoulli = Bernoulli()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#cauchy","title":"Cauchy","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.cauchy","title":"<code>genjax.generative_functions.distributions.cauchy = Cauchy()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#categorical","title":"Categorical","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.categorical","title":"<code>genjax.generative_functions.distributions.categorical = Categorical()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#dirichlet","title":"Dirichlet","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.dirichlet","title":"<code>genjax.generative_functions.distributions.dirichlet = Dirichlet()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#exponential","title":"Exponential","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.exponential","title":"<code>genjax.generative_functions.distributions.exponential = Exponential()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#gamma","title":"Gamma","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.gamma","title":"<code>genjax.generative_functions.distributions.gamma = Gamma()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#laplace","title":"Laplace","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.laplace","title":"<code>genjax.generative_functions.distributions.laplace = Laplace()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#logistic","title":"Logistic","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.logistic","title":"<code>genjax.generative_functions.distributions.logistic = Logistic()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#multivariate-normal","title":"Multivariate normal","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.mv_normal","title":"<code>genjax.generative_functions.distributions.mv_normal = MultivariateNormal()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#normal","title":"Normal","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.normal","title":"<code>genjax.generative_functions.distributions.normal = Normal()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#pareto","title":"Pareto","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.pareto","title":"<code>genjax.generative_functions.distributions.pareto = Pareto()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#poisson","title":"Poisson","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.poisson","title":"<code>genjax.generative_functions.distributions.poisson = Poisson()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/inference/index.html","title":"Inference","text":"<p><code>genjax</code> exposes several inference algorithms which are implemented utilizing the generative function interface.</p>"},{"location":"genjax/library/inference/is.html","title":"Importance sampling","text":"<p>This module exposes two variants of importance sampling, differing in their return signature.</p> <p>The first is <code>ImportanceSampling</code>.</p> <p>Sampling importance resampling runs importance sampling, and then resamples a single particle from the particle collection to return.</p>"},{"location":"genjax/library/inference/is.html#genjax.inference.ImportanceSampling","title":"<code>genjax.inference.ImportanceSampling</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Bootstrap and proposal importance sampling for generative functions.</p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass ImportanceSampling(Pytree):\n\"\"\"\n    Bootstrap and proposal importance sampling for generative functions.\n    \"\"\"\nnum_particles: IntArray\nmodel: GenerativeFunction\nproposal: Union[None, GenerativeFunction] = None\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@typecheck\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn ImportanceSampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\ndef _bootstrap_importance_sampling(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\n):\nkey, sub_keys = slash(key, self.num_particles)\n_, (lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nmodel_args,\n)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (trs, log_normalized_weights, log_ml_estimate)\ndef _proposal_importance_sampling(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\nproposal_args: Tuple,\n):\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n_, p_trs = jax.vmap(self.proposal.simulate, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nproposal_args,\n)\nobservations = jax.tree_util.map(\nlambda v: jnp.repeats(v, self.num_particles), observations\n)\nchm = p_trs.get_choices().merge(observations)\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n_, (lws, m_trs) = jax.vmap(self.model.importance, in_axes=(0, 0, None))(\nsub_keys,\nchm,\nmodel_args,\n)\nlws = lws - p_trs.get_score()\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (m_trs, log_normalized_weights, log_ml_estimate)\n@typecheck\ndef apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance sampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_sampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance sampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_sampling(key, choice_map, model_args)\n@typecheck\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.ImportanceSampling.new","title":"<code>new(num_particles, model, proposal=None)</code>  <code>classmethod</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@typecheck\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn ImportanceSampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.ImportanceSampling.apply","title":"<code>apply(key, choice_map, *args)</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@typecheck\ndef apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance sampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_sampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance sampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_sampling(key, choice_map, model_args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax.inference.SamplingImportanceResampling","title":"<code>genjax.inference.SamplingImportanceResampling</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass SamplingImportanceResampling(Pytree):\nnum_particles: IntArray\nmodel: GenerativeFunction\nproposal: Union[None, GenerativeFunction] = None\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn SamplingImportanceResampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\ndef _bootstrap_importance_resampling(\nself,\nkey: PRNGKey,\nobs: ChoiceMap,\nmodel_args: Tuple,\n):\nkey, sub_keys = slash(key, self.num_particles)\n_, (lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\nsub_keys, obs, model_args\n)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nkey, sub_key = jax.random.split(key)\nind = jax.random.categorical(sub_key, log_normalized_weights)\ntr = jax.tree_util.tree_map(lambda v: v[ind], trs)\nlnw = log_normalized_weights[ind]\nreturn key, (tr, lnw, log_ml_estimate)\ndef apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance resampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_resampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance resampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_resampling(key, choice_map, model_args)\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.SamplingImportanceResampling.new","title":"<code>new(num_particles, model, proposal=None)</code>  <code>classmethod</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn SamplingImportanceResampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.SamplingImportanceResampling.apply","title":"<code>apply(key, choice_map, *args)</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>def apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance resampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_resampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance resampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_resampling(key, choice_map, model_args)\n</code></pre>"}]}