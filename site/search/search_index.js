var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"<p>Short: it's a probabilistic programming library. It's primary computational object (the generative function) supports a well-studied and useful formal interface. It empowers users with the ability to customize and optimize their model and inference algorithms.</p> <p>Gen is a multi-paradigm (generative, differentiable, incremental) system for probabilistic programming. GenJAX is an implementation of Gen on top of JAX - exposing the ability to programmatically construct and manipulate generative functions (1) (computational objects which represent probability measures over structured sample spaces) on native devices, accelerators, and other parallel fabrics. </p> <ol> <li> <p>By design, generative functions expose a concise interface for expressing approximate and differentiable inference algorithms. </p> <p>The set of generative functions is extensible! You can implement your own - allowing advanced users to performance optimize their critical modeling/inference code paths.</p> <p>You can (and we, at the MIT Probabilistic Computing Project, do!) use these objects for machine learning - including robotics, natural language processing, reasoning about agents, and modelling / creating systems which exhibit human-like reasoning.</p> <p>A precise mathematical formulation of generative functions is given in Marco Cusumano-Towner's PhD thesis.</p> </li> </ol>"},{"location":"index.html#why-gen","title":"Why Gen?","text":"<p>GenJAX is a Gen implementation. If you're considering using GenJAX, or why this library exists - it's worth starting by understanding why Gen exists. Gen exists because probabilistic modeling and inference is hard - both computationally, and existing tooling.</p>"},{"location":"genjax/diff_jl.html","title":"Diffing against Gen.jl","text":"<p><code>GenJAX</code> is inherits concepts from Gen and algorithm reference implementations from <code>Gen.jl</code> - there are a few necessary design deviations between <code>GenJAX</code> and <code>Gen.jl</code> that stem from JAX's underlying array programming model. In this section, we describe several of these differences and try to highlight workarounds or discuss the reason for the discrepancy.</p>"},{"location":"genjax/diff_jl.html#turing-universality","title":"Turing universality","text":"<p><code>Gen.jl</code> is Turing universal - it can encode any computable distribution, including those expressed by forms of unbounded recursion.</p> <p>It is a bit ambiguous whether or not <code>GenJAX</code> falls in this category: JAX does not feature mechanisms for dynamic shape allocations, but it does feature mechanisms for unbounded recursion.</p> <p>The former provides a technical barrier to implementing Gen's trace machinery. While JAX allows for unbounded recursion, to support Gen's interfaces we also need the ability to dynamically allocate choice data. This requirement is currently at tension with XLA's requirements of knowing the static shape of everything.</p> <p>However, <code>GenJAX</code> supports generative function combinators with bounded recursion / unfold chain length. Ahead of time, these combinators can be directed to pre-allocate arrays with enough size to handle recursion/looping within the bounds that the programmer sets. If these bounds are exceeded, a Python runtime error will be thrown (both on and off JAX device).</p> <p>In practice, this means that some performance engineering (space vs. expressivity) is required of the programmer. It's certainly feasible to express bounded recursive computations which terminate with probability 1 - but you'll need to ahead of time allocate space for it.</p>"},{"location":"genjax/diff_jl.html#mutation","title":"Mutation","text":"<p>Just like JAX, GenJAX disallows mutation - expressing a mutation to an array must be done through special interfaces, and those interfaces return full copies. There are special circumstances where these interfaces will be performed in place.</p>"},{"location":"genjax/diff_jl.html#to-jit-or-not-to-jit","title":"To JIT or not to JIT","text":"<p><code>Gen.jl</code> is written in Julia, which automatically JITs everything. <code>GenJAX</code>, by virtue of being constructed on top of JAX, allows us to JIT JAX compatible code - but the JIT process is user directed. Thus, the idioms that are used to express and optimize inference code are necessarily different compared to <code>Gen.jl</code>. In the inference standard library, you'll typically find algorithms implemented as dataclasses which inherit (and implement) the <code>jax.Pytree</code> interfaces. Implementing these interfaces allow usage of inference dataclasses and methods in jittable code - and, as a bonus, allow us to be specific about trace vs. runtime known values.</p> <p>In general, it's productive to enclose as much of a computation as possible in a <code>jax.jit</code> block. This can sometimes lead to long trace times. If trace times are ballooning, a common source is explicit for-loops (with known bounds, else JAX will complain). In these cases, you might look at Advice on speeding up compilation time. We've taken care to optimize (by e.g. using XLA primitives) the code which we expose from GenJAX - but if you find something out of the ordinary, file an issue!</p>"},{"location":"genjax/notebooks.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"genjax/library/core.html","title":"Core","text":"<p>This module provides the core functionality and JAX compatibility layer which the <code>GenJAX</code> generative function modeling and inference modules are built on top of. It contains (truncated, and in no particular order):</p> <ul> <li> <p>Core Gen associated data types for generative functions.</p> </li> <li> <p>Utility functionality for automatically registering class definitions as valid <code>Pytree</code> method implementors (guaranteeing <code>flatten</code>/<code>unflatten</code> compatibility across JAX transform boundaries). For more information, see Pytrees.</p> </li> <li> <p>Staging functionality that allows linear lifting of pure, numerical Python programs to <code>ClosedJaxpr</code> instances.</p> </li> <li> <p>Transformation interpreters: interpreter-based transformations on which operate on <code>ClosedJaxpr</code> instances. Interpreters are all written in initial style - they operate on <code>ClosedJaxpr</code> instances, and don't implement their own custom <code>jax.Tracer</code> types - but they are JAX compatible, implying that they can be staged out for zero runtime cost.</p> </li> <li> <p>Masking functionality which allows active/inactive flagging of data - useful when branching patterns of computation require uncertainty in whether or not data is active with respect to a generative computation.</p> </li> </ul>"},{"location":"genjax/library/core.html#genjax.core.GenerativeFunction","title":"<code>genjax.core.GenerativeFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Abstract class which provides an inheritance base for user-defined implementations of the generative function interface methods e.g. the <code>BuiltinGenerativeFunction</code> and <code>Distribution</code> languages both implement a class inheritor of <code>GenerativeFunction</code>.</p> <p>Any implementation will interact with the JAX tracing machinery, however, so there are specific API requirements above the requirements enforced in other languages (unlike Gen in Julia, for example).</p> <p>The user must match the interface signatures of the native JAX implementation. This is not statically checked - but failure to do so will lead to unintended behavior or errors.</p> <p>To support argument and choice gradients via JAX, the user must provide a differentiable <code>importance</code> implementation.</p> Source code in <code>genjax/_src/core/datatypes.py</code> <pre><code>@dataclasses.dataclass\nclass GenerativeFunction(Pytree):\n\"\"\"Abstract class which provides an inheritance base for user-defined\n    implementations of the generative function interface methods e.g. the\n    `BuiltinGenerativeFunction` and `Distribution` languages both implement a\n    class inheritor of `GenerativeFunction`.\n\n    Any implementation will interact with the JAX tracing machinery,\n    however, so there are specific API requirements above the requirements\n    enforced in other languages (unlike Gen in Julia, for example).\n\n    The user *must* match the interface signatures of the native JAX\n    implementation. This is not statically checked - but failure to do so\n    will lead to unintended behavior or errors.\n\n    To support argument and choice gradients via JAX, the user must\n    provide a differentiable `importance` implementation.\n    \"\"\"\n\n    # This is used in tracing -- the user is not required to provide\n    # a PRNGKey, because the value of the key is not important, only\n    # the fact that the value has type PRNGKey.\n    def __abstract_call__(self, *args) -&gt; Tuple[PRNGKey, Any]:\n        key = jax.random.PRNGKey(0)\n        key, tr = self.simulate(key, args)\n        retval = tr.get_retval()\n        return key, retval\n\n    def get_trace_type(self, *args, **kwargs) -&gt; TraceType:\n        shape = kwargs.get(\"shape\", ())\n        return Bottom(shape)\n\n    @abc.abstractmethod\n    def simulate(\n        self,\n        key: PRNGKey,\n        args: Tuple,\n    ) -&gt; Tuple[PRNGKey, Trace]:\n        pass\n\n    @abc.abstractmethod\n    def importance(\n        self,\n        key: PRNGKey,\n        chm: ChoiceMap,\n        args: Tuple,\n    ) -&gt; Tuple[PRNGKey, Tuple[FloatArray, Trace]]:\n        pass\n\n    @abc.abstractmethod\n    def update(\n        self,\n        key: PRNGKey,\n        original: Trace,\n        new: ChoiceMap,\n        diffs: Tuple,\n    ) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, Trace, ChoiceMap]]:\n        pass\n\n    @abc.abstractmethod\n    def assess(\n        self,\n        key: PRNGKey,\n        evaluation_point: ChoiceMap,\n        args: Tuple,\n    ) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\n        pass\n\n    def unzip(\n        self,\n        key: PRNGKey,\n        fixed: ChoiceMap,\n    ) -&gt; Tuple[\n        PRNGKey,\n        Callable[[ChoiceMap, Tuple], FloatArray],\n        Callable[[ChoiceMap, Tuple], Any],\n    ]:\n        key, sub_key = jax.random.split(key)\n\n        def score(differentiable: Tuple, nondifferentiable: Tuple) -&gt; FloatArray:\n            provided, args = tree_zipper(differentiable, nondifferentiable)\n            merged = fixed.merge(provided)\n            _, (_, score) = self.assess(sub_key, merged, args)\n            return score\n\n        def retval(differentiable: Tuple, nondifferentiable: Tuple) -&gt; Any:\n            provided, args = tree_zipper(differentiable, nondifferentiable)\n            merged = fixed.merge(provided)\n            _, (retval, _) = self.assess(sub_key, merged, args)\n            return retval\n\n        return key, score, retval\n\n    # A higher-level gradient API - it relies upon `unzip`,\n    # but provides convenient access to first-order gradients.\n    def choice_grad(self, key, trace, selection):\n        fixed = selection.complement().filter(trace.strip())\n        evaluation_point = selection.filter(trace.strip())\n        key, scorer, _ = self.unzip(key, fixed)\n        grad, nograd = tree_grad_split(\n            (evaluation_point, trace.get_args()),\n        )\n        choice_gradient_tree, _ = jax.grad(scorer)(grad, nograd)\n        return key, choice_gradient_tree\n</code></pre>"},{"location":"genjax/library/core.html#genjax._src.core.datatypes.GenerativeFunction.simulate","title":"<code>simulate(key, args)</code>  <code>abstractmethod</code>","text":"Source code in <code>genjax/_src/core/datatypes.py</code> <pre><code>@abc.abstractmethod\ndef simulate(\n    self,\n    key: PRNGKey,\n    args: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n    pass\n</code></pre>"},{"location":"genjax/library/core.html#genjax._src.core.datatypes.GenerativeFunction.importance","title":"<code>importance(key, chm, args)</code>  <code>abstractmethod</code>","text":"Source code in <code>genjax/_src/core/datatypes.py</code> <pre><code>@abc.abstractmethod\ndef importance(\n    self,\n    key: PRNGKey,\n    chm: ChoiceMap,\n    args: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, Trace]]:\n    pass\n</code></pre>"},{"location":"genjax/library/core.html#genjax._src.core.datatypes.GenerativeFunction.update","title":"<code>update(key, original, new, diffs)</code>  <code>abstractmethod</code>","text":"Source code in <code>genjax/_src/core/datatypes.py</code> <pre><code>@abc.abstractmethod\ndef update(\n    self,\n    key: PRNGKey,\n    original: Trace,\n    new: ChoiceMap,\n    diffs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, Trace, ChoiceMap]]:\n    pass\n</code></pre>"},{"location":"genjax/library/core.html#genjax._src.core.datatypes.GenerativeFunction.assess","title":"<code>assess(key, evaluation_point, args)</code>  <code>abstractmethod</code>","text":"Source code in <code>genjax/_src/core/datatypes.py</code> <pre><code>@abc.abstractmethod\ndef assess(\n    self,\n    key: PRNGKey,\n    evaluation_point: ChoiceMap,\n    args: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\n    pass\n</code></pre>"},{"location":"genjax/library/generative_functions.html","title":"Generative functions","text":"<p>This module contains several standard generative function classes useful for structuring probabilistic programs.</p>"},{"location":"genjax/library/inference.html","title":"Inference","text":""}]}