{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"homepage.html","title":"Overview","text":"<p>GenJAX: a probabilistic programming library designed to scale probabilistic modeling and inference into high performance settings. (1)</p> <ol> <li> <p>Here, high performance means massively parallel, either cores or devices.</p> <p>For those whom this overview page may be irrelevant: the value proposition is about putting expressive models and customizable Bayesian inference on GPUs, TPUs, etc - without sacrificing abstraction or modularity.</p> </li> </ol> <p>Gen is a multi-paradigm (generative, differentiable, incremental) system for probabilistic programming. GenJAX is an implementation of Gen on top of JAX (2) - exposing the ability to programmatically construct and manipulate generative functions (1) (computational objects which represent probability measures over structured sample spaces), with compilation to native devices, accelerators, and other parallel fabrics. </p> <ol> <li> <p>By design, generative functions expose a concise interface for expressing approximate and differentiable inference algorithms. </p> <p>The set of generative functions is extensible! You can implement your own - allowing advanced users to performance optimize their critical modeling/inference code paths.</p> <p>You can (and we, at the MIT Probabilistic Computing Project, do!) use these objects for machine learning - including robotics, natural language processing, reasoning about agents, and modelling / creating systems which exhibit human-like reasoning.</p> <p>A precise mathematical formulation of generative functions is given in Marco Cusumano-Towner's PhD thesis.</p> </li> <li> <p>If the usage of JAX is not a dead giveaway, GenJAX is written in Python.</p> </li> </ol> Model codeInference code <p><p> Defining a beta-bernoulli process model as a generative function in GenJAX. </p></p> <pre><code>@genjax.gen\ndef model():\np = beta(0, 1) @ \"p\"\nv = bernoulli(p) @ \"v\"\nreturn v\n</code></pre> <p><p> This works for any generative function, not just the beta-bernoulli model. </p></p> <pre><code>def importance_sampling(\nkey: PRNGKey,\ngen_fn: GenerativeFunction,\nmodel_args: Tuple,\nobs: ChoiceMap,\nn_samples: Int,\n): # (1)!\nkey, sub_keys = genjax.slash(key, n_samples)  # split keys\n_, (lws, trs) = jax.vmap(\ngen_fn.importance, # (2)!\nin_axes=(0, None, None),\n)(sub_keys, obs, args)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (trs, log_normalized_weights, log_ml_estimate)\n</code></pre> <ol> <li> <p>Here's a few notes about the signature:</p> <ul> <li><code>PRNGKey</code> is the type of <code>jax.random.PRNGKey</code>. In GenJAX, we pass keys into generative code, and generative code returns a changed key.</li> <li><code>GenerativeFunction</code> refers to generative functions, objects which expose Gen's probabilistic interface.</li> <li>For now, think of <code>ChoiceMap</code> as the type of object which Gen uses to express conditioning.</li> </ul> </li> <li> <p><code>gen_fn.importance</code> is a generative function interface method. Generative functions are responsible for implementing this method, to support conditional sampling and conditional density estimation. You can learn a lot more about this method in the generative function interface.</p> </li> </ol>"},{"location":"homepage.html#what-sort-of-things-do-you-use-genjax-for","title":"What sort of things do you use GenJAX for?","text":"Real time object tracking <p>Real time tracking of objects in 3D using probabilistic rendering. (Left) Ground truth, (center) depth mask, (right) inference overlaid on ground truth.</p> <p><p> </p></p>"},{"location":"homepage.html#why-gen","title":"Why Gen?","text":"<p>GenJAX is a Gen implementation. If you're considering using GenJAX - it's worth starting by understanding what problems Gen purports to solve.</p>"},{"location":"homepage.html#the-evolution-of-probabilistic-programming-languages","title":"The evolution of probabilistic programming languages","text":"<p>Probabilistic modeling and inference is hard: understanding a domain well enough to construct a probabilistic model in the Bayesian paradigm is challenging, and that's half the battle - the other half is designing effective inference algorithms to probe the implications of the model (1).</p> <ol> <li> <p>Some probabilistic programming languages restrict the set of allowable models, providing (in return) efficient (often, exact) inference. </p> <p>Gen considers a wide class of models - include Bayesian nonparametrics, open-universe models, and models over rich structures (like programs!) - which don't natively support efficient exact inference.</p> </li> </ol> <p>Model writers have historically considered the following design loop.</p> <pre><code>graph LR\n  A[Design model.] --&gt; B[Implement inference by hand.];\n  B --&gt; C[Model + inference okay?];\n  C --&gt; D[Happy.];\n  C --&gt; A;</code></pre> <p>The first generation (1) of probabilistic programming systems introduced inference engines which could operate abstractly over many different models, without requiring the programmer to return and tweak their inference code. The utopia envisioned by these systems is shown below.</p> <ol> <li> <p>Here, the definition of \"first generation\" includes systems like JAGS, BUGS, BLOG, IBAL, Church, Infer.NET, Figaro, Stan, amongst others.</p> <p>But more precisely, many systems preceded the DARPA PPAML project - which gave rise to several novel systems, including the predecessors of Gen.</p> </li> </ol> <pre><code>graph LR\n  A[Design model.] --&gt; D[Model + inference okay?];\n  B[Inference engine.] ---&gt; D;\n  D --&gt; E[Happy.];\n  D ---&gt; A;</code></pre> <p>The problem with this utopia is that we often need to customize our inference algorithms (1) to achieve maximum performance, with respect to accuracy as well as runtime (2). First generation systems were not designed with this in mind.</p> <ol> <li>Here, programmable inference denotes using a custom proposal distribution in importance sampling, or a custom variational family for variational inference, or even a custom kernel in Markov chain Monte Carlo.</li> <li>Composition of inference programs can also be highly desirable when performing inference in complex models, or designing a probabilistic application from several modeling and inference components. The first examples of universal inference engines ignored this design problem.</li> </ol>"},{"location":"homepage.html#programmable-inference","title":"Programmable inference","text":"<p>A worthy design goal is to allow users to customize when required, while retaining the rapid model/inference iteration properties explored by first generation systems.</p> <p>Gen addresses this goal by introducing a separation between modeling and inference code: the generative function interface.</p> <p> </p> <p>The interface provides an abstraction layer that inference algorithms can call to compute the necessary (and hard to get right!) math (1). Probabilistic application developers can also extend the interface to new modeling languages - and immediately gain access to advanced inference procedures.</p> <ol> <li> <p>Examples of hard-to-get-right math: importance weights, accept reject ratios, and gradient estimators. </p> <p>For simple models and inference, one might painlessly derive these quantities. As soon as the model/inference gets complicated, however, you might find yourself thanking the interface.</p> </li> </ol>"},{"location":"homepage.html#whose-using-gen","title":"Whose using Gen?","text":"<p>Gen supports a growing list of users, with collaboration across academic research labs and industry affiliates.</p> <p> </p> <p>We're looking to expand our user base! If you're interested, please contact us to get involved.</p>"},{"location":"genjax/diff_jl.html","title":"Comparisons with Gen.jl","text":"<p>GenJAX implements concepts from Gen, and implements several inference algorithms with reference to implementations from <code>Gen.jl</code>. In general, you should find that the programming patterns and interface idioms should match closely with <code>Gen.jl</code>.</p> <p>However, there are a few necessary design deviations between <code>genjax</code> and <code>Gen.jl</code> that stem from restrictions arising from JAX's compilation model. In this section, we describe several of these differences and try to highlight workarounds or discuss the reason for the discrepancy.</p>"},{"location":"genjax/diff_jl.html#turing-universality","title":"Turing universality","text":"<p><code>Gen.jl</code> is Turing universal - it can encode any computable distribution, including those expressed by forms of unbounded recursion.</p> <p>Arguing from a practical perspective, <code>genjax</code> also falls into this category, but several things are harder to encode for GPUs(1). Additionally, JAX does not feature mechanisms for dynamic shape allocations, but it does feature mechanisms for unbounded recursion.</p> <ol> <li>We expect that GPU/TPU deployment to be the dominant usage pattern for <code>genjax</code> - and defer optimized CPU deployment to other implementations of Gen.</li> </ol> <p>Lack of dynamic allocations provides a technical barrier to implementing Gen's trace machinery for generative functions which feature recursive calls to other generative functions. While JAX allows for unbounded recursion, to generally support recording trace data - we also need the ability to dynamically allocate choice data. This requirement is currently at tension with XLA's requirements of knowing the static shape of everything. Nonetheless, one might imagine pre-allocating large arrays - passing them into <code>jax.lax.while_op</code> implementations of certain types of recursion, etc. Painful and impractical - yes, but theoretical possible.</p> <p><code>genjax</code> supports generative function combinators with bounded recursion / unfold chain length. Ahead of time, these combinators can be directed to pre-allocate arrays with enough size to handle recursion/looping within the bounds that the programmer sets. If these bounds are exceeded, a Python runtime error will be thrown (both on and off JAX device).</p> <p>In practice, this means that some performance engineering (space vs. expressivity) is required of the programmer. It's certainly feasible to express bounded recursive computations which terminate with probability 1 - but you'll need to ahead of time allocate space for it.</p>"},{"location":"genjax/diff_jl.html#mutation","title":"Mutation","text":"<p>Just like JAX, GenJAX disallows mutation - expressing a mutating operation on an array must be done through special JAX interfaces. Outside of JIT compilation, those interfaces often fully copy array data. Inside of JIT compilation, there are special circumstances where these operations will be performed in place.</p>"},{"location":"genjax/diff_jl.html#to-jit-or-not-to-jit","title":"To JIT or not to JIT","text":"<p><code>Gen.jl</code> is written in Julia, which automatically JITs everything. <code>genjax</code>, by virtue of being constructed on top of JAX, allows us to JIT JAX compatible code - but the JIT process is user directed. Thus, the idioms that are used to express and optimize inference code are necessarily different compared to <code>Gen.jl</code>. In the inference standard library, you'll typically find algorithms implemented as dataclasses which inherit (and implement) the <code>jax.Pytree</code> interfaces. Implementing these interfaces allow usage of inference dataclasses and methods in jittable code - and, as a bonus, allow us to be specific about trace vs. runtime known values.</p> <p>In general, it's productive to enclose as much of a computation as possible in a <code>jax.jit</code> block. This can sometimes lead to long trace times. If trace times are ballooning, a common source is explicit for-loops (with known bounds, else JAX will complain). In these cases, you might look at Advice on speeding up compilation time. We've taken care to optimize (by e.g. using XLA primitives) the code which we expose from GenJAX - but if you find something out of the ordinary, file an issue!</p>"},{"location":"genjax/language_aperitifs.html","title":"Language ap\u00e9ritifs","text":"<p>This page assumes that the reader has familiarity with trace-based probabilistic programming systems.</p> <p>The implementation of GenJAX adhers to commonly accepted JAX idioms (1) and modern functional programming patterns (2).</p> <ol> <li>One example: everything is a Pytree. Implies another: everything is JAX traceable by default.</li> <li>Modern here meaning patterns concerning the composition of effectful computations via effect handling abstractions.</li> </ol> <p>GenJAX consists of a set of languages based around transforming pure functions to apply semantic transformations. On this page, we'll provide a taste of some of these languages.</p>"},{"location":"genjax/language_aperitifs.html#the-builtin-language","title":"The builtin language","text":"<p>GenJAX provides a builtin language which supports a <code>trace</code> primitive and the ability to invoke other generative functions as callees:</p> <pre><code>@genjax.gen\ndef submodel():\nx = trace(\"x\", normal)(0.0, 1.0) # explicit\nreturn x\n@genjax.gen\ndef model():\nx = submodel() @ \"sub\" # sugared\nreturn x\n</code></pre> <p>The <code>trace</code> call is a JAX primitive which is given semantics by transformations which implement the semantics of inference interfaces described in Generative functions.</p> <p>Addresses (here, <code>\"x\"</code> and <code>\"sub\"</code>) are important - addressed random choices within <code>trace</code> allow us to structure the address hierarchy for the measure over choice maps which generative functions in this language define.</p> <p>Because convenient idioms for working with addresses is so important in Gen, the generative functions from the builtin language also support a form of \"splatting\" addresses into a caller.</p> <pre><code>@genjax.gen\ndef model():\nx = submodel.inline()\nreturn x\n</code></pre> <p>Invoking the <code>submodel</code> via the <code>inline</code> interface here means that the addresses in <code>submodel</code> are flattened into the address level for the <code>model</code>. If there's overlap, that's a problem! But GenJAX will yell at you for that.</p>"},{"location":"genjax/language_aperitifs.html#structured-control-flow-with-combinators","title":"Structured control flow with combinators","text":"<p>The base modeling language is the <code>BuiltinGenerativeFunction</code> language shown above. The builtin language is based on pure functions, with the interface semantics implemented using program transformations. But we'd also like to take advantage of structured control flow in our generative computations. </p> <p>Users gain access to structured control flow via combinators, other generative function mini-languages which implement the interfaces in control flow compatible ways.</p> <pre><code>@functools.partial(genjax.Map, in_axes=(0, 0))\n@genjax.gen\ndef kernel(x, y):\nz = normal(x + y, 1.0) @ \"z\"\nreturn z\n</code></pre> <p>This defines a <code>MapCombinator</code> generative function - a generative function whose interfaces take care of applying <code>vmap</code> in the appropriate ways (1).</p> <ol> <li>Read: compatible with JIT, gradients, and incremental computation.</li> </ol> <p><code>MapCombinator</code> has a vectorial friend named <code>UnfoldCombinator</code> which implements a <code>scan</code>-like pattern of generative computation.</p> <pre><code>@functools.partial(genjax.Unfold, max_length = 10)\n@genjax.gen\ndef scanner(prev, static_args):\nsigma, = static_args\nnew = normal(prev, sigma) @ \"z\"\nreturn new\n</code></pre> <p><code>UnfoldCombinator</code> allows the expression of general state space models - modeled as a generative function which supports a dependent-for (1) control flow pattern.</p> <ol> <li>Dependent-for means that each iteration may depend on the output from the previous iteration. Think of <code>jax.lax.scan</code> here.</li> </ol> <p><code>UnfoldCombinator</code> allows uncertainty over the length of the chain:</p> <pre><code>@genjax.gen\ndef top_model(p):\nlength = truncated_geometric(10, p) @ \"l\"\ninitial_state = normal(0.0, 1.0) @ \"init\"\nsigma = normal(0.0, 1.0) @ \"sigma\"\n(v, xs) = scanner(length, initial_state, sigma)\nreturn v\n</code></pre> <p>Here, <code>length</code> is drawn from a truncated geometric distribution, and determines the index range of the chain which participates in the generative computation.</p> <p>Of course, combinators are composable.</p> <pre><code>@functools.partial(genjax.Map, in_axes = (0, ))\n@genjax.gen\ndef top_model(p):\nlength = truncated_geometric(10, p) @ \"l\"\ninitial_state = normal(0.0, 1.0) @ \"init\"\nsigma = normal(0.0, 1.0) @ \"sigma\"\n(v, xs) = scanner(length, initial_state, sigma)\nreturn v\n</code></pre> <p>Now we're describing a broadcastable generative function whose internal choices include a chain-like generative structure with dynamic truncation using padding. And we could go on!</p>"},{"location":"genjax/notebooks.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"genjax/concepts/generative_functions.html","title":"Generative functions","text":"<p>Gen is all about generative functions: computational objects which support an interface that helps automate the tricky math involved in programming Bayesian inference algorithms. In this section, we'll unpack the generative function interface and explain the mathematics behind generative functions (1).</p> <ol> <li>For a deeper dive, enjoy Marco Cusumano-Towner's PhD thesis.</li> </ol>"},{"location":"genjax/library/index.html","title":"Library reference","text":"<p>This is the API documentation for modules and symbols which are exposed publicly from <code>genjax</code>. </p> <p>The <code>genjax</code> package consists of several modules, many of which rely on functionality from the <code>genjax.core</code> module, and build upon datatypes, transforms, and generative datatypes which are documented there. Generative function languages use the core datatypes and transforms to implement the generative function interface. Inference and learning algorithms are then implemented using the interface.</p> <ul> <li>The core documentation discusses key datatypes and transformations, which are used throughout the codebase.</li> <li>The documentation on generative function languages describes the functionality and usage for several generative function implementations, including distributions, a function-like language with primitives that allow callee generative functions, and combinator languages which provide structured patterns of control flow.</li> </ul> <ul> <li>The documentation on extension modules describes how users can extend GenJAX with new generative functions and inference functionality, while depending on 3rd party libraries.</li> </ul>"},{"location":"genjax/library/core/index.html","title":"Core","text":""},{"location":"genjax/library/core/index.html#genjax._src.core","title":"<code>core</code>","text":"<p>This module provides the core functionality and JAX compatibility layer which <code>GenJAX</code> generative function and inference modules are built on top of. It contains (truncated, and in no particular order):</p> <ul> <li> <p>Core data types for the associated data types of generative functions.</p> </li> <li> <p>Utility abstract data types (mixins) for automatically registering class definitions as valid <code>Pytree</code> method implementors (guaranteeing <code>flatten</code>/<code>unflatten</code> compatibility across JAX transform boundaries). For more information, see Pytrees.</p> </li> <li> <p>Transformation interpreters: interpreter-based transformations which operate on <code>ClosedJaxpr</code> instances, as well as staging functionality for staging out computations to <code>ClosedJaxpr</code> instances. The core interpreters are written in a mixed initial / final style. The application of all interpreters are JAX compatible, meaning that the application of any interpreter can be staged out to eliminate the interpreter overhead.</p> </li> </ul>"},{"location":"genjax/library/core/datatypes.html","title":"Core datatypes","text":"<p>GenJAX exposes a set of core abstract classes which build on JAX's <code>Pytree</code> interface. These datatypes are used as abstract base mixins for many of the key dataclasses in GenJAX.</p>"},{"location":"genjax/library/core/datatypes.html#genjax.core.Pytree","title":"<code>Pytree</code>","text":"<p>Abstract base class which registers a class with JAX's <code>Pytree</code> system.</p> <p>Users who mixin this ABC for class definitions are required to implement <code>flatten</code> below. In turn, instances of the class gain access to a large set of utility functions for working with <code>Pytree</code> data, as well as the ability to use <code>jax.tree_util</code> Pytree functionality.</p> Source code in <code>src/genjax/_src/core/pytree/pytree.py</code> <pre><code>class Pytree:\n\"\"\"&gt; Abstract base class which registers a class with JAX's `Pytree`\n    system.\n    Users who mixin this ABC for class definitions are required to\n    implement `flatten` below. In turn, instances of the class gain\n    access to a large set of utility functions for working with `Pytree`\n    data, as well as the ability to use `jax.tree_util` Pytree\n    functionality.\n    \"\"\"\ndef __init_subclass__(cls, **kwargs):\njtu.register_pytree_node(\ncls,\ncls.flatten,\ncls.unflatten,\n)\n@abc.abstractmethod\ndef flatten(self) -&gt; Tuple[Tuple, Tuple]:\n\"\"\"`flatten` must be implemented when a user mixes `Pytree` into the\n        declaration of a new class or dataclass.\n        The implementation of `flatten` assumes the following contract:\n        * must return a 2-tuple of tuples.\n        * the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.\n        * the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for `Pytree` equality comparison.\n        For more information, consider [JAX's documentation on Pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n        Returns:\n            dynamic: Dynamic data which supports JAX tracer values.\n            static: Static data which is JAX trace time constant.\n        Examples:\n            Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the `Pytree` mixin.\n            ```python\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            ```\n            !!! info \"Ordering fields in `Pytree` declarations\"\n                Note that the ordering in the dataclass declaration **does matter** - you should put static fields first. The automatically defined `unflatten` method (c.f. below) assumes this ordering.\n            Now, given the declaration, you can use `jax.tree_util` flattening/unflatten functionality.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import genjax\n            import jax.tree_util as jtu\n            from genjax.core import Pytree\n            from dataclasses import dataclass\n            console = genjax.pretty()\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            f = MyFoo(0, 1.0)\n            leaves, form = jtu.tree_flatten(f)\n            print(console.render(leaves))\n            new = jtu.tree_unflatten(form, leaves)\n            print(console.render(new))\n            ```\n        \"\"\"\n@classmethod\ndef unflatten(cls, data, xs):\n\"\"\"Given an implementation of `flatten` (c.f. above), `unflatten` is\n        automatically defined and registered with JAX's `Pytree` system.\n        `unflatten` allows usage of `jtu.tree_unflatten` to create instances of a declared class that mixes `Pytree` from a `PyTreeDef` for that class and leaf data.\n        Examples:\n            Our example from `flatten` above also applies here - where we use `jtu.tree_unflatten` to create a new instance of `MyFoo` from a `PyTreeDef` and leaf data.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import genjax\n            import jax.tree_util as jtu\n            from genjax.core import Pytree\n            from dataclasses import dataclass\n            console = genjax.pretty()\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            f = MyFoo(0, 1.0)\n            leaves, form = jtu.tree_flatten(f)\n            new = jtu.tree_unflatten(form, leaves)\n            print(console.render(new))\n            ```\n        \"\"\"\nreturn cls(*data, *xs)\n@classmethod\ndef new(cls, *args, **kwargs):\nreturn cls(*args, **kwargs)\n# This exposes slicing the struct-of-array representation,\n# taking leaves and indexing/randing into them on the first index,\n# returning a value with the same `Pytree` structure.\ndef slice(self, index_or_index_array):\n\"\"\"&gt; Utility available to any class which mixes `Pytree` base. This\n        method supports indexing/slicing on indices when leaves are arrays.\n        `obj.slice(index)` will take an instance whose class extends `Pytree`, and return an instance of the same class type, but with leaves indexed into at `index`.\n        Arguments:\n            index_or_index_array: An `Int` index or an array of indices which will be used to index into the leaf arrays of the `Pytree` instance.\n        Returns:\n            new_instance: A `Pytree` instance of the same type, whose leaf values are the results of indexing into the leaf arrays with `index_or_index_array`.\n        \"\"\"\nreturn jtu.tree_map(lambda v: v[index_or_index_array], self)\ndef stack(self, *trees):\nreturn tree_stack([self, *trees])\ndef unstack(self):\nreturn tree_unstack(self)\n###################\n# Pretty printing #\n###################\n# Can be customized by Pytree mixers.\ndef __rich_tree__(self, tree):\nsub_tree = gpp.tree_pformat(self)\ntree.add(sub_tree)\nreturn tree\n# Defines default pretty printing.\ndef __rich_console__(self, console, options):\ntree = gpp.tree_pformat(self)\nyield tree\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax.core.AddressTree","title":"<code>AddressTree</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> <p>The <code>AddressTree</code> class is used to define abstract classes for tree- shaped datatypes. These classes are used to implement trace, choice map, and selection types.</p> <p>One should think of <code>AddressTree</code> as providing a convenient base class for many of the generative datatypes declared in GenJAX. <code>AddressTree</code> mixes in <code>Pytree</code> automatically.</p> Source code in <code>src/genjax/_src/core/datatypes/address_tree.py</code> <pre><code>@dataclass\nclass AddressTree(Pytree, metaclass=LiftedTypeMeta):\n\"\"\"&gt; The `AddressTree` class is used to define abstract classes for tree-\n    shaped datatypes. These classes are used to implement trace, choice map,\n    and selection types.\n    One should think of `AddressTree` as providing a convenient base class for\n    many of the generative datatypes declared in GenJAX. `AddressTree` mixes in\n    `Pytree` automatically.\n    \"\"\"\n@abc.abstractmethod\ndef has_subtree(self, addr) -&gt; BoolArray:\npass\n@abc.abstractmethod\ndef get_subtree(self, addr):\npass\n@abc.abstractmethod\ndef get_subtrees_shallow(self):\npass\n###########\n# Dunders #\n###########\ndef __getitem__(self, addr):\nsubtree = self.get_subtree(addr)\nif isinstance(subtree, AddressLeaf):\nv = subtree.get_leaf_value()\nreturn v\nelse:\nreturn subtree\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax.core.AddressLeaf","title":"<code>AddressLeaf</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AddressTree</code></p> <p>The <code>AddressLeaf</code> class specializes <code>AddressTree</code> to classes without any internal subtrees.</p> <p><code>AddressLeaf</code> is a convenient base for generative datatypes which don't keep reference to other <code>AddressTree</code> instances - things like <code>ValueChoiceMap</code> (whose only choice value is a single value, not a dictionary or other tree-like object). <code>AddressLeaf</code> extends <code>AddressTree</code> with a special extension method <code>get_leaf_value</code>.</p> Source code in <code>src/genjax/_src/core/datatypes/address_tree.py</code> <pre><code>@dataclass\nclass AddressLeaf(AddressTree):\n\"\"\"&gt; The `AddressLeaf` class specializes `AddressTree` to classes without\n    any internal subtrees.\n    `AddressLeaf` is a convenient base for generative datatypes which don't keep reference to other `AddressTree` instances - things like `ValueChoiceMap` (whose only choice value is a single value, not a dictionary or other tree-like object). `AddressLeaf` extends `AddressTree` with a special extension method `get_leaf_value`.\n    \"\"\"\n@abc.abstractmethod\ndef get_leaf_value(self):\npass\n@abc.abstractmethod\ndef set_leaf_value(self, v):\npass\ndef has_subtree(self, addr):\nreturn False\ndef get_subtree(self, addr):\nraise Exception(\nf\"{type(self)} is a AddressLeaf: it does not address any internal choices.\"\n)\ndef get_subtrees_shallow(self):\nraise Exception(\nf\"{type(self)} is a AddressLeaf: it does not have any subtrees.\"\n)\n</code></pre>"},{"location":"genjax/library/core/functional_types.html","title":"Functional types","text":"<p>GenJAX provides a set of extension <code>Pytree</code> types to enable modeling idioms which require runtime uncertainty - including models with switching (including address set inhomogeneity), and state space models with dynamic length. To encode runtime uncertainty, we utilize JAX encodings of functional option and sum types. We describe these types below, as well as their usage.</p>"},{"location":"genjax/library/core/functional_types.html#option-types-the-masking-system","title":"(Option types) The masking system","text":"<p>GenJAX contains a system for tagging data with flags, to indicate if the data is valid or invalid during inference interface computations at runtime. The key data structure which supports this system is <code>genjax.core.Mask</code>.</p>"},{"location":"genjax/library/core/functional_types.html#genjax.core.Mask","title":"<code>Mask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> <p>The <code>Mask</code> datatype provides access to the masking system. The masking system is heavily influenced by the functional <code>Option</code> monad.</p> <p>Masks can be used in a variety of ways as part of generative computations - their primary role is to denote data which is valid under inference computations. Valid data can be used as constraints in choice maps, and participate in inference computations (like scores, and importance weights or density ratios).</p> <p>Masks are also used internally by generative function combinators which include uncertainty over structure.</p> <p>Users are expected to interact with <code>Mask</code> instances by either:</p> <ul> <li> <p>Unmasking them using the <code>Mask.unmask</code> interface. This interface uses JAX's <code>checkify</code> transformation to ensure that masked data exposed to a user is used only when valid. If a user chooses to <code>Mask.unmask</code> a <code>Mask</code> instance, they are also expected to use <code>jax.experimental.checkify.checkify</code> to transform their function to one which could return an error.</p> </li> <li> <p>Using <code>Mask.match</code> - which allows a user to provide \"none\" and \"some\" lambdas. The \"none\" lambda should accept no arguments, while the \"some\" lambda should accept an argument whose type is the same as the masked value. These lambdas should return the same type (<code>Pytree</code>, array, etc) of value.</p> </li> </ul> Source code in <code>src/genjax/_src/core/datatypes/masking.py</code> <pre><code>@dataclass\nclass Mask(Pytree):\n\"\"\"The `Mask` datatype provides access to the masking system. The masking\n    system is heavily influenced by the functional `Option` monad.\n    Masks can be used in a variety of ways as part of generative computations - their primary role is to denote data which is valid under inference computations. Valid data can be used as constraints in choice maps, and participate in inference computations (like scores, and importance weights or density ratios).\n    Masks are also used internally by generative function combinators which include uncertainty over structure.\n    Users are expected to interact with `Mask` instances by either:\n    * Unmasking them using the `Mask.unmask` interface. This interface uses JAX's `checkify` transformation to ensure that masked data exposed to a user is used only when valid. If a user chooses to `Mask.unmask` a `Mask` instance, they are also expected to use [`jax.experimental.checkify.checkify`](https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.checkify.checkify.html) to transform their function to one which could return an error.\n    * Using `Mask.match` - which allows a user to provide \"none\" and \"some\" lambdas. The \"none\" lambda should accept no arguments, while the \"some\" lambda should accept an argument whose type is the same as the masked value. These lambdas should return the same type (`Pytree`, array, etc) of value.\n    \"\"\"\nmask: BoolArray\nvalue: Any\ndef flatten(self):\nreturn (self.mask, self.value), ()\n@classmethod\ndef new(cls, mask: BoolArray, inner):\nif isinstance(inner, Mask):\nreturn Mask(\njnp.logical_and(mask, inner.mask),\ninner.value,\n)\nelse:\nreturn cls(mask, inner)\n@typecheck\ndef match(self, none: Callable, some: Callable) -&gt; Any:\n\"\"\"&gt; Pattern match on the `Mask` type - by providing \"none\"\n        and \"some\" lambdas.\n        The \"none\" lambda should accept no arguments, while the \"some\" lambda should accept the same type as the value in the `Mask`. Both lambdas should return the same type (array, or `jax.Pytree`).\n        Arguments:\n            none: A lambda to handle the \"none\" branch. The type of the return value must agree with the \"some\" branch.\n            some: A lambda to handle the \"some\" branch. The type of the return value must agree with the \"none\" branch.\n        Returns:\n            value: A value computed by either the \"none\" or \"some\" lambda, depending on if the `Mask` is valid (e.g. `Mask.mask` is `True`).\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import jax.numpy as jnp\n            import genjax\n            console = genjax.pretty()\n            masked = genjax.mask(False, jnp.ones(5))\n            v1 = masked.match(lambda: 10.0, lambda v: jnp.sum(v))\n            masked = genjax.mask(True, jnp.ones(5))\n            v2 = masked.match(lambda: 10.0, lambda v: jnp.sum(v))\n            print(console.render((v1, v2)))\n            ```\n        \"\"\"\nreturn jax.lax.cond(\nself.mask,\nlambda: some(self.value),\nlambda: none(),\n)\ndef unmask(self):\n\"\"\"&gt; Unmask the `Mask`, returning the value within.\n        This operation is inherently unsafe with respect to inference semantics, and is only valid if the `Mask` is valid at runtime. To enforce validity checks, use `genjax.global_options.allow_checkify(True)` and then handle any code which utilizes `Mask.unmask` with [`jax.experimental.checkify.checkify`](https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.checkify.checkify.html).\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import jax.numpy as jnp\n            import genjax\n            console = genjax.pretty()\n            masked = genjax.mask(True, jnp.ones(5))\n            print(console.render(masked.unmask()))\n            ```\n            Here's an example which uses `jax.experimental.checkify`. To enable runtime checks, the user must enable them explicitly in `genjax`.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import jax.numpy as jnp\n            import jax.experimental.checkify as checkify\n            import genjax\n            console = genjax.pretty()\n            genjax.global_options.allow_checkify(True)\n            masked = genjax.mask(False, jnp.ones(5))\n            err, _ = checkify.checkify(masked.unmask)()\n            print(console.render(err))\n            genjax.global_options.allow_checkify(False)\n            ```\n        \"\"\"\n# If a user chooses to `unmask`, require that they\n# jax.experimental.checkify.checkify their call in transformed\n# contexts.\ndef _check():\ncheck_flag = jnp.all(self.mask)\ncheckify.check(check_flag, \"Mask is False, the masked value is invalid.\\n\")\nglobal_options.optional_check(_check)\nreturn self.value\ndef unsafe_unmask(self):\n# Unsafe version of unmask -- should only be used internally.\nreturn self.value\n###########\n# Dunders #\n###########\ndef __getattr__(self, name):\nsub = getattr(self.value, name)\nif isinstance(sub, Callable):\n@functools.wraps(sub)\ndef wrapper(*args):\nv = sub(*args)\nreturn Mask(self.mask, v)\nreturn wrapper\nelse:\nreturn Mask(self.mask, sub)\ndef __getitem__(self, name):\ns = self.value[name]\nreturn Mask(self.mask, s)\n@dispatch\ndef __eq__(self, other: \"Mask\"):\nreturn jnp.logical_and(\njnp.logical_and(self.mask, other.mask),\nself.value == other.value,\n)\n@dispatch\ndef __eq__(self, other: Any):\nreturn jnp.logical_and(\nself.mask,\nself.value == other,\n)\ndef __hash__(self):\nhash1 = hash(self.value)\nhash2 = hash(self.mask)\nreturn hash((hash1, hash2))\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\ndoc = gpp._pformat_array(self.mask, short_arrays=True)\nval_tree = gpp.tree_pformat(self.value, short_arrays=True)\nsub_tree = Tree(f\"[bold](Mask, {doc})\")\nsub_tree.add(val_tree)\ntree.add(sub_tree)\nreturn tree\n# Defines custom pretty printing.\ndef __rich_console__(self, console, options):\ntree = Tree(\"\")\ntree = self.__rich_tree__(tree)\nyield tree\n</code></pre>"},{"location":"genjax/library/core/functional_types.html#sum-types-tagged-unions","title":"(Sum types) Tagged unions","text":"<p>Like option types, sum types allow representing a form of type uncertainty which can be useful when working within the restricted <code>jax.lax</code> control flow model.</p>"},{"location":"genjax/library/core/functional_types.html#genjax.core.TaggedUnion","title":"<code>TaggedUnion</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/core/datatypes/tagged_unions.py</code> <pre><code>@dataclass\nclass TaggedUnion(Pytree):\ntag: IntArray\nvalues: List[Any]\ndef flatten(self):\nreturn (self.tag, self.values), ()\n@classmethod\n@typecheck\ndef new(cls, tag: IntArray, values: List[Any]):\nreturn cls(tag, values)\ndef _static_assert_tagged_union_switch_num_callables_is_num_values(self, callables):\nassert len(callables) == len(self.values)\ndef _static_assert_tagged_union_switch_returns_same_type(self, vs):\nreturn True\n@typecheck\ndef match(self, *callables: Callable):\nassert len(callables) == len(self.values)\nself._static_assert_tagged_union_switch_num_callables_is_num_values(callables)\nvs = list(map(lambda v: v[0](v[1]), zip(callables, self.values)))\nself._static_assert_tagged_union_switch_returns_same_type(vs)\nvs = jnp.array(vs)\nreturn vs[self.tag]\n###########\n# Dunders #\n###########\ndef __getattr__(self, name):\nsubs = list(map(lambda v: getattr(v, name), self.values))\nif subs and all(map(lambda v: isinstance(v, Callable), subs)):\ndef wrapper(*args):\nvs = [s(*args) for s in subs]\nreturn TaggedUnion(self.tag, vs)\nreturn wrapper\nelse:\nreturn TaggedUnion(self.tag, subs)\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\ndoc = gpp._pformat_array(self.tag, short_arrays=True)\nvals_tree = gpp.tree_pformat(self.values, short_arrays=True)\nsub_tree = Tree(f\"[bold](Tagged, {doc})\")\nsub_tree.add(vals_tree)\ntree.add(sub_tree)\nreturn tree\n# Defines custom pretty printing.\ndef __rich_console__(self, console, options):\ntree = Tree(\"\")\ntree = self.__rich_tree__(tree)\nyield tree\n</code></pre>"},{"location":"genjax/library/core/generative.html","title":"Generative datatypes","text":""},{"location":"genjax/library/core/generative.html#generative-functions","title":"Generative functions","text":"<p>The main computational objects in Gen are generative functions. These objects support an abstract interface of methods and associated types. The interface is designed to allow inference layers to abstract over implementations.</p> <p>Below, we document the abstract base class, and illustrate example usage using concrete implementors. Full descriptions of concrete generative function languages are described in their own documentation module.</p> <p>Logspace for numerical stability</p> <p>In Gen, all relevant inference quantities are given in logspace(1). Most implementations also use logspace, for the same reason. In discussing the math below, we'll often say \"the score\" or \"an importance weight\" and drop the \\(\\log\\) modifier as implicit.</p> <ol> <li>For more on numerical stability &amp; log probabilities, see Log probabilities.</li> </ol> <p>Key generative datatypes in Gen</p> <p>This documentation page contains the type and interface documentation for the primary generative datatypes used in Gen. The documentation on this page deals with the abstract base classes for these datatypes. </p> <p>Any concrete <code>GenerativeFunction</code> implementor should be documented with the language which implements it. Thus, specific generative function languages are not documented here.</p> <p>The interface definitions of generative functions often interact with JAX tracing machinery. GenJAX does not strictly impose this requirement, but does provide a generative function subclass called <code>JAXGenerativeFunction</code> which provides default compatibility definitions for JAX tracing.</p> <p>Other generative function languages which utilize callee generative functions can enforce JAX compatibility by typechecking on <code>JAXGenerativeFunction</code>.</p>"},{"location":"genjax/library/core/generative.html#genjax.core.GenerativeFunction","title":"<code>GenerativeFunction</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> <p>Abstract base class for generative functions.</p> <p>Generative functions are computational objects which expose convenient interfaces for probabilistic modeling and inference. They consist (often, subsets) of a few ingredients:</p> <ul> <li>\\(p(c, r; x)\\): a probability kernel over choice maps (\\(c\\)) and untraced randomness (\\(r\\)) given arguments (\\(x\\)).</li> <li>\\(q(r; x, c)\\): a probability kernel over untraced randomness (\\(r\\)) given arguments (\\(x\\)) and choice map assignments (\\(c\\)).</li> <li>\\(f(x, c, r)\\): a deterministic return value function.</li> <li>\\(q(u; x, u')\\): internal proposal distributions for choice map assignments (\\(u\\)) given other assignments (\\(u'\\)) and arguments (\\(x\\)).</li> </ul> <p>The interface of methods and associated datatypes which these objects expose is called the generative function interface (GFI). Inference algorithms are written against this interface, providing a layer of abstraction above the implementation.</p> <p>Generative functions are allowed to partially implement the interface, with the consequence that partially implemented generative functions may have restricted inference behavior.</p> <p>Interaction with JAX</p> <p>Concrete implementations of <code>GenerativeFunction</code> will likely interact with the JAX tracing machinery if used with the languages exposed by <code>genjax</code>. Hence, there are specific implementation requirements which are more stringent than the requirements enforced in other Gen implementations (e.g. Gen in Julia).</p> <ul> <li>For broad compatibility, the implementation of the interfaces should be compatible with JAX tracing.</li> <li>If a user wishes to implement a generative function which is not compatible with JAX tracing, that generative function may invoke other JAX compat generative functions, but likely cannot be invoked inside of JAX compat generative functions.</li> </ul> <p>Aside from JAX compatibility, an implementor should match the interface signatures documented below. This is not statically checked - but failure to do so will lead to unintended behavior or errors.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass GenerativeFunction(Pytree):\n\"\"\"&gt; Abstract base class for generative functions.\n    Generative functions are computational objects which expose convenient interfaces for probabilistic modeling and inference. They consist (often, subsets) of a few ingredients:\n    * $p(c, r; x)$: a probability kernel over choice maps ($c$) and untraced randomness ($r$) given arguments ($x$).\n    * $q(r; x, c)$: a probability kernel over untraced randomness ($r$) given arguments ($x$) and choice map assignments ($c$).\n    * $f(x, c, r)$: a deterministic return value function.\n    * $q(u; x, u')$: internal proposal distributions for choice map assignments ($u$) given other assignments ($u'$) and arguments ($x$).\n    The interface of methods and associated datatypes which these objects expose is called _the generative function interface_ (GFI). Inference algorithms are written against this interface, providing a layer of abstraction above the implementation.\n    Generative functions are allowed to partially implement the interface, with the consequence that partially implemented generative functions may have restricted inference behavior.\n    !!! info \"Interaction with JAX\"\n        Concrete implementations of `GenerativeFunction` will likely interact with the JAX tracing machinery if used with the languages exposed by `genjax`. Hence, there are specific implementation requirements which are more stringent than the requirements\n        enforced in other Gen implementations (e.g. Gen in Julia).\n        * For broad compatibility, the implementation of the interfaces *should* be compatible with JAX tracing.\n        * If a user wishes to implement a generative function which is not compatible with JAX tracing, that generative function may invoke other JAX compat generative functions, but likely cannot be invoked inside of JAX compat generative functions.\n    Aside from JAX compatibility, an implementor *should* match the interface signatures documented below. This is not statically checked - but failure to do so\n    will lead to unintended behavior or errors.\n    \"\"\"\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\nshape = kwargs.get(\"shape\", ())\nreturn Bottom(shape)\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Trace:\n\"\"\"&gt; Given a `key: PRNGKey` and arguments `x: Tuple`, the generative\n        function sample a choice map $c \\sim p(\\cdot; x)$, as well as any\n        untraced randomness $r \\sim p(\\cdot; x, c)$ to produce a trace $t = (x,\n        c, r)$.\n        While the types of traces `t` are formally defined by $(x, c, r)$, they will often store additional information - like the _score_ ($s$):\n        $$\n        s = \\log \\\\frac{p(c, r; x)}{q(r; x, c)}\n        $$\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            tr: A trace capturing the data and inference data associated with the generative function invocation.\n        Examples:\n            Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            tr = genjax.normal.simulate(key, (0.0, 1.0))\n            print(console.render(tr))\n            ```\n            Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                y = genjax.normal(x, 1.0) @ \"y\"\n                return y\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            print(console.render(tr))\n            ```\n        \"\"\"\nraise NotImplementedError\ndef propose(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[Any, FloatArray, ChoiceMap]:\n\"\"\"&gt; Given a `key: PRNGKey` and arguments ($x$), execute the generative\n        function, returning a tuple containing the return value from the\n        generative function call, the score ($s$) of the choice map assignment,\n        and the choice map ($c$).\n        The default implementation just calls `simulate`, and then extracts the data from the `Trace` returned by `simulate`. Custom generative functions can overload the implementation for their own uses (e.g. if they don't have an associated `Trace` datatype, but can be used as a proposal).\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            retval: the return value from the generative function invocation\n            s: the score ($s$) of the choice map assignment\n            chm: the choice map assignment ($c$)\n        Examples:\n            Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            (r, w, chm) = genjax.normal.propose(key, (0.0, 1.0))\n            print(console.render(chm))\n            ```\n            Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                y = genjax.normal(x, 1.0) @ \"y\"\n                return y\n            key = jax.random.PRNGKey(314159)\n            (r, w, chm) = model.propose(key, ())\n            print(console.render(chm))\n            ```\n        \"\"\"\ntr = self.simulate(key, args)\nchm = tr.get_choices()\nscore = tr.get_score()\nretval = tr.get_retval()\nreturn (retval, score, chm)\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, Trace]:\n\"\"\"&gt; Given a `key: PRNGKey`, a choice map indicating constraints ($u$),\n        and arguments ($x$), execute the generative function, and return an\n        importance weight estimate of the conditional density evaluated at the\n        non-constrained choices, and a trace whose choice map ($c = u' \u29fa u$) is\n        consistent with the constraints ($u$), with unconstrained choices\n        ($u'$) proposed from an internal proposal.\n        Arguments:\n            key: A `PRNGKey`.\n            chm: A choice map indicating constraints ($u$).\n            args: Arguments to the generative function ($x$).\n        Returns:\n            w: An importance weight.\n            tr: A trace capturing the data and inference data associated with the generative function invocation.\n        The importance weight `w` is given by:\n        $$\n        w = \\log \\\\frac{p(u' \u29fa u, r; x)}{q(u'; u, x)q(r; x, t)}\n        $$\n        \"\"\"\nraise NotImplementedError\ndef update(\nself,\nkey: PRNGKey,\ntrace: Trace,\nnew: ChoiceMap,\ndiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, Trace, ChoiceMap]:\nraise NotImplementedError\ndef assess(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[Any, FloatArray]:\n\"\"\"&gt; Given a `key: PRNGKey`, a complete choice map indicating\n        constraints ($u$) for all choices, and arguments ($x$), execute the\n        generative function, and return the return value of the invocation, and\n        the score of the choice map ($s$).\n        Arguments:\n            key: A `PRNGKey`.\n            chm: A complete choice map indicating constraints ($u$) for all choices.\n            args: Arguments to the generative function ($x$).\n        Returns:\n            retval: The return value from the generative function invocation.\n            score: The score of the choice map.\n        The score ($s$) is given by:\n        $$\n        s = \\log \\\\frac{p(c, r; x)}{q(r; x, c)}\n        $$\n        \"\"\"\nraise NotImplementedError\ndef restore_with_aux(\nself,\ninterface_data: Tuple,\naux: Tuple,\n) -&gt; Trace:\nraise NotImplementedError\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax.core.JAXGenerativeFunction","title":"<code>JAXGenerativeFunction</code>  <code>dataclass</code>","text":"<p>             Bases: <code>GenerativeFunction</code>, <code>Pytree</code></p> <p>A <code>GenerativeFunction</code> subclass for JAX compatible generative functions.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass JAXGenerativeFunction(GenerativeFunction, Pytree):\n\"\"\"A `GenerativeFunction` subclass for JAX compatible generative\n    functions.\"\"\"\n# This is used to support tracing.\n# Below, a default implementation: GenerativeFunctions\n# may customize this to improve compilation time.\ndef __abstract_call__(self, *args) -&gt; Any:\n# This should occur only during abstract evaluation,\n# the fact that the value has type PRNGKey is all that matters.\nkey = jax.random.PRNGKey(0)\ntr = self.simulate(key, args)\nretval = tr.get_retval()\nreturn retval\ndef unzip(\nself,\nkey: PRNGKey,\nfixed: ChoiceMap,\n) -&gt; Tuple[\nCallable[[ChoiceMap, Tuple], FloatArray],\nCallable[[ChoiceMap, Tuple], Any],\n]:\ndef score(differentiable: Tuple, nondifferentiable: Tuple) -&gt; FloatArray:\nprovided, args = tree_zipper(differentiable, nondifferentiable)\nmerged = fixed.safe_merge(provided)\n_, (_, score) = self.assess(key, merged, args)\nreturn score\ndef retval(differentiable: Tuple, nondifferentiable: Tuple) -&gt; Any:\nprovided, args = tree_zipper(differentiable, nondifferentiable)\nmerged = fixed.safe_merge(provided)\n_, (retval, _) = self.assess(key, merged, args)\nreturn retval\nreturn score, retval\n# A higher-level gradient API - it relies upon `unzip`,\n# but provides convenient access to first-order gradients.\ndef choice_grad(self, key, trace, selection):\nfixed = trace.strip().filter(selection.complement())\nchm = trace.strip().filter(selection.filter)\nscorer, _ = self.unzip(key, fixed)\ngrad, nograd = tree_grad_split(\n(chm, trace.get_args()),\n)\nchoice_gradient_tree, _ = jax.grad(scorer)(grad, nograd)\nreturn choice_gradient_tree\n</code></pre>"},{"location":"genjax/library/core/generative.html#traces","title":"Traces","text":"<p>Traces are data structures which record (execution and inference) data about the invocation of generative functions.</p> <p>Traces are often specialized to a generative function language, to take advantage of data locality, and other representation optimizations.</p> <p>Traces support a set of accessor method interfaces designed to provide convenient manipulation when handling traces in inference algorithms.</p>"},{"location":"genjax/library/core/generative.html#genjax.core.Trace","title":"<code>Trace</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code>, <code>AddressTree</code></p> <p>Abstract base class for traces of generative functions.</p> <p>A <code>Trace</code> is a data structure used to represent sampled executions of generative functions.</p> <p>Traces track metadata associated with log probabilities of choices, as well as other data associated with the invocation of a generative function, including the arguments it was invoked with, its return value, and the identity of the generative function itself.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass Trace(ChoiceMap, AddressTree):\n\"\"\"&gt; Abstract base class for traces of generative functions.\n    A `Trace` is a data structure used to represent sampled executions\n    of generative functions.\n    Traces track metadata associated with log probabilities of choices,\n    as well as other data associated with the invocation of a generative\n    function, including the arguments it was invoked with, its return\n    value, and the identity of the generative function itself.\n    \"\"\"\n@abc.abstractmethod\ndef get_retval(self) -&gt; Any:\n\"\"\"Returns the return value from the generative function invocation\n        which created the `Trace`.\n        Examples:\n            Here's an example using `genjax.normal` (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            tr = genjax.normal.simulate(key, (0.0, 1.0))\n            retval = tr.get_retval()\n            chm = tr.get_choices()\n            v = chm.get_leaf_value()\n            print(console.render((retval, v)))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_score(self) -&gt; FloatArray:\n\"\"\"Return the joint log score of the `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            score = tr.get_score()\n            x_score = bernoulli.logpdf(tr[\"x\"], 0.3)\n            y_score = bernoulli.logpdf(tr[\"y\"], 0.3)\n            print(console.render((score, x_score + y_score)))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_args(self) -&gt; Tuple:\npass\n@abc.abstractmethod\ndef get_choices(self) -&gt; ChoiceMap:\n\"\"\"Return a `ChoiceMap` representation of the set of traced random\n        choices sampled during the execution of the generative function to\n        produce the `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            chm = tr.get_choices()\n            print(console.render(chm))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_gen_fn(self) -&gt; \"GenerativeFunction\":\n\"\"\"Returns the generative function whose invocation created the\n        `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            tr = genjax.normal.simulate(key, (0.0, 1.0))\n            gen_fn = tr.get_gen_fn()\n            print(console.render(gen_fn))\n            ```\n        \"\"\"\n@dispatch\ndef project(\nself,\nselection: NoneSelection,\n) -&gt; FloatArray:\nreturn 0.0\n@dispatch\ndef project(\nself,\nselection: AllSelection,\n) -&gt; FloatArray:\nreturn self.get_score()\n@dispatch\ndef project(self, selection: \"Selection\") -&gt; FloatArray:\n\"\"\"Given a `Selection`, return the total contribution to the joint log\n        score of the addresses contained within the `Selection`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            selection = genjax.select(\"x\")\n            x_score = tr.project(selection)\n            x_score_t = genjax.bernoulli.logpdf(tr[\"x\"], 0.3)\n            print(console.render((x_score_t, x_score)))\n            ```\n        \"\"\"\nraise NotImplementedError\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nchoices: ChoiceMap,\nargdiffs: Tuple,\n):\ngen_fn = self.get_gen_fn()\nreturn gen_fn.update(key, self, choices, argdiffs)\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nchoices: ChoiceMap,\n):\ngen_fn = self.get_gen_fn()\nargs = self.get_args()\nargdiffs = tree_diff_no_change(args)\nreturn gen_fn.update(key, self, choices, argdiffs)\ndef get_aux(self) -&gt; Tuple:\nraise NotImplementedError\n#################################\n# Default choice map interfaces #\n#################################\ndef is_empty(self):\nreturn self.strip().is_empty()\ndef filter(\nself,\nselection: Selection,\n) -&gt; ChoiceMap:\nreturn self.strip().filter(selection)\ndef merge(self, other: ChoiceMap) -&gt; Tuple[ChoiceMap, ChoiceMap]:\nreturn self.strip().merge(other.strip())\ndef has_subtree(self, addr) -&gt; BoolArray:\nchoices = self.get_choices()\nreturn choices.has_subtree(addr)\ndef get_subtree(self, addr) -&gt; ChoiceMap:\nchoices = self.get_choices()\nreturn choices.get_subtree(addr)\ndef get_subtrees_shallow(self):\nchoices = self.get_choices()\nreturn choices.get_subtrees_shallow()\ndef get_selection(self):\nreturn self.strip().get_selection()\ndef strip(self):\n\"\"\"Remove all `Trace` metadata, and return a choice map.\n        `ChoiceMap` instances produced by `tr.get_choices()` will preserve `Trace` instances. `strip` recursively calls `get_choices` to remove `Trace` instances.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            tr = genjax.normal.simulate(key, (0.0, 1.0))\n            chm = tr.strip()\n            print(console.render(chm))\n            ```\n        \"\"\"\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self.get_choices(), is_leaf=_check)\n</code></pre>"},{"location":"genjax/library/core/generative.html#choice-maps","title":"Choice maps","text":""},{"location":"genjax/library/core/generative.html#genjax.core.ChoiceMap","title":"<code>ChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AddressTree</code></p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass ChoiceMap(AddressTree, metaclass=LiftedTypeMeta):\n@abc.abstractmethod\ndef is_empty(self) -&gt; BoolArray:\npass\n@abc.abstractmethod\ndef merge(\nself,\nother: \"ChoiceMap\",\n) -&gt; Tuple[\"ChoiceMap\", \"ChoiceMap\"]:\npass\n@dispatch\ndef filter(\nself,\nselection: AllSelection,\n) -&gt; \"ChoiceMap\":\nreturn self\n@dispatch\ndef filter(\nself,\nselection: NoneSelection,\n) -&gt; \"ChoiceMap\":\nreturn EmptyChoiceMap()\n@dispatch\ndef filter(\nself,\nselection: Selection,\n) -&gt; \"ChoiceMap\":\n\"\"\"Filter the addresses in a choice map, returning a new choice map.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            chm = tr.strip()\n            selection = genjax.select(\"x\")\n            filtered = chm.filter(selection)\n            print(console.render(filtered))\n            ```\n        \"\"\"\nraise NotImplementedError\n@dispatch\ndef replace(\nself,\nselection: AllSelection,\nreplacement: \"ChoiceMap\",\n) -&gt; \"ChoiceMap\":\nreturn replacement\n@dispatch\ndef replace(\nself,\nselection: NoneSelection,\nreplacement: \"ChoiceMap\",\n) -&gt; \"ChoiceMap\":\nreturn self\n@dispatch\ndef replace(\nself,\nselection: Selection,\nreplacement: \"ChoiceMap\",\n) -&gt; \"ChoiceMap\":\n\"\"\"Replace the submaps selected by `selection` with `replacement`,\n        returning a new choice map.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            chm = tr.strip()\n            replacement = genjax.choice_map({\"z\": 5.0})\n            selection = genjax.select(\"x\")\n            replaced = chm.replace(selection, replacement)\n            print(console.render(replaced))\n            ```\n        \"\"\"\nraise NotImplementedError\n@dispatch\ndef insert(\nself,\nselection: AllSelection,\nextension: \"ChoiceMap\",\n) -&gt; \"ChoiceMap\":\nreturn extension\n@dispatch\ndef insert(\nself,\nselection: NoneSelection,\nextension: \"ChoiceMap\",\n) -&gt; \"ChoiceMap\":\nreturn self\n@dispatch\ndef insert(\nself,\nselection: Selection,\nextension: \"ChoiceMap\",\n) -&gt; \"ChoiceMap\":\n\"\"\"Extend the submap selected by `selection` with `extension`,\n        returning a new choice map.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            chm = tr.strip()\n            extension = genjax.choice_map({\"z\": 5.0})\n            selection = genjax.select(\"x\")\n            extended = chm.insert(selection, extension)\n            print(console.render(extended))\n            ```\n        \"\"\"\nraise NotImplementedError\ndef get_selection(self) -&gt; \"Selection\":\n\"\"\"Convert a `ChoiceMap` to a `Selection`.\"\"\"\nraise Exception(\nf\"`get_selection` is not implemented for choice map of type {type(self)}\",\n)\ndef safe_merge(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\nnew, discard = self.merge(other)\nassert discard.is_empty()\nreturn new\ndef unsafe_merge(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\nnew, _ = self.merge(other)\nreturn new\ndef get_choices(self):\nreturn self\ndef strip(self):\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self, is_leaf=_check)\n###########\n# Dunders #\n###########\ndef __eq__(self, other):\nreturn self.flatten() == other.flatten()\n# Optional: mutable setter.\ndef __setitem__(self, key, value):\nraise Exception(\nf\"ChoiceMap of type {type(self)} does not implement __setitem__.\",\n)\ndef __add__(self, other):\nreturn self.safe_merge(other)\n###################\n# Pretty printing #\n###################\n# Defines custom pretty printing.\ndef __rich_console__(self, console, options):\ntree = rich.tree.Tree(\"\")\ntree = self.__rich_tree__(tree)\nyield tree\n</code></pre>"},{"location":"genjax/library/core/generative.html#selections","title":"Selections","text":""},{"location":"genjax/library/core/generative.html#genjax.core.Selection","title":"<code>Selection</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AddressTree</code></p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass Selection(AddressTree):\n@abc.abstractmethod\ndef complement(self) -&gt; \"Selection\":\n\"\"\"Return a `Selection` which filters addresses to the complement set\n        of the provided `Selection`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            tr = model.simulate(key, ())\n            chm = tr.strip()\n            selection = genjax.select(\"x\")\n            complement = selection.complement()\n            filtered = chm.filter(complement)\n            print(console.render(filtered))\n            ```\n        \"\"\"\ndef get_selection(self):\nreturn self\ndef get_subtrees_shallow(self):\nraise Exception(\nf\"Selection of type {type(self)} does not implement get_subtrees_shallow.\",\n)\n###########\n# Dunders #\n###########\ndef __getitem__(self, addr):\nsubtree = self.get_subtree(addr)\nreturn subtree\n###################\n# Pretty printing #\n###################\n# Defines custom pretty printing.\ndef __rich_console__(self, console, options):\ntree = rich.tree.Tree(\"\")\ntree = self.__rich_tree__(tree)\nyield tree\n</code></pre>"},{"location":"genjax/library/core/interpreters.html","title":"Interpreters & transforms","text":"<p>User interaction with <code>genjax</code> interpreters</p> <p>Users are not expected to interact with the data structures and functions described below directly. Instead, generative function languages and advanced DSLs can utilize the functionality described here to expose useful functionality.</p> <p>JAX supports transformations of pure, numerical Python programs by staging out interpreters which evaluate <code>Jaxpr</code> representations of programs.</p> <p>The <code>Core</code> module features interpreter infrastructure, and common transforms designed to facilitate certain types of transformations.</p>"},{"location":"genjax/library/core/interpreters.html#contextual-interpreter","title":"Contextual interpreter","text":"<p>A common interpreter idiom in JAX involves overloading desired primitives with context-specific behavior by inheriting from <code>Trace</code> and define the correct methods to process the primitives.</p> <p><code>ContextualInterpreter</code> provides this idiom in GenJAX: this interprter mixes initial style (e.g. the Python program is immediately staged, and then an interpreter walks the <code>Jaxpr</code> representation) with custom <code>Trace</code> and <code>Tracer</code> overloads. </p> <p>This pattern supports a wide range of program transformations, and allows parametrization over the evaluation order of the inner interpreter (e.g. forward evaluation, or CPS).</p> <p>User interaction with <code>ContextualInterpreter</code></p> <p>Users are not expected to interact with this functionality, but we document its implementation for advanced users or those interested in implementing program transformations with JAX.</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context","title":"<code>context</code>","text":"<p>This module contains a transformation infrastructure based on interpreters with stateful contexts and custom primitive handling lookups.</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context.ContextualTracer","title":"<code>ContextualTracer</code>","text":"<p>             Bases: <code>Tracer</code></p> <p>A <code>ContextualTracer</code> encapsulates a single value.</p> Source code in <code>src/genjax/_src/core/interpreters/context.py</code> <pre><code>class ContextualTracer(jc.Tracer):\n\"\"\"A `ContextualTracer` encapsulates a single value.\"\"\"\ndef __init__(self, trace: \"ContextualTrace\", val: Value):\nself._trace = trace\nself.val = val\n@property\ndef aval(self):\nreturn jc.raise_to_shaped(jc.get_aval(self.val))\ndef full_lower(self):\nreturn self\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context.ContextualTrace","title":"<code>ContextualTrace</code>","text":"<p>             Bases: <code>Trace</code></p> <p>An evaluating trace that dispatches to a dynamic context.</p> Source code in <code>src/genjax/_src/core/interpreters/context.py</code> <pre><code>class ContextualTrace(jc.Trace):\n\"\"\"An evaluating trace that dispatches to a dynamic context.\"\"\"\ndef pure(self, val: Value) -&gt; ContextualTracer:\nreturn ContextualTracer(self, val)\ndef sublift(self, tracer: ContextualTracer) -&gt; ContextualTracer:\nreturn self.pure(tracer.val)\ndef lift(self, val: Value) -&gt; ContextualTracer:\nreturn self.pure(val)\ndef process_primitive(\nself,\nprimitive: jc.Primitive,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n) -&gt; Union[ContextualTracer, List[ContextualTracer]]:\ncontext = staging.get_dynamic_context(self)\ncustom_rule = context.get_custom_rule(primitive)\nif custom_rule:\nreturn custom_rule(self, *tracers, **params)\nreturn self.default_process_primitive(primitive, tracers, params)\ndef default_process_primitive(\nself,\nprimitive: jc.Primitive,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n) -&gt; Union[ContextualTracer, List[ContextualTracer]]:\ncontext = staging.get_dynamic_context(self)\nvals = [v.val for v in tracers]\nif context.can_process(primitive):\noutvals = context.process_primitive(primitive, *vals, **params)\nreturn jax_util.safe_map(self.pure, outvals)\nsubfuns, params = primitive.get_bind_params(params)\nargs = subfuns + vals\noutvals = primitive.bind(*args, **params)\nif not primitive.multiple_results:\noutvals = [outvals]\nout_tracers = jax_util.safe_map(self.full_raise, outvals)\nif primitive.multiple_results:\nreturn out_tracers\nreturn out_tracers[0]\ndef process_call(\nself,\ncall_primitive: jc.Primitive,\nf: Any,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_higher_order_primitive(\nself, call_primitive, f, tracers, params, False\n)\ndef post_process_call(self, call_primitive, out_tracers, params):\nvals = tuple(t.val for t in out_tracers)\nmaster = self.main\ndef todo(x):\ntrace = ContextualTrace(master, jc.cur_sublevel())\nreturn jax_util.safe_map(functools.partial(ContextualTracer, trace), x)\nreturn vals, todo\ndef process_map(\nself,\ncall_primitive: jc.Primitive,\nf: Any,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_higher_order_primitive(\nself, call_primitive, f, tracers, params, True\n)\npost_process_map = post_process_call\ndef process_custom_jvp_call(self, primitive, fun, jvp, tracers, *, symbolic_zeros):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_custom_jvp_call(\nself, primitive, fun, jvp, tracers, symbolic_zeros=symbolic_zeros\n)\ndef post_process_custom_jvp_call(self, out_tracers, jvp_was_run):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_jvp_call(self, out_tracers, jvp_was_run)\ndef process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers, out_trees):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_custom_vjp_call(\nself, primitive, fun, fwd, bwd, tracers, out_trees\n)\ndef post_process_custom_vjp_call(self, out_tracers, params):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_vjp_call(self, out_tracers, params)\ndef post_process_custom_vjp_call_fwd(self, out_tracers, out_trees):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_vjp_call_fwd(self, out_tracers, out_trees)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#incremental-computation","title":"Incremental computation","text":""},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental","title":"<code>incremental</code>","text":"<p>This module supports incremental computation using generalized tangents (e.g. <code>ChangeTangent</code> below).</p> <p>By default, <code>genjax</code> provides two types of <code>ChangeTangent</code>:</p> <ul> <li><code>NoChange</code> - indicating that a value has not changed.</li> <li><code>UnknownChange</code> - indicating that a value has changed, without further information about the change.</li> </ul> <p><code>ChangeTangents</code> are provided along with primal values into <code>Diff</code> instances. The generative function <code>update</code> interface expects tuples of <code>Diff</code> instances (<code>argdiffs</code>).</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental.Diff","title":"<code>Diff</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/core/transforms/incremental.py</code> <pre><code>@dataclasses.dataclass\nclass Diff(Pytree):\nprimal: Any\ntangent: Any\ndef flatten(self):\nreturn (self.primal, self.tangent), ()\n@classmethod\ndef new(cls, primal, tangent):\nassert not isinstance(primal, Diff)\nstatic_check_is_change_tangent(tangent)\nreturn Diff(primal, tangent)\ndef get_primal(self):\nreturn self.primal\ndef get_tangent(self):\nreturn self.tangent\ndef get_tracers(self, trace):\n# If we're not in a `DiffTrace` context -\n# we shouldn't try and make DiffTracers.\nif not isinstance(trace, DiffTrace):\nreturn self.primal\nreturn DiffTracer(trace, self.primal, self.tangent)\ndef unpack(self):\nreturn self.primal, self.tangent\n@typecheck\n@classmethod\ndef from_tracer(cls, tracer: DiffTracer):\nif tracer.tangent is None:\ntangent = NoChange\nelse:\ntangent = tracer.tangent\nreturn Diff(tracer.val, tangent)\n@classmethod\ndef inflate(cls, tree, change_tangent):\n\"\"\"Create an instance of `type(tree)` with the same structure as tree,\n        but with all values replaced with `change_tangent`\"\"\"\nreturn jtu.tree_map(lambda _: change_tangent, tree)\n@classmethod\ndef no_change(cls, tree):\nreturn jtu.tree_map(lambda v: Diff.new(v, NoChange), tree)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental.Diff.inflate","title":"<code>inflate(tree, change_tangent)</code>  <code>classmethod</code>","text":"<p>Create an instance of <code>type(tree)</code> with the same structure as tree, but with all values replaced with <code>change_tangent</code></p> Source code in <code>src/genjax/_src/core/transforms/incremental.py</code> <pre><code>@classmethod\ndef inflate(cls, tree, change_tangent):\n\"\"\"Create an instance of `type(tree)` with the same structure as tree,\n    but with all values replaced with `change_tangent`\"\"\"\nreturn jtu.tree_map(lambda _: change_tangent, tree)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental.tree_diff_primal","title":"<code>tree_diff_primal(v)</code>","text":"Source code in <code>src/genjax/_src/core/transforms/incremental.py</code> <pre><code>def tree_diff_primal(v):\ndef _inner(v):\nif static_check_is_diff(v):\nreturn v.get_primal()\nelse:\nreturn v\nreturn jtu.tree_map(lambda v: _inner(v), v, is_leaf=static_check_is_diff)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental.tree_diff_tangent","title":"<code>tree_diff_tangent(v)</code>","text":"Source code in <code>src/genjax/_src/core/transforms/incremental.py</code> <pre><code>def tree_diff_tangent(v):\ndef _inner(v):\nif static_check_is_diff(v):\nreturn v.get_tangent()\nelse:\nreturn v\nreturn jtu.tree_map(lambda v: _inner(v), v, is_leaf=static_check_is_diff)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental.tree_diff_no_change","title":"<code>tree_diff_no_change(tree)</code>","text":"Source code in <code>src/genjax/_src/core/transforms/incremental.py</code> <pre><code>def tree_diff_no_change(tree):\nreturn tree_diff(tree, NoChange)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.incremental.tree_diff_unknown_change","title":"<code>tree_diff_unknown_change(tree)</code>","text":"Source code in <code>src/genjax/_src/core/transforms/incremental.py</code> <pre><code>def tree_diff_unknown_change(tree):\nreturn tree_diff(tree, UnknownChange)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#stateful-computation","title":"Stateful computation","text":"<p><code>harvest</code> from oryx</p> <p>The <code>harvest</code> transformation is from Oryx. GenJAX supports an implementation which essentially matches the <code>Oryx</code> version, customized for a few higher-level tools.</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.harvest","title":"<code>harvest</code>","text":"<p>This module contains a general-purpose set of tools for transforming functions with a specific side-effect mechanism into pure functions. The names of the transformations in this module are inspired by the Sow/Reap mechanism in Wolfram Mathematica.</p> <p>The harvest module exposes two main functions: <code>sow</code> and <code>harvest</code>.</p> <ul> <li><code>sow</code> is used to tag values.</li> <li><code>harvest</code> can inject values into functions or pull out tagged values.</li> </ul> <p><code>harvest</code> is a very general purpose transformation purely focused on converting functions that have special side-effects (defined using <code>sow</code>) and \"functionalizing\" them.</p> <p>Specifically, a function <code>f :: (x: X) -&gt; Y</code> has a set of defined intermediates, or <code>Sows</code>. This set can be divided into intermediates you are \"collecting\" and intermediates you are \"injecting\", or <code>Reaps</code> and <code>Plants</code> respectively. Functionalizing <code>f</code> now gives you <code>harvest(f) :: (plants: Plants, x: X) -&gt; Tuple[Y, Reaps]</code>.</p> <p>Generally, most users will not need to use <code>harvest</code> directly, but will use wrappers around it.</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.harvest.sow","title":"<code>sow(value, *, tag, meta, mode='strict')</code>","text":"<p>Marks a value with a metadata value and a tag.</p> <p><code>sow</code> is the function used to tag values in a function. It takes in a single positional argument, <code>value</code>, which is returned as an output, so <code>sow</code> outside of a tracing context behaves like the identity function, i.e. <code>sow(x, ...) == x</code>. It also takes in two mandatory keyword arguments, <code>tag</code> and <code>name</code>.</p> <ul> <li> <p><code>tag</code> is a string used to namespace intermediate values in a function. For example, some intermediates may be useful for logging or debugging. The tag enables <code>harvest</code> to interact with only one set of intermediates at a time.</p> </li> <li> <p>The <code>name</code> is a string that describes the value you are <code>sow</code>-ing. Eventually, when calling <code>harvest</code> on a function, the <code>name</code> is used as the identifier for the intermediate value.</p> </li> </ul> <p>Finally, <code>sow</code> takes in an optional string keyword argument <code>mode</code>, which is by default set to <code>'strict'</code>. The <code>mode</code> of a <code>sow</code> describes how it behaves when the same name appears multiple times. In \"strict\" mode, <code>sow</code> will error if the same <code>(tag, name)</code> appears more than once. Another option is <code>'append'</code>, in which all sows of the same name will be appended into a growing array. Finally, there is <code>'clobber'</code>, where only the final sown value for a given <code>(tag, name)</code> will be returned. The final optional argument for <code>sow</code> is <code>key</code>, which will automatically be tied-in to the output of <code>sow</code> to introduce a fake data-dependence. By default, it is <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>A JAX value to be tagged and metad.</p> required <code>tag</code> <code>Hashable</code> <p>A <code>String</code> representing the tag of the sown value.</p> required <code>meta</code> <code>Any</code> <p>A piece of metadata to sow the value with.</p> required <code>mode</code> <code>String</code> <p>The mode by which to sow the value. There are three options:</p> <ul> <li> <p><code>'strict'</code> - if another value is sown with the same metadata and tag in the same context, harvest will throw an error.</p> </li> <li> <p><code>'clobber'</code> - if another is value is sown with the same meta and tag, it will replace this value</p> </li> <li> <p><code>'append'</code> - sown values of the same meta and tag are appended to a growing list. Append mode assumes some ordering on the values being sown defined by data-dependence.</p> </li> </ul> <code>'strict'</code> <p>Returns:</p> Name Type Description <code>value</code> <code>Any</code> <p>The original <code>value</code> that was passed in.</p> Source code in <code>src/genjax/_src/core/transforms/harvest.py</code> <pre><code>def sow(\nvalue: Any,\n*,\ntag: Hashable,\nmeta: Any,\nmode: String = \"strict\",\n) -&gt; Any:\n\"\"\"&gt; Marks a value with a metadata value and a tag.\n    `sow` is the function used to tag values in a function. It takes in a single\n    positional argument, `value`, which is returned as an output, so `sow` outside\n    of a tracing context behaves like the identity function, i.e.\n    `sow(x, ...) == x`. It also takes in two mandatory keyword arguments,\n    `tag` and `name`.\n    * `tag` is a string used to namespace intermediate values in a\n    function. For example, some intermediates may be useful\n    for logging or debugging. The tag enables `harvest` to interact with only one set of intermediates at a time.\n    * The `name` is a string that describes the value you are `sow`-ing. Eventually,\n    when calling `harvest` on a function, the `name` is used as the identifier\n    for the intermediate value.\n    Finally, `sow` takes in an optional string keyword argument `mode`, which is by\n    default set to `'strict'`. The `mode` of a `sow` describes how it behaves when\n    the same name appears multiple times. In \"strict\" mode, `sow` will error if the\n    same `(tag, name)` appears more than once. Another option is `'append'`, in\n    which all sows of the same name will be appended into a growing array. Finally,\n    there is `'clobber'`, where only the final sown value for a given `(tag, name)`\n    will be returned. The final optional argument for `sow` is `key`, which will\n    automatically be tied-in to the output of `sow` to introduce a fake\n    data-dependence. By default, it is `None`.\n    Args:\n      value: A JAX value to be tagged and metad.\n      tag: A `String` representing the tag of the sown value.\n      meta: A piece of metadata to sow the value with.\n      mode: The mode by which to sow the value. There are three options:\n        * `'strict'` - if another value is sown with the same metadata and tag in the\n        same context, harvest will throw an error.\n        * `'clobber'` - if another is\n        value is sown with the same meta and tag, it will replace this value\n        * `'append'` - sown values of the same meta and tag are appended to a\n        growing list. Append mode assumes some ordering on the values being sown\n        defined by data-dependence.\n    Returns:\n        value: The original `value` that was passed in.\n    \"\"\"\nvalue = jtu.tree_map(jc.raise_as_much_as_possible, value)\nflat_args, in_tree = jtu.tree_flatten(value)\nout_flat = sow_p.bind(*flat_args, meta=meta, tag=tag, mode=mode, tree=in_tree)\nreturn jtu.tree_unflatten(in_tree, out_flat)\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.transforms.harvest.harvest","title":"<code>harvest(fn, *, tag)</code>","text":"<p><code>harvest</code> is a function transformation that augments the behaviors of <code>sow</code>s in the function body. By default, invoking <code>sow</code> acts as the identity function and does not affect the semantics of a function. Harvesting <code>f</code> produces a function that can take advantage of <code>sow</code>s present in its execution.</p> <p><code>harvest</code> is a function that takes in a function <code>f</code> and a <code>tag: String</code>. <code>harvest</code> will only interact with <code>sow</code>s whose tag matches the input <code>tag</code>. The returned function can interact with the <code>sow</code> invocations in the function body in either of two ways.</p> <ul> <li> <p>The first is via \"injection\", where intermediate values in the function values can be overridden. <code>harvest(f)</code> takes in an additional initial argument, <code>plants</code>, a dictionary mapping names to values. Each name in <code>plants</code> should correspond to a <code>sow</code> in <code>f</code>, and while running <code>harvest(f)</code> rather than using the value at runtime for the <code>sow</code>, we substitute in the value from the <code>plants</code> dictionary.</p> </li> <li> <p>The other way in which <code>harvest(f)</code> interacts with <code>sow</code> invocations is that if it encounters a <code>sow</code> whose tag matches and whose name is not in <code>plants</code>, it will add the output of the <code>sow</code> to a dictionary mapping the sow name to its output, called <code>reaps</code>. The <code>reaps</code> dictionary, at the end of <code>harvest(f)</code>'s execution, will contain the outputs of all <code>sow</code>s whose values were not injected, or \"planted.\".</p> </li> </ul> <p>The general convention is that, for any given execution of <code>harvest(f, tag=tag)</code>, there will be *no more remaining sow invocations of the given tag if the function were to be harvested again, i.e. if we were to nest harvests with the same tag <code>harvest(harvest(f, tag='some_tag'), tag='some_tag')</code>, the outer harvest would have nothing to plant or to reap.</p> Source code in <code>src/genjax/_src/core/transforms/harvest.py</code> <pre><code>def harvest(\nfn: Callable,\n*,\ntag: Hashable,\n):\n\"\"\"&gt; `harvest` is a function transformation that augments the behaviors of\n    `sow`s in the function body. By default, invoking `sow` acts as the\n    identity function and does not affect the semantics of a function.\n    Harvesting `f` produces a function that can take advantage of `sow`s\n    present in its execution.\n    `harvest` is a function that takes in a function\n    `f` and a `tag: String`. `harvest` will only interact with `sow`s whose tag\n    matches the input `tag`. The returned function can interact with the `sow` invocations\n    in the function body in either of two ways.\n    * The first is via \"injection\",\n    where intermediate values in the function values can be overridden.\n    `harvest(f)` takes in an additional initial argument, `plants`, a\n    dictionary mapping names to values. Each name in `plants` should correspond\n    to a `sow` in `f`, and while running `harvest(f)` rather than using the\n    value at runtime for the `sow`, we substitute in the value from the\n    `plants` dictionary.\n    * The other way in which `harvest(f)` interacts with\n    `sow` invocations is that if it encounters a `sow` whose tag matches and whose name is\n    *not* in `plants`, it will add the output of the `sow` to a dictionary\n    mapping the sow name to its output, called `reaps`. The `reaps` dictionary,\n    at the end of `harvest(f)`'s execution, will contain the outputs of all\n    `sow`s whose values were not injected, or \"planted.\".\n    The general convention is that, for any given execution of\n    `harvest(f, tag=tag)`, there will be *no more remaining sow invocations of the\n    given tag if the function were to be harvested again, i.e. if we were to\n    nest harvests with the same tag `harvest(harvest(f, tag='some_tag'),\n    tag='some_tag')`, the outer harvest would have nothing to plant or\n    to reap.\n    \"\"\"\n@functools.wraps(fn)\ndef wrapper(plants, *args, **kwargs):\nf = plant(fn, tag=tag)\nf = reap(f, tag=tag)\nreturn f(plants, *args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#sharp-edges","title":"Sharp edges","text":"<ul> <li><code>harvest</code> has undefined semantics under automatic differentiation. If a function   you're taking the gradient of has a <code>sow</code>, it might produce unintuitive   results when harvested. To better control gradient semantics, you can use   <code>jax.custom_jvp</code> or <code>jax.custom_vjp</code>. The current implementation sows primals   and tangents in the JVP but ignore cotangents in the VJP. These particular   semantics are subject to change.</li> <li>Planting values into a <code>pmap</code> is partially working. Harvest tries to plant all   the values, assuming they have a leading map dimension.</li> </ul>"},{"location":"genjax/library/core/interpreters.html#examples","title":"Examples","text":""},{"location":"genjax/library/core/interpreters.html#using-sow-and-harvest","title":"Using <code>sow</code> and <code>harvest</code>","text":"<pre><code>def f(x):\ny = sow(x + 1., tag='intermediate', name='y')\nreturn y + 1.\n# Injecting, or \"planting\" a value for `y`.\nharvest(f, tag='intermediate')({'y': 0.}, 1.)  # ==&gt; (1., {})\nharvest(f, tag='intermediate')({'y': 0.}, 5.)  # ==&gt; (1., {})\n# Collecting , or \"reaping\" the value of `y`.\nharvest(f, tag='intermediate')({}, 1.)  # ==&gt; (3., {'y': 2.})\nharvest(f, tag='intermediate')({}, 5.)  # ==&gt; (7., {'y': 6.})\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#using-reap-and-plant","title":"Using <code>reap</code> and <code>plant</code>.","text":"<p><code>reap</code> and <code>plant</code> are simple wrappers around <code>harvest</code>. <code>reap</code> only pulls intermediate values without injecting, and <code>plant</code> only injects values without collecting intermediate values.</p> <pre><code>def f(x):\ny = sow(x + 1., tag='intermediate', name='y')\nreturn y + 1.\n# Injecting, or \"planting\" a value for `y`.\nplant(f, tag='intermediate')({'y': 0.}, 1.)  # ==&gt; 1.\nplant(f, tag='intermediate')({'y': 0.}, 5.)  # ==&gt; 1.\n# Collecting , or \"reaping\" the value of `y`.\nreap(f, tag='intermediate')(1.)  # ==&gt; {'y': 2.}\nreap(f, tag='intermediate')(5.)  # ==&gt; {'y': 6.}\n</code></pre>"},{"location":"genjax/library/core/runtime_debugger.html","title":"The runtime debugger","text":"<p>GenJAX features a runtime (1) debugging system implemented via JAX transformations on stateful functions. The debugger will not detect static errors or tracing errors, but can be used to inspect the values flowing through your code.</p> <ol> <li>Here, runtime is used in contrast to JAX tracing time (which is akin to \"compile time\"). The runtime debugger can be used to inspect values of functions which JAX can successfully trace through.</li> </ol>"},{"location":"genjax/library/core/runtime_debugger.html#genjax.core.runtime_debugger","title":"<code>runtime_debugger</code>","text":""},{"location":"genjax/library/core/runtime_debugger.html#genjax.core.runtime_debugger.record_value","title":"<code>record_value(value)</code>","text":"<p>Record a value, allowing the debugger to store it in the debug recording, along with the caller's stack frame information.</p> <p>The user is not expected to use this function, but to instead use the multimethod <code>record</code> below which will dispatch appropriately based on invocation types.</p> Source code in <code>src/genjax/_src/core/runtime_debugger.py</code> <pre><code>@typecheck\ndef record_value(value: typing.Any) -&gt; typing.Any:\n\"\"\"&gt; Record a value, allowing the debugger to store it in the debug\n    recording, along with the caller's stack frame information.\n    The user is not expected to use this function, but to instead use the multimethod `record` below which will dispatch appropriately based on invocation types.\n    \"\"\"\ncaller_frame_info = inspect.stack()[3]\nfile_name = caller_frame_info.filename\nsource_line = caller_frame_info.lineno\nname = caller_frame_info.function\nmodule = \"\"\nframe = Frame(\nfile_name,\nsource_line,\nmodule,\nname,\n)\ntag_with_frame(\n{\"value\": value},\nframe=frame,\n)\nreturn value\n</code></pre>"},{"location":"genjax/library/core/runtime_debugger.html#genjax.core.runtime_debugger.record_call","title":"<code>record_call(f)</code>","text":"<p>Transform a function into a version which records the arguments to its invocation, as well as the return value.</p> <p>The transformed version allows the debugger to store this information in the debug recording, along other debug information, including the definition file, the source line start, the module, and the name of the function.</p> <p>The user is not expected to use this function, but to instead use the multimethod <code>record</code> below which will dispatch appropriately based on invocation types.</p> Source code in <code>src/genjax/_src/core/runtime_debugger.py</code> <pre><code>@typecheck\ndef record_call(f: typing.Callable) -&gt; typing.Callable:\n\"\"\"&gt; Transform a function into a version which records the arguments to its\n    invocation, as well as the return value.\n    &gt; The transformed version allows the debugger to store this\n    &gt; information in the debug recording, along other debug information,\n    &gt; including the definition file, the source line start, the module,\n    &gt; and the name of the function.\n    The user is not expected to use this function, but to instead use the multimethod `record` below which will dispatch appropriately based on invocation types.\n    \"\"\"\n@functools.wraps(f)\ndef wrapper(*args):\nretval = f(*args)\nsourceline_start = inspect.getsourcelines(f)[1]\nmodule = inspect.getmodule(f)\nname = f.__name__\nframe = Frame(\nrepr(module),\nsourceline_start,\nmodule,\nname,\n)\ntag_with_frame(\n{\"args\": args, \"return\": retval},\nframe=frame,\n)\nreturn retval\nreturn wrapper\n</code></pre>"},{"location":"genjax/library/core/runtime_debugger.html#genjax.core.runtime_debugger.record","title":"<code>record(v)</code>","text":"<p>A multimethod which dispatches to <code>record_value</code>, or wraps a <code>JAXGenerativeFunction</code> in <code>DebugCombinator</code> to allow recording information about generative function interface invocations.</p> Source code in <code>src/genjax/core/runtime_debugger.py</code> <pre><code>@dispatch\ndef record(v: Any) -&gt; Any:\n\"\"\"A multimethod which dispatches to `record_value`, or wraps a\n    `JAXGenerativeFunction` in `DebugCombinator` to allow recording information\n    about generative function interface invocations.\"\"\"\nreturn record_value(v)\n</code></pre>"},{"location":"genjax/library/core/runtime_debugger.html#genjax.core.runtime_debugger.tag","title":"<code>tag(name, val)</code>","text":"<p>Tag a value, allowing the debugger to store and return it as state in the <code>DebuggerTags</code> produced by <code>pull</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>genjax.Union[String, genjax.Tuple[String, ...]]</code> <p>A <code>String</code> or <code>Tuple[String, ...]</code> providing a reference name (key, into a <code>Trie</code>) for the tagged value.</p> required <code>val</code> <code>Any</code> <p>The value. Must be a JAX compatible type.</p> required <p>Returns:</p> Name Type Description <code>val</code> <code>Any</code> <p>The value. Must be a JAX compatible type.</p> Source code in <code>src/genjax/_src/core/runtime_debugger.py</code> <pre><code>@typecheck\ndef tag(\nname: typing.Union[typing.String, typing.Tuple[typing.String, ...]],\nval: typing.Any,\n) -&gt; typing.Any:\n\"\"\"Tag a value, allowing the debugger to store and return it as state in\n    the `DebuggerTags` produced by `pull`.\n    Arguments:\n        name: A `String` or `Tuple[String, ...]` providing a reference name (key, into a `Trie`) for the tagged value.\n        val: The value. Must be a JAX compatible type.\n    Returns:\n        val: The value. Must be a JAX compatible type.\n    \"\"\"\nf = functools.partial(\nharvest.sow,\ntag=TAGGING_NAMESPACE,\nmeta=name,\n)\nreturn f(val)\n</code></pre>"},{"location":"genjax/library/core/runtime_debugger.html#genjax.core.runtime_debugger.pull","title":"<code>pull(f)</code>","text":"<p>Transform a function into one which returns a debugger recording and debugger tags.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>A function contain <code>record</code> invocations which will be transformed by the debugger transformation.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>A new function which accepts the same arguments as the original function <code>f</code>, but returns a tuple, where the first element is the original return value, and the second element is a 2-tuple containing a <code>DebuggerRecording</code> instance and a <code>DebuggerTags</code> instance.</p> <p>Examples:</p> <p>Here's an example using pure functions, without generative semantics.</p> SourceResult <pre><code>import jax.numpy as jnp\nimport genjax\nimport genjax.core.runtime_debugger as debug\nconsole = genjax.pretty()\ndef foo(x):\nv = jnp.ones(10) * x\ndebug.record(v)\nz = v / 2\nreturn z\nv, (recording, tags) = debug.pull(foo)(3.0)\nprint(console.render(recording))\n</code></pre> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Runtime debugger recording (follows evaluation order) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 &lt;code block: n1&gt;:8 in foo                                                  \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 recorded values \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 value = Array([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], dtype=float32) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Here's an example where we mix <code>tag</code> and <code>record</code>.</p> SourceResult <pre><code>import jax.numpy as jnp\nimport genjax\nimport genjax.core.runtime_debugger as debug\nconsole = genjax.pretty()\ndef foo(x):\nv = jnp.ones(10) * x\ndebug.record(debug.tag(\"v\", v))\nz = v / 2\nreturn z\nv, (recording, tags) = debug.pull(foo)(3.0)\nprint(console.render(recording))\nprint(console.render(tags))\nprint(console.render(tags[\"v\"]))\n</code></pre> <p><pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Runtime debugger recording (follows evaluation order) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 &lt;code block: n2&gt;:8 in foo                                                  \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 recorded values \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 value = Array([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], dtype=float32) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>DebuggerTags\n\u2514\u2500\u2500 tagged\n    \u2514\u2500\u2500 Trie\n        \u2514\u2500\u2500 :v\n            \u2514\u2500\u2500  f32[10]\n</code></pre> <pre><code>[3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n</code></pre></p> <p>Here's an example using generative functions. Now, <code>debug.record</code> will transform <code>GenerativeFunction</code> instances into <code>debug.DebugCombinator</code>, wrapping <code>debug.record_call</code> around their generative function interface invocations.</p> SourceResult <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\nimport genjax.core.runtime_debugger as debug\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\n@genjax.gen\ndef foo(x):\nv = jnp.ones(10) * x\nx = debug.record(genjax.tfp_normal)(jnp.sum(v), 2.0) @ \"x\"\nreturn x\nv, (recording, tags) = debug.pull(foo.simulate)(key, (3.0, ))\nprint(console.render(recording))\n</code></pre> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Runtime debugger recording (follows evaluation order) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 &lt;module 'genjax._src.generative_functions.distributions.distribution' fro\u2026 \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 recorded values \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502   args = (                                                             \u2502 \u2502\n\u2502 \u2502          \u2502   Array([  73112985, 1939741593], dtype=uint32),            \u2502 \u2502\n\u2502 \u2502          \u2502   (Array(30., dtype=float32), 2.0)                          \u2502 \u2502\n\u2502 \u2502          )                                                             \u2502 \u2502\n\u2502 \u2502 return = DistributionTrace(                                            \u2502 \u2502\n\u2502 \u2502          \u2502   gen_fn=TFPDistribution(                                   \u2502 \u2502\n\u2502 \u2502          \u2502   \u2502   distribution=&lt;class                                   \u2502 \u2502\n\u2502 \u2502          'tensorflow_probability.substrates.jax.distributions.normal.\u2026 \u2502 \u2502\n\u2502 \u2502          \u2502   ),                                                        \u2502 \u2502\n\u2502 \u2502          \u2502   args=(Array(30., dtype=float32), 2.0),                    \u2502 \u2502\n\u2502 \u2502          \u2502   value=Array(29.783539, dtype=float32),                    \u2502 \u2502\n\u2502 \u2502          \u2502   score=Array(-1.6179426, dtype=float32)                    \u2502 \u2502\n\u2502 \u2502          )                                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> Source code in <code>src/genjax/_src/core/runtime_debugger.py</code> <pre><code>@typecheck\ndef pull(\nf: typing.Callable,\n) -&gt; typing.Callable:\n\"\"\"Transform a function into one which returns a debugger recording and\n    debugger tags.\n    Arguments:\n        f: A function contain `record` invocations which will be transformed by the debugger transformation.\n    Returns:\n        callable: A new function which accepts the same arguments as the original function `f`, but returns a tuple, where the first element is the original return value, and the second element is a 2-tuple containing a `DebuggerRecording` instance and a `DebuggerTags` instance.\n    Examples:\n        Here's an example using pure functions, without generative semantics.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax.numpy as jnp\n        import genjax\n        import genjax.core.runtime_debugger as debug\n        console = genjax.pretty()\n        def foo(x):\n            v = jnp.ones(10) * x\n            debug.record(v)\n            z = v / 2\n            return z\n        v, (recording, tags) = debug.pull(foo)(3.0)\n        print(console.render(recording))\n        ```\n        Here's an example where we mix `tag` and `record`.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax.numpy as jnp\n        import genjax\n        import genjax.core.runtime_debugger as debug\n        console = genjax.pretty()\n        def foo(x):\n            v = jnp.ones(10) * x\n            debug.record(debug.tag(\"v\", v))\n            z = v / 2\n            return z\n        v, (recording, tags) = debug.pull(foo)(3.0)\n        print(console.render(recording))\n        print(console.render(tags))\n        print(console.render(tags[\"v\"]))\n        ```\n        Here's an example using generative functions. Now, `debug.record` will transform `GenerativeFunction` instances into `debug.DebugCombinator`, wrapping `debug.record_call` around their generative function interface invocations.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import jax.numpy as jnp\n        import genjax\n        import genjax.core.runtime_debugger as debug\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        @genjax.gen\n        def foo(x):\n            v = jnp.ones(10) * x\n            x = debug.record(genjax.tfp_normal)(jnp.sum(v), 2.0) @ \"x\"\n            return x\n        v, (recording, tags) = debug.pull(foo.simulate)(key, (3.0, ))\n        print(console.render(recording))\n        ```\n    \"\"\"\ndef _collect(f):\nreturn harvest.reap(\nharvest.reap(\nf,\nstate=DebuggerRecording.new(),\ntag=RECORDING_NAMESPACE,\n),\nstate=DebuggerTags.new(),\ntag=TAGGING_NAMESPACE,\n)\ndef wrapped(\n*args: typing.Any,\n**kwargs,\n) -&gt; typing.Tuple[typing.Any, typing.Tuple[DebuggerRecording, DebuggerTags]]:\n(v, recording_state), tagging_state = _collect(f)(*args, **kwargs)\nreturn v, (\nharvest.tree_unreap(recording_state),\nharvest.tree_unreap(tagging_state),\n)\nreturn wrapped\n</code></pre>"},{"location":"genjax/library/extensions/index.html","title":"Extension modules","text":"<p>Because the set of generative function types is extensible, languages which extend Gen's interfaces to new types of generative computation are a natural part of Gen's ecosystem.</p>"},{"location":"genjax/library/extensions/index.html#lazy-loading","title":"Lazy loading","text":"<p>Users who wish to implement extension modules whose functionality depends on external 3rd party packages have a few patterns at their disposal. A user can simply build off <code>genjax</code> in their own repo, implementing their own generative functions, and inference, etc - while relying on 3rd party dependencies as well. This is likely the most common pattern, and should be a happy path for extension.</p> <p>Extension modules which are considered useful and worthy of trunk support (to be bundled and tested with the <code>genjax</code> system) can utilize a lazy loading system:</p>"},{"location":"genjax/library/extensions/index.html#genjax.extras.LazyLoader","title":"<code>LazyLoader</code>","text":"<p>             Bases: <code>ModuleType</code></p> <p>A lazy loading system which allows extension modules to optionally depend on 3rd party dependencies which may be too heavyweight to include as required dependencies for <code>genjax</code> proper.</p> <p>Examples:</p> <p>To utilize the system, the <code>LazyLoader</code> expects that you provide a local name for the module, globals, and the source module. Here's example usage for an extension module utilizing <code>tinygp</code> - we give the lazy loaded module the name <code>tinygp</code>, and tell the loader that the module path is <code>genjax._src.extras.tinygp</code>:</p> <pre><code># tinygp provides Gaussian process model ingredients.\ntinygp = LazyLoader(\n\"tinygp\",\nglobals(),\n\"genjax._src.extras.tinygp\",\n)\n</code></pre> <p>The <code>tinygp</code> and <code>blackjax</code> extension modules rely on this system to implement functionality, while optionally depending on the presence of <code>tinygp</code> and <code>blackjax</code> (both 3rd party dependencies) for usage.</p> SourceResult <pre><code>import jax\nimport genjax\nimport tinygp.kernels as kernels\nconsole = genjax.pretty()\n# Extension module\ntinygp = genjax.extras.tinygp\nkernel_scaled = 4.5 * kernels.ExpSquared(scale=1.5)\nmodel = tinygp.GaussianProcess(kernel_scaled)\nprint(console.render(model))\n</code></pre> <pre><code>GaussianProcess\n\u2514\u2500\u2500 kernel\n    \u2514\u2500\u2500 Product\n        \u251c\u2500\u2500 kernel1\n        \u2502   \u2514\u2500\u2500 Constant\n        \u2502       \u2514\u2500\u2500 value\n        \u2502           \u2514\u2500\u2500 (const) 4.5\n        \u2514\u2500\u2500 kernel2\n            \u2514\u2500\u2500 ExpSquared\n                \u251c\u2500\u2500 scale\n                \u2502   \u2514\u2500\u2500 (const) 1.5\n                \u2514\u2500\u2500 distance\n                    \u2514\u2500\u2500 L2Distance\n</code></pre> Source code in <code>src/genjax/_src/extras/__init__.py</code> <pre><code>class LazyLoader(types.ModuleType):\n\"\"\"&gt; A lazy loading system which allows extension modules to optionally\n    depend on 3rd party dependencies which may be too heavyweight to include as\n    required dependencies for `genjax` proper.\n    Examples:\n        To utilize the system, the `LazyLoader` expects that you provide a local name for the module, globals, and the source module. Here's example usage for an extension module utilizing `tinygp` - we give the lazy loaded module the name `tinygp`, and tell the loader that the module path is `genjax._src.extras.tinygp`:\n        ```python\n        # tinygp provides Gaussian process model ingredients.\n        tinygp = LazyLoader(\n            \"tinygp\",\n            globals(),\n            \"genjax._src.extras.tinygp\",\n        )\n        ```\n        The `tinygp` and `blackjax` extension modules rely on this system to implement functionality, while optionally depending on the presence of `tinygp` and `blackjax` (both 3rd party dependencies) for usage.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        import tinygp.kernels as kernels\n        console = genjax.pretty()\n        # Extension module\n        tinygp = genjax.extras.tinygp\n        kernel_scaled = 4.5 * kernels.ExpSquared(scale=1.5)\n        model = tinygp.GaussianProcess(kernel_scaled)\n        print(console.render(model))\n        ```\n    \"\"\"\ndef __init__(self, local_name, parent_module_globals, name):\nself._local_name = local_name\nself._parent_module_globals = parent_module_globals\nsuper(LazyLoader, self).__init__(name)\ndef _load(self):\ntry:\nmodule = importlib.import_module(self.__name__)\nself._parent_module_globals[self._local_name] = module\nself.__dict__.update(module.__dict__)\nreturn module\nexcept ModuleNotFoundError as e:\ne.add_note(\nf\"(GenJAX) Attempted to load {self._local_name} extension but failed, is it installed in your environment?\"\n)\nraise e\ndef __getattr__(self, item):\nmodule = self._load()\nreturn getattr(module, item)\ndef __dir__(self):\nmodule = self._load()\nreturn dir(module)\n</code></pre>"},{"location":"genjax/library/extensions/index.html#currently-supported-modules","title":"Currently supported modules","text":"<p>Here, we document two current modules:</p> <ul> <li>An extension module which extends the generative function interface to Gaussian processes using functionality from <code>tinygp</code>.</li> <li>An extension module for inference which provides a compatibility layer between generative functions and <code>blackjax</code> for state-of-the-art Hamiltonian Monte Carlo (HMC) and No-U-Turn sampling (NUTS) inference.</li> </ul>"},{"location":"genjax/library/extensions/blackjax.html","title":"HMC/NUTS from <code>blackjax</code>","text":""},{"location":"genjax/library/extensions/tinygp.html","title":"Gaussian processes from <code>tinygp</code>","text":""},{"location":"genjax/library/generative_functions/index.html","title":"Generative function languages","text":""},{"location":"genjax/library/generative_functions/index.html#genjax._src.generative_functions","title":"<code>generative_functions</code>","text":"<p>This module contains several standard generative function classes useful for structuring probabilistic programs.</p> <ul> <li>The <code>distributions</code> module exports standard distributions from several sources, including SciPy (<code>scipy</code>), TensorFlow Probability Distributions (<code>tfd</code>), and custom distributions.<ul> <li>The <code>distributions</code> module also contains a small <code>oryx</code>-like language called <code>coryx</code> which implements the generative function interface for programs with inverse log determinant Jacobian (ildj) compatible return value functions of distribution random choices.</li> <li>The <code>distributions</code> module also contains an implementation of <code>gensp</code>, a research language for probabilistic programming with estimated densities.</li> </ul> </li> <li>The <code>builtin</code> module contains a function-like language for defining generative functions from programs.</li> <li>The <code>combinators</code> module contains combinators which support transforming generative functions into new ones with structured control flow patterns of computation.</li> </ul>"},{"location":"genjax/library/generative_functions/builtin.html","title":"Builtin language","text":"<p>This module provides a function-like modeling language. The generative function interfaces are implemented for objects in this language using transformations by JAX interpreters.</p> <p>The language also exposes a set of JAX primitives which allow hierarchical construction of generative programs. These programs can utilize other generative functions inside of a new JAX primitive (<code>trace</code>) to create hierarchical patterns of generative computation.</p>"},{"location":"genjax/library/generative_functions/builtin.html#usage","title":"Usage","text":"<p>The <code>Builtin</code> language is a common foundation for constructing models. It exposes a DSL based on JAX primitives and transformations which allows the programmer to construct generative functions out of Python functions. </p> <p>Below, we illustrate a simple example:</p> <pre><code>from genjax import beta \nfrom genjax import bernoulli \nfrom genjax import uniform \nfrom genjax import gen\n@genjax.gen\ndef beta_bernoulli_process(u):\np = beta(0, u) @ \"p\"\nv = bernoulli(p) @ \"v\"\nreturn v\n@genjax.gen\ndef joint():\nu = uniform() @ \"u\"\nv = beta_bernoulli_process(u) @ \"bbp\"\nreturn v\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#language-primitives","title":"Language primitives","text":"<p>The builtin language exposes custom primitives, which are handled by JAX interpreters to support the semantics of the generative function interface.</p>"},{"location":"genjax/library/generative_functions/builtin.html#trace","title":"<code>trace</code>","text":"<p>The <code>trace</code> primitive provides access to the ability to invoke another generative function as a callee. </p> <p>Returning to our example above:</p> SourceResult <pre><code>import genjax\nfrom genjax import beta \nfrom genjax import bernoulli \nfrom genjax import gen\n@gen\ndef beta_bernoulli_process(u):\n# Invoking `trace` can be sweetened, or unsweetened.\np = genjax.trace(\"p\", beta)(0, u) # not sweet\nv = bernoulli(p) @ \"v\" # sweet\nreturn v\n</code></pre> <p>Now, programs written in the DSL which utilize <code>trace</code> have generative function interface method implementations which store callee choice data in the trace:</p> SourceResult <pre><code>import jax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\ntr = beta_bernoulli_process.simulate(key, (2, ))\nprint(console.render(tr))\n</code></pre> <pre><code>\u2514\u2500\u2500 BuiltinTrace\n    \u251c\u2500\u2500 gen_fn\n    \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502       \u2514\u2500\u2500 source\n    \u2502           \u2514\u2500\u2500 &lt;function beta_bernoulli_process&gt;\n    \u251c\u2500\u2500 args\n    \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2514\u2500\u2500 (const) 2\n    \u251c\u2500\u2500 retval\n    \u2502   \u2514\u2500\u2500  bool[]\n    \u251c\u2500\u2500 choices\n    \u2502   \u2514\u2500\u2500 Trie\n    \u2502       \u251c\u2500\u2500 :v\n    \u2502       \u2502   \u2514\u2500\u2500 DistributionTrace\n    \u2502       \u2502       \u251c\u2500\u2500 gen_fn\n    \u2502       \u2502       \u2502   \u2514\u2500\u2500 Bernoulli\n    \u2502       \u2502       \u251c\u2500\u2500 args\n    \u2502       \u2502       \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502       \u2502       \u2514\u2500\u2500  f32[]\n    \u2502       \u2502       \u251c\u2500\u2500 value\n    \u2502       \u2502       \u2502   \u2514\u2500\u2500  bool[]\n    \u2502       \u2502       \u2514\u2500\u2500 score\n    \u2502       \u2502           \u2514\u2500\u2500  f32[]\n    \u2502       \u2514\u2500\u2500 :p\n    \u2502           \u2514\u2500\u2500 DistributionTrace\n    \u2502               \u251c\u2500\u2500 gen_fn\n    \u2502               \u2502   \u2514\u2500\u2500 Beta\n    \u2502               \u251c\u2500\u2500 args\n    \u2502               \u2502   \u2514\u2500\u2500 tuple\n    \u2502               \u2502       \u251c\u2500\u2500 (const) 0\n    \u2502               \u2502       \u2514\u2500\u2500 (const) 2\n    \u2502               \u251c\u2500\u2500 value\n    \u2502               \u2502   \u2514\u2500\u2500  f32[]\n    \u2502               \u2514\u2500\u2500 score\n    \u2502                   \u2514\u2500\u2500  f32[]\n    \u251c\u2500\u2500 cache\n    \u2502   \u2514\u2500\u2500 Trie\n    \u2514\u2500\u2500 score\n        \u2514\u2500\u2500  f32[]\n</code></pre> <p>Notice how the rendered result <code>Trace</code> has addresses in its choice trie for <code>\"p\"</code> and <code>\"v\"</code> - corresponding to the invocation of the beta and Bernoulli distribution generative functions.</p> <p>The <code>trace</code> primitive is a critical element of structuring hierarchical generative computation in the builtin language.</p>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.trace","title":"<code>trace(addr, gen_fn, **kwargs)</code>","text":"<p>Invoke a generative function, binding its generative semantics with the current caller.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>Any</code> <p>An address denoting the site of a generative function invocation.</p> required <code>gen_fn</code> <code>GenerativeFunction</code> <p>A generative function invoked as a callee of <code>BuiltinGenerativeFunction</code>.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>A callable which wraps the <code>trace_p</code> primitive, accepting arguments (<code>args</code>) and binding the primitive with them. This raises the primitive to be handled by <code>BuiltinGenerativeFunction</code> transformations.</p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_primitives.py</code> <pre><code>@typecheck\ndef trace(addr: Any, gen_fn: GenerativeFunction, **kwargs) -&gt; Callable:\n\"\"\"Invoke a generative function, binding its generative semantics with the\n    current caller.\n    Arguments:\n        addr: An address denoting the site of a generative function invocation.\n        gen_fn: A generative function invoked as a callee of `BuiltinGenerativeFunction`.\n    Returns:\n        callable: A callable which wraps the `trace_p` primitive, accepting arguments (`args`) and binding the primitive with them. This raises the primitive to be handled by `BuiltinGenerativeFunction` transformations.\n    \"\"\"\nassert isinstance(gen_fn, GenerativeFunction)\nstatic_address_type_check(addr)\nreturn lambda *args: _trace(gen_fn, addr, *args, **kwargs)\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#cache","title":"<code>cache</code>","text":"<p>The <code>cache</code> primitive is designed to expose a space vs. time trade-off for incremental computation in Gen's <code>update</code> interface.</p>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.cache","title":"<code>cache(addr, fn, *args, **kwargs)</code>","text":"<p>Invoke a generative function, binding its generative semantics with the current caller.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>Any</code> <p>An address denoting the site of a function invocation.</p> required <code>fn</code> <code>Callable</code> <p>A deterministic function whose return value is cached under the arguments (memoization) inside <code>BuiltinGenerativeFunction</code> traces.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>A callable which wraps the <code>cache_p</code> primitive, accepting arguments (<code>args</code>) and binding the primitive with them. This raises the primitive to be handled by <code>BuiltinGenerativeFunction</code> transformations.</p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_primitives.py</code> <pre><code>@typecheck\ndef cache(addr: Any, fn: Callable, *args: Any, **kwargs) -&gt; Callable:\n\"\"\"Invoke a generative function, binding its generative semantics with the\n    current caller.\n    Arguments:\n        addr: An address denoting the site of a function invocation.\n        fn: A deterministic function whose return value is cached under the arguments (memoization) inside `BuiltinGenerativeFunction` traces.\n    Returns:\n        callable: A callable which wraps the `cache_p` primitive, accepting arguments (`args`) and binding the primitive with them. This raises the primitive to be handled by `BuiltinGenerativeFunction` transformations.\n    \"\"\"\n# fn must be deterministic.\nassert not isinstance(fn, GenerativeFunction)\nstatic_address_type_check(addr)\nreturn lambda *args: _cache(fn, addr, *args, **kwargs)\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#generative-datatypes","title":"Generative datatypes","text":"<p>The builtin language implements a trie-like trace.</p>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.BuiltinTrace","title":"<code>BuiltinTrace</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Trace</code>, <code>SupportsPickleSerialization</code></p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_datatypes.py</code> <pre><code>@dataclass\nclass BuiltinTrace(\nTrace,\nSupportsPickleSerialization,\n):\ngen_fn: GenerativeFunction\nargs: Tuple\nretval: Any\nchoices: Trie\ncache: Trie\nscore: FloatArray\ndef flatten(self):\nreturn (\nself.gen_fn,\nself.args,\nself.retval,\nself.choices,\nself.cache,\nself.score,\n), ()\n@typecheck\n@classmethod\ndef new(\ncls,\ngen_fn: GenerativeFunction,\nargs: Tuple,\nretval: Any,\nchoices: Trie,\ncache: Trie,\nscore: FloatArray,\n):\nreturn BuiltinTrace(gen_fn, args, retval, choices, cache, score)\ndef get_gen_fn(self):\nreturn self.gen_fn\ndef get_choices(self):\nreturn HierarchicalChoiceMap(self.choices)\ndef get_retval(self):\nreturn self.retval\ndef get_score(self):\nreturn self.score\ndef get_args(self):\nreturn self.args\n@dispatch\ndef project(\nself,\nselection: HierarchicalSelection,\n) -&gt; FloatArray:\nweight = 0.0\nfor (k, subtrace) in self.choices.get_subtrees_shallow():\nif selection.has_subtree(k):\nweight += subtrace.project(selection.get_subtree(k))\nreturn weight\ndef has_cached_value(self, addr):\nreturn self.cache.has_subtree(addr)\ndef get_cached_value(self, addr):\nreturn self.cache.get_subtree(addr)\ndef get_aux(self):\nreturn (self.cache,)\n#################\n# Serialization #\n#################\n@dispatch\ndef dumps(\nself,\nbackend: PickleSerializationBackend,\n) -&gt; PickleDataFormat:\nargs, retval, score = self.args, self.retval, self.score\nchoices_payload = []\naddr_payload = []\nfor (addr, subtrace) in self.choices.get_subtrees_shallow():\ninner_payload = subtrace.dumps(backend)\nchoices_payload.append(inner_payload)\naddr_payload.append(addr)\npayload = [\nbackend.dumps(args),\nbackend.dumps(retval),\nbackend.dumps(score),\nbackend.dumps(addr_payload),\nbackend.dumps(choices_payload),\n]\nreturn PickleDataFormat(payload)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/index.html","title":"Generative function combinators","text":""},{"location":"genjax/library/generative_functions/combinators/index.html#genjax._src.generative_functions.combinators","title":"<code>combinators</code>","text":"<p>The <code>combinators</code> module exposes generative function combinators, generative functions which accept other generative functions as configuration arguments, and implement structured patterns of control flow (as well as other types of modifications) as their generative function interface implementations.</p> <p>GenJAX features several standard combinators:</p> <ul> <li><code>UnfoldCombinator</code> - which exposes a scan-like pattern for generative computation in a state space pattern via implementations utilizing <code>jax.lax.scan</code>.</li> <li><code>MapCombinator</code> - which exposes generative vectorization over input arguments, whose implementation utilizes <code>jax.vmap</code>.</li> <li><code>SwitchCombinator</code> - which exposes stochastic branching patterns utilizing <code>jax.lax.switch</code>.</li> </ul>"},{"location":"genjax/library/generative_functions/combinators/map.html","title":"Map combinator","text":"<p>GenJAX's <code>MapCombinator</code> is a combinator which exposes vectorization to the input arguments of generative functions.</p>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax.generative_functions.combinators.MapCombinator","title":"<code>MapCombinator</code>  <code>dataclass</code>","text":"<p>             Bases: <code>JAXGenerativeFunction</code>, <code>SupportsBuiltinSugar</code></p> <p><code>MapCombinator</code> accepts a generative function as input and provides <code>vmap</code>-based implementations of the generative function interface methods.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef add_normal_noise(x):\nnoise1 = genjax.normal(0.0, 1.0) @ \"noise1\"\nnoise2 = genjax.normal(0.0, 1.0) @ \"noise2\"\nreturn x + noise1 + noise2\n# Creating a `MapCombinator` via the preferred `new` class method.\nmapped = genjax.MapCombinator.new(add_normal_noise, in_axes=(0,))\nkey = jax.random.PRNGKey(314159)\narr = jnp.ones(100)\ntr = jax.jit(genjax.simulate(mapped))(key, (arr, ))\nprint(console.render(tr))\n</code></pre> <pre><code>\u2514\u2500\u2500 MapTrace\n    \u251c\u2500\u2500 gen_fn\n    \u2502   \u2514\u2500\u2500 MapCombinator\n    \u2502       \u251c\u2500\u2500 in_axes\n    \u2502       \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502       \u2514\u2500\u2500 (const) 0\n    \u2502       \u2514\u2500\u2500 kernel\n    \u2502           \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502               \u2514\u2500\u2500 source\n    \u2502                   \u2514\u2500\u2500 &lt;function add_normal_noise&gt;\n    \u251c\u2500\u2500 inner\n    \u2502   \u2514\u2500\u2500 BuiltinTrace\n    \u2502       \u251c\u2500\u2500 gen_fn\n    \u2502       \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502       \u2502       \u2514\u2500\u2500 source\n    \u2502       \u2502           \u2514\u2500\u2500 &lt;function add_normal_noise&gt;\n    \u2502       \u251c\u2500\u2500 args\n    \u2502       \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502       \u2514\u2500\u2500  f32[100]\n    \u2502       \u251c\u2500\u2500 retval\n    \u2502       \u2502   \u2514\u2500\u2500  f32[100]\n    \u2502       \u251c\u2500\u2500 choices\n    \u2502       \u2502   \u2514\u2500\u2500 Trie\n    \u2502       \u2502       \u251c\u2500\u2500 :noise1\n    \u2502       \u2502       \u2502   \u2514\u2500\u2500 DistributionTrace\n    \u2502       \u2502       \u2502       \u251c\u2500\u2500 gen_fn\n    \u2502       \u2502       \u2502       \u2502   \u2514\u2500\u2500 Normal\n    \u2502       \u2502       \u2502       \u251c\u2500\u2500 args\n    \u2502       \u2502       \u2502       \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502       \u2502       \u2502       \u251c\u2500\u2500  f32[100]\n    \u2502       \u2502       \u2502       \u2502       \u2514\u2500\u2500  f32[100]\n    \u2502       \u2502       \u2502       \u251c\u2500\u2500 value\n    \u2502       \u2502       \u2502       \u2502   \u2514\u2500\u2500  f32[100]\n    \u2502       \u2502       \u2502       \u2514\u2500\u2500 score\n    \u2502       \u2502       \u2502           \u2514\u2500\u2500  f32[100]\n    \u2502       \u2502       \u2514\u2500\u2500 :noise2\n    \u2502       \u2502           \u2514\u2500\u2500 DistributionTrace\n    \u2502       \u2502               \u251c\u2500\u2500 gen_fn\n    \u2502       \u2502               \u2502   \u2514\u2500\u2500 Normal\n    \u2502       \u2502               \u251c\u2500\u2500 args\n    \u2502       \u2502               \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502               \u2502       \u251c\u2500\u2500  f32[100]\n    \u2502       \u2502               \u2502       \u2514\u2500\u2500  f32[100]\n    \u2502       \u2502               \u251c\u2500\u2500 value\n    \u2502       \u2502               \u2502   \u2514\u2500\u2500  f32[100]\n    \u2502       \u2502               \u2514\u2500\u2500 score\n    \u2502       \u2502                   \u2514\u2500\u2500  f32[100]\n    \u2502       \u251c\u2500\u2500 cache\n    \u2502       \u2502   \u2514\u2500\u2500 Trie\n    \u2502       \u2514\u2500\u2500 score\n    \u2502           \u2514\u2500\u2500  f32[100]\n    \u251c\u2500\u2500 args\n    \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2514\u2500\u2500  f32[100]\n    \u251c\u2500\u2500 retval\n    \u2502   \u2514\u2500\u2500  f32[100]\n    \u2514\u2500\u2500 score\n        \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/map_combinator.py</code> <pre><code>@dataclass\nclass MapCombinator(JAXGenerativeFunction, SupportsBuiltinSugar):\n\"\"\"&gt; `MapCombinator` accepts a generative function as input and provides\n    `vmap`-based implementations of the generative function interface methods.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import jax.numpy as jnp\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def add_normal_noise(x):\n            noise1 = genjax.normal(0.0, 1.0) @ \"noise1\"\n            noise2 = genjax.normal(0.0, 1.0) @ \"noise2\"\n            return x + noise1 + noise2\n        # Creating a `MapCombinator` via the preferred `new` class method.\n        mapped = genjax.MapCombinator.new(add_normal_noise, in_axes=(0,))\n        key = jax.random.PRNGKey(314159)\n        arr = jnp.ones(100)\n        tr = jax.jit(genjax.simulate(mapped))(key, (arr, ))\n        print(console.render(tr))\n        ```\n    \"\"\"\nin_axes: Tuple\nkernel: JAXGenerativeFunction\ndef flatten(self):\nreturn (self.kernel,), (self.in_axes,)\n@typecheck\n@classmethod\ndef new(\ncls,\nkernel: JAXGenerativeFunction,\nin_axes: Tuple,\n) -&gt; \"MapCombinator\":\n\"\"\"The preferred constructor for `MapCombinator` generative function\n        instances. The shorthand symbol is `Map = MapCombinator.new`.\n        Arguments:\n            kernel: A single `JAXGenerativeFunction` instance.\n            in_axes: A tuple specifying which `args` to broadcast over.\n        Returns:\n            instance: A `MapCombinator` instance.\n        \"\"\"\nreturn MapCombinator(in_axes, kernel)\ndef __abstract_call__(self, *args) -&gt; Any:\nreturn jax.vmap(self.kernel.__abstract_call__, in_axes=self.in_axes)(*args)\ndef _static_check_broadcastable(self, args):\n# Argument broadcast semantics must be fully specified\n# in `in_axes`.\nif not len(args) == len(self.in_axes):\nraise Exception(\nf\"MapCombinator requires that length of the provided in_axes kwarg match the number of arguments provided to the invocation.\\nA mismatch occured with len(args) = {len(args)} and len(self.in_axes) = {len(self.in_axes)}\"\n)\ndef _static_broadcast_dim_length(self, args):\ndef find_axis_size(axis, x):\nif axis is not None:\nleaves = jax.tree_util.tree_leaves(x)\nif leaves:\nreturn leaves[0].shape[axis]\nreturn ()\naxis_sizes = jax.tree_util.tree_map(find_axis_size, self.in_axes, args)\naxis_sizes = set(jax.tree_util.tree_leaves(axis_sizes))\nif len(axis_sizes) == 1:\n(d_axis_size,) = axis_sizes\nelse:\nraise ValueError(f\"Inconsistent batch axis sizes: {axis_sizes}\")\nreturn d_axis_size\n@typecheck\ndef get_trace_type(\nself,\n*args,\n) -&gt; TraceType:\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nkernel_tt = self.kernel.get_trace_type(*args)\nreturn VectorTraceType(kernel_tt, broadcast_dim_length)\n@typecheck\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; MapTrace:\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nsub_keys = jax.random.split(key, broadcast_dim_length)\ntr = jax.vmap(self.kernel.simulate, in_axes=(0, self.in_axes))(sub_keys, args)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = MapTrace(self, tr, args, retval, jnp.sum(scores))\nreturn map_tr\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: VectorChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, MapTrace]:\ndef _importance(key, chm, args):\nreturn self.kernel.importance(key, chm, args)\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nsub_keys = jax.random.split(key, broadcast_dim_length)\ninner = chm.inner\n(w, tr) = jax.vmap(_importance, in_axes=(0, 0, self.in_axes))(\nsub_keys, inner, args\n)\nw = jnp.sum(w)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = MapTrace(self, tr, args, retval, jnp.sum(scores))\nreturn (w, map_tr)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: IndexChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, MapTrace]:\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindex_array = jnp.arange(0, broadcast_dim_length)\nsub_keys = jax.random.split(key, broadcast_dim_length)\ndef _importance(key, index, chm, args):\nsubmap = chm.get_subtree(index)\nreturn self.kernel.importance(key, submap, args)\n(w, tr) = jax.vmap(_importance, in_axes=(0, 0, None, self.in_axes))(\nsub_keys, index_array, chm, args\n)\nw = jnp.sum(w)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = MapTrace(self, tr, args, retval, jnp.sum(scores))\nreturn (w, map_tr)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: EmptyChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, MapTrace]:\nmap_tr = self.simulate(key, args)\nw = 0.0\nreturn (w, map_tr)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: HierarchicalChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, MapTrace]:\nindchm = IndexChoiceMap.convert(chm)\nreturn self.importance(key, indchm, args)\n@dispatch\ndef maybe_restore_arguments_kernel_update(\nself,\nkey: PRNGKey,\nprev: DropArgumentsTrace,\nsubmap: ChoiceMap,\noriginal_arguments: Tuple,\nargdiffs: Tuple,\n):\nrestored = prev.restore(original_arguments)\nreturn self.kernel.update(key, restored, submap, argdiffs)\n@dispatch\ndef maybe_restore_arguments_kernel_update(\nself,\nkey: PRNGKey,\nprev: Trace,\nsubmap: ChoiceMap,\noriginal_arguments: Tuple,\nargdiffs: Tuple,\n):\nreturn self.kernel.update(key, prev, submap, argdiffs)\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nprev: MapTrace,\nchm: IndexChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, MapTrace, ChoiceMap]:\nargs = tree_diff_primal(argdiffs)\noriginal_args = prev.get_args()\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindex_array = jnp.arange(0, broadcast_dim_length)\nsub_keys = jax.random.split(key, broadcast_dim_length)\ninner_trace = prev.inner\n@typecheck\ndef _update_inner(\nkey: PRNGKey,\nindex: IntArray,\nprev: Trace,\nchm: ChoiceMap,\noriginal_args: Tuple,\nargdiffs: Tuple,\n):\nsubmap = chm.get_subtree(index)\nreturn self.maybe_restore_arguments_kernel_update(\nkey, prev, submap, original_args, argdiffs\n)\n(retval_diff, w, tr, discard) = jax.vmap(\n_update_inner,\nin_axes=(0, 0, 0, None, self.in_axes, self.in_axes),\n)(sub_keys, index_array, inner_trace, chm, original_args, argdiffs)\nw = jnp.sum(w)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = MapTrace(self, tr, args, retval, jnp.sum(scores))\ndiscard = VectorChoiceMap(discard)\nreturn (retval_diff, w, map_tr, discard)\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nprev: MapTrace,\nchm: VectorChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, MapTrace, ChoiceMap]:\nargs = tree_diff_primal(argdiffs)\noriginal_args = prev.get_args()\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nprev_inaxes_tree = jtu.tree_map(\nlambda v: None if v.shape == () else 0, prev.inner\n)\nsub_keys = jax.random.split(key, broadcast_dim_length)\n(retval_diff, w, tr, discard) = jax.vmap(\nself.maybe_restore_arguments_kernel_update,\nin_axes=(0, prev_inaxes_tree, 0, self.in_axes, self.in_axes),\n)(sub_keys, prev.inner, chm.inner, original_args, argdiffs)\nw = jnp.sum(w)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = MapTrace(self, tr, args, retval, jnp.sum(scores))\ndiscard = VectorChoiceMap(discard)\nreturn (retval_diff, w, map_tr, discard)\n# The choice map passed in here is empty, but perhaps\n# the arguments have changed.\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nprev: MapTrace,\nchm: EmptyChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, MapTrace, ChoiceMap]:\nprev_inaxes_tree = jtu.tree_map(\nlambda v: None if v.shape == () else 0, prev.inner\n)\nargs = tree_diff_primal(argdiffs)\noriginal_args = prev.get_args()\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nsub_keys = jax.random.split(key, broadcast_dim_length)\n(retval_diff, w, tr, discard) = jax.vmap(\nself.maybe_restore_arguments_kernel_update,\nin_axes=(0, prev_inaxes_tree, 0, self.in_axes, self.in_axes),\n)(sub_keys, prev.inner, chm, original_args, argdiffs)\nw = jnp.sum(w)\nretval = tr.get_retval()\nmap_tr = MapTrace(self, tr, args, retval, jnp.sum(tr.get_score()))\nreturn (retval_diff, w, map_tr, discard)\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nprev: MapTrace,\nchm: ChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, MapTrace, ChoiceMap]:\nmaybe_idx_chm = IndexChoiceMap.convert(chm)\nreturn self.update(key, prev, maybe_idx_chm, argdiffs)\ndef _optional_index_check(\nself,\ncheck: BoolArray,\ntruth: IntArray,\nindex: IntArray,\n):\ndef _check():\ncheckify.check(\nnot np.all(check),\nf\"\\nMapCombinator {self} received a choice map with mismatched indices in assess.\\nReference:\\n{truth}\\nPassed in:\\n{index}\",\n)\nglobal_options.optional_check(_check)\n@typecheck\ndef assess(\nself,\nkey: PRNGKey,\nchm: VectorChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[Any, FloatArray]:\nself._static_check_broadcastable(args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindices = jnp.array([i for i in range(0, broadcast_dim_length)])\ncheck = jnp.count_nonzero(indices - chm.get_index()) == 0\n# This inserts a `checkify.check` for bounds checking.\n# If there is an index failure, `assess` must fail\n# because we must provide a constraint for every generative\n# function call.\nself._optional_index_check(check, indices, chm.get_index())\ninner = chm.inner\nsub_keys = jax.random.split(key, broadcast_dim_length)\n(retval, score) = jax.vmap(self.kernel.assess, in_axes=(0, 0, self.in_axes))(\nsub_keys, inner, args\n)\nreturn (retval, jnp.sum(score))\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/map.html#choice-maps-for-map","title":"Choice maps for <code>Map</code>","text":"<p>(This section is also mirrored for <code>UnfoldCombinator</code>)</p> <p><code>Map</code> produces <code>VectorChoiceMap</code> instances (a type of choice map shared with <code>UnfoldCombinator</code>).</p> <p>To utilize <code>importance</code>, <code>update</code>, or <code>assess</code> with <code>Map</code>, it suffices to provide either a <code>VectorChoiceMap</code> for constraints, or an <code>IndexChoiceMap</code>. </p>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax.generative_functions.combinators.VectorChoiceMap","title":"<code>VectorChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass VectorChoiceMap(ChoiceMap):\ninner: Union[ChoiceMap, Trace]\ndef flatten(self):\nreturn (self.inner,), ()\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: EmptyChoiceMap,\n) -&gt; EmptyChoiceMap:\nreturn inner\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: ChoiceMap,\n) -&gt; ChoiceMap:\n# Static assertion: all leaves must have same first dim size.\nstatic_check_tree_leaves_have_matching_leading_dim(inner)\nreturn VectorChoiceMap(inner)\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: Dict,\n) -&gt; ChoiceMap:\nchm = choice_map(inner)\nreturn VectorChoiceMap.new(chm)\ndef is_empty(self):\nreturn self.inner.is_empty()\n@dispatch\ndef filter(\nself,\nselection: HierarchicalSelection,\n) -&gt; ChoiceMap:\nreturn VectorChoiceMap.new(self.inner.filter(selection))\n@dispatch\ndef filter(\nself,\nselection: IndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner)\nflags = jnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\n@dispatch\ndef filter(\nself,\nselection: ComplementIndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner.complement())\nflags = jnp.logical_not(\njnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\ndef get_selection(self):\nsubselection = self.inner.get_selection()\nreturn subselection\ndef has_subtree(self, addr):\nreturn self.inner.has_subtree(addr)\ndef get_subtree(self, addr):\nreturn self.inner.get_subtree(addr)\ndef get_subtrees_shallow(self):\nreturn self.inner.get_subtrees_shallow()\n@dispatch\ndef merge(self, other: \"VectorChoiceMap\") -&gt; Tuple[ChoiceMap, ChoiceMap]:\nnew, discard = self.inner.merge(other.inner)\nreturn VectorChoiceMap(new), VectorChoiceMap(discard)\n@dispatch\ndef merge(self, other: EmptyChoiceMap) -&gt; Tuple[ChoiceMap, ChoiceMap]:\nreturn self, other\n###########\n# Dunders #\n###########\ndef __hash__(self):\nreturn hash(self.inner)\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\nsub_tree = rich.tree.Tree(\"[bold](Vector)\")\nself.inner.__rich_tree__(sub_tree)\ntree.add(sub_tree)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax.generative_functions.combinators.IndexChoiceMap","title":"<code>IndexChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass IndexChoiceMap(ChoiceMap):\nindices: IntArray\ninner: ChoiceMap\ndef flatten(self):\nreturn (self.indices, self.inner), ()\n@classmethod\ndef convert(cls, chm: ChoiceMap) -&gt; \"IndexChoiceMap\":\nindices = []\nsubtrees = []\nfor (k, v) in chm.get_subtrees_shallow():\nif isinstance(k, IntArray):\nindices.append(k)\nsubtrees.append(v)\nelse:\nraise Exception(\nf\"Failed to convert choice map of type {type(chm)} to IndexChoiceMap.\"\n)\ninner = tree_stack(subtrees)\nindices = jnp.array(indices)\nreturn IndexChoiceMap.new(indices, inner)\n@classmethod\n@dispatch\ndef new(cls, indices: IntArray, inner: ChoiceMap) -&gt; ChoiceMap:\n# Promote raw integers (or scalars) to non-null leading dim.\nindices = jnp.array(indices)\nif not indices.shape:\nindices = indices[:, None]\n# Verify that dimensions are consistent before creating an\n# `IndexChoiceMap`.\n_ = static_check_tree_leaves_have_matching_leading_dim((inner, indices))\n# if you try to wrap around an EmptyChoiceMap, do nothing.\nif isinstance(inner, EmptyChoiceMap):\nreturn inner\nreturn IndexChoiceMap(indices, inner)\n@classmethod\n@dispatch\ndef new(cls, indices: List, inner: ChoiceMap) -&gt; ChoiceMap:\nindices = jnp.array(indices)\nreturn IndexChoiceMap.new(indices, inner)\n@classmethod\n@dispatch\ndef new(cls, indices: Any, inner: Dict) -&gt; ChoiceMap:\ninner = choice_map(inner)\nreturn IndexChoiceMap.new(indices, inner)\ndef is_empty(self):\nreturn self.inner.is_empty()\n@dispatch\ndef filter(\nself,\nselection: HierarchicalSelection,\n) -&gt; ChoiceMap:\nreturn IndexChoiceMap(self.indices, self.inner.filter(selection))\ndef has_subtree(self, addr):\nif not isinstance(addr, Tuple) and len(addr) == 1:\nreturn False\n(idx, addr) = addr\nreturn jnp.logical_and(idx in self.indices, self.inner.has_subtree(addr))\n@dispatch\ndef filter(\nself,\nselection: IndexSelection,\n) -&gt; ChoiceMap:\nflags = jnp.isin(selection.indices, self.indices)\nfiltered_inner = self.inner.filter(selection.inner)\nmasked = mask(flags, filtered_inner)\nreturn IndexChoiceMap(self.indices, masked)\ndef has_subtree(self, addr):\nif not isinstance(addr, Tuple) and len(addr) == 1:\nreturn False\n(idx, addr) = addr\nreturn jnp.logical_and(idx in self.indices, self.inner.has_subtree(addr))\n@dispatch\ndef get_subtree(self, addr: Tuple):\nif not isinstance(addr, Tuple) and len(addr) == 1:\nreturn EmptyChoiceMap()\n(idx, addr) = addr\nsubtree = self.inner.get_subtree(addr)\nif isinstance(subtree, EmptyChoiceMap):\nreturn EmptyChoiceMap()\nelse:\n(slice_index,) = jnp.nonzero(idx == self.indices, size=1)\nsubtree = jtu.tree_map(lambda v: v[slice_index], subtree)\nreturn mask(idx in self.indices, subtree)\n@dispatch\ndef get_subtree(self, idx: IntArray):\n(slice_index,) = jnp.nonzero(idx == self.indices, size=1)\nslice_index = slice_index[0]\nsubtree = jtu.tree_map(lambda v: v[slice_index] if v.shape else v, self.inner)\nreturn mask(jnp.isin(idx, self.indices), subtree)\ndef get_selection(self):\nreturn self.inner.get_selection()\ndef get_subtrees_shallow(self):\nraise NotImplementedError\ndef merge(self, _: ChoiceMap):\nraise Exception(\"TODO: can't merge IndexChoiceMaps\")\ndef get_index(self):\nreturn self.indices\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\ndoc = gpp._pformat_array(self.indices, short_arrays=True)\nsub_tree = rich.tree.Tree(f\"[bold](Index,{doc})\")\nself.inner.__rich_tree__(sub_tree)\ntree.add(sub_tree)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/map.html#selections-for-vectorchoicemap","title":"Selections for <code>VectorChoiceMap</code>","text":"<p>(This section is also mirrored for <code>UnfoldCombinator</code>)</p> <p>To <code>filter</code> from <code>VectorChoiceMap</code>, or <code>project</code> from <code>MapTrace</code> both <code>HierarchicalSelection</code> and <code>IndexSelection</code> can be used.</p>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax.generative_functions.combinators.MapTrace","title":"<code>MapTrace</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Trace</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/map_combinator.py</code> <pre><code>@dataclass\nclass MapTrace(Trace):\ngen_fn: GenerativeFunction\ninner: Trace\nargs: Tuple\nretval: Any\nscore: FloatArray\ndef flatten(self):\nreturn (\nself.gen_fn,\nself.inner,\nself.args,\nself.retval,\nself.score,\n), ()\ndef get_args(self):\nreturn self.args\ndef get_choices(self):\nreturn VectorChoiceMap.new(self.inner)\ndef get_gen_fn(self):\nreturn self.gen_fn\ndef get_retval(self):\nreturn self.retval\ndef get_score(self):\nreturn self.score\n@dispatch\ndef maybe_restore_arguments_project(\nself,\ninner: Trace,\nselection: Selection,\n):\nreturn inner.project(selection)\n@dispatch\ndef maybe_restore_arguments_project(\nself,\ninner: DropArgumentsTrace,\nselection: Selection,\n):\noriginal_arguments = self.get_args()\n# Shape of arguments doesn't matter when we project.\nrestored = inner.restore(original_arguments)\nreturn restored.project(selection)\n@dispatch\ndef project(\nself,\nselection: IndexSelection,\n) -&gt; FloatArray:\ninner_project = self.maybe_restore_arguments_project(\nself.inner,\nselection.inner,\n)\nreturn jnp.sum(\njnp.take(inner_project, selection.indices, mode=\"fill\", fill_value=0.0)\n)\n@dispatch\ndef project(\nself,\nselection: HierarchicalSelection,\n) -&gt; FloatArray:\ninner_project = self.maybe_restore_arguments_project(\nself.inner,\nselection,\n)\nreturn jnp.sum(inner_project)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax.generative_functions.combinators.VectorChoiceMap","title":"<code>VectorChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass VectorChoiceMap(ChoiceMap):\ninner: Union[ChoiceMap, Trace]\ndef flatten(self):\nreturn (self.inner,), ()\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: EmptyChoiceMap,\n) -&gt; EmptyChoiceMap:\nreturn inner\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: ChoiceMap,\n) -&gt; ChoiceMap:\n# Static assertion: all leaves must have same first dim size.\nstatic_check_tree_leaves_have_matching_leading_dim(inner)\nreturn VectorChoiceMap(inner)\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: Dict,\n) -&gt; ChoiceMap:\nchm = choice_map(inner)\nreturn VectorChoiceMap.new(chm)\ndef is_empty(self):\nreturn self.inner.is_empty()\n@dispatch\ndef filter(\nself,\nselection: HierarchicalSelection,\n) -&gt; ChoiceMap:\nreturn VectorChoiceMap.new(self.inner.filter(selection))\n@dispatch\ndef filter(\nself,\nselection: IndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner)\nflags = jnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\n@dispatch\ndef filter(\nself,\nselection: ComplementIndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner.complement())\nflags = jnp.logical_not(\njnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\ndef get_selection(self):\nsubselection = self.inner.get_selection()\nreturn subselection\ndef has_subtree(self, addr):\nreturn self.inner.has_subtree(addr)\ndef get_subtree(self, addr):\nreturn self.inner.get_subtree(addr)\ndef get_subtrees_shallow(self):\nreturn self.inner.get_subtrees_shallow()\n@dispatch\ndef merge(self, other: \"VectorChoiceMap\") -&gt; Tuple[ChoiceMap, ChoiceMap]:\nnew, discard = self.inner.merge(other.inner)\nreturn VectorChoiceMap(new), VectorChoiceMap(discard)\n@dispatch\ndef merge(self, other: EmptyChoiceMap) -&gt; Tuple[ChoiceMap, ChoiceMap]:\nreturn self, other\n###########\n# Dunders #\n###########\ndef __hash__(self):\nreturn hash(self.inner)\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\nsub_tree = rich.tree.Tree(\"[bold](Vector)\")\nself.inner.__rich_tree__(sub_tree)\ntree.add(sub_tree)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/switch.html","title":"Switch combinator","text":"<p>GenJAX's <code>SwitchCombinator</code> is a combinator which branching control flow for generative computation by utilizing <code>jax.lax.switch</code>.</p>"},{"location":"genjax/library/generative_functions/combinators/switch.html#genjax.generative_functions.combinators.SwitchCombinator","title":"<code>SwitchCombinator</code>  <code>dataclass</code>","text":"<p>             Bases: <code>JAXGenerativeFunction</code>, <code>SupportsBuiltinSugar</code></p> <p><code>SwitchCombinator</code> accepts multiple generative functions as input and implements <code>GenerativeFunction</code> interface semantics that support branching control flow patterns, including control flow patterns which branch on other stochastic choices.</p> <p>Existence uncertainty</p> <p>This pattern allows <code>GenJAX</code> to express existence uncertainty over random choices -- as different generative function branches need not share addresses.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef branch_1():\nx = genjax.normal(0.0, 1.0) @ \"x1\"\n@genjax.gen\ndef branch_2():\nx = genjax.bernoulli(0.3) @ \"x2\"\n# Creating a `SwitchCombinator` via the preferred `new` class method.\nswitch = genjax.SwitchCombinator.new(branch_1, branch_2)\nkey = jax.random.PRNGKey(314159)\njitted = jax.jit(genjax.simulate(switch))\n_ = jitted(key, (0, ))\ntr = jitted(key, (1, ))\nprint(console.render(tr))\n</code></pre> <pre><code>\u2514\u2500\u2500 SwitchTrace\n    \u251c\u2500\u2500 gen_fn\n    \u2502   \u2514\u2500\u2500 SwitchCombinator\n    \u2502       \u2514\u2500\u2500 branches\n    \u2502           \u2514\u2500\u2500 list\n    \u2502               \u251c\u2500\u2500 BuiltinGenerativeFunction\n    \u2502               \u2502   \u2514\u2500\u2500 source\n    \u2502               \u2502       \u2514\u2500\u2500 &lt;function branch_1&gt;\n    \u2502               \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502                   \u2514\u2500\u2500 source\n    \u2502                       \u2514\u2500\u2500 &lt;function branch_2&gt;\n    \u251c\u2500\u2500 chm\n    \u2502   \u2514\u2500\u2500 SwitchChoiceMap\n    \u2502       \u251c\u2500\u2500 index\n    \u2502       \u2502   \u2514\u2500\u2500  i32[]\n    \u2502       \u2514\u2500\u2500 submaps\n    \u2502           \u2514\u2500\u2500 list\n    \u2502               \u251c\u2500\u2500 BuiltinTrace\n    \u2502               \u2502   \u251c\u2500\u2500 gen_fn\n    \u2502               \u2502   \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502               \u2502   \u2502       \u2514\u2500\u2500 source\n    \u2502               \u2502   \u2502           \u2514\u2500\u2500 &lt;function branch_1&gt;\n    \u2502               \u2502   \u251c\u2500\u2500 args\n    \u2502               \u2502   \u2502   \u2514\u2500\u2500 tuple\n    \u2502               \u2502   \u251c\u2500\u2500 retval\n    \u2502               \u2502   \u2502   \u2514\u2500\u2500 (const) None\n    \u2502               \u2502   \u251c\u2500\u2500 choices\n    \u2502               \u2502   \u2502   \u2514\u2500\u2500 Trie\n    \u2502               \u2502   \u2502       \u2514\u2500\u2500 :x1\n    \u2502               \u2502   \u2502           \u2514\u2500\u2500 DistributionTrace\n    \u2502               \u2502   \u2502               \u251c\u2500\u2500 gen_fn\n    \u2502               \u2502   \u2502               \u2502   \u2514\u2500\u2500 Normal\n    \u2502               \u2502   \u2502               \u251c\u2500\u2500 args\n    \u2502               \u2502   \u2502               \u2502   \u2514\u2500\u2500 tuple\n    \u2502               \u2502   \u2502               \u2502       \u251c\u2500\u2500  f32[]\n    \u2502               \u2502   \u2502               \u2502       \u2514\u2500\u2500  f32[]\n    \u2502               \u2502   \u2502               \u251c\u2500\u2500 value\n    \u2502               \u2502   \u2502               \u2502   \u2514\u2500\u2500  f32[]\n    \u2502               \u2502   \u2502               \u2514\u2500\u2500 score\n    \u2502               \u2502   \u2502                   \u2514\u2500\u2500  f32[]\n    \u2502               \u2502   \u251c\u2500\u2500 cache\n    \u2502               \u2502   \u2502   \u2514\u2500\u2500 Trie\n    \u2502               \u2502   \u2514\u2500\u2500 score\n    \u2502               \u2502       \u2514\u2500\u2500  f32[]\n    \u2502               \u2514\u2500\u2500 BuiltinTrace\n    \u2502                   \u251c\u2500\u2500 gen_fn\n    \u2502                   \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502                   \u2502       \u2514\u2500\u2500 source\n    \u2502                   \u2502           \u2514\u2500\u2500 &lt;function branch_2&gt;\n    \u2502                   \u251c\u2500\u2500 args\n    \u2502                   \u2502   \u2514\u2500\u2500 tuple\n    \u2502                   \u251c\u2500\u2500 retval\n    \u2502                   \u2502   \u2514\u2500\u2500 (const) None\n    \u2502                   \u251c\u2500\u2500 choices\n    \u2502                   \u2502   \u2514\u2500\u2500 Trie\n    \u2502                   \u2502       \u2514\u2500\u2500 :x2\n    \u2502                   \u2502           \u2514\u2500\u2500 DistributionTrace\n    \u2502                   \u2502               \u251c\u2500\u2500 gen_fn\n    \u2502                   \u2502               \u2502   \u2514\u2500\u2500 Bernoulli\n    \u2502                   \u2502               \u251c\u2500\u2500 args\n    \u2502                   \u2502               \u2502   \u2514\u2500\u2500 tuple\n    \u2502                   \u2502               \u2502       \u2514\u2500\u2500  f32[]\n    \u2502                   \u2502               \u251c\u2500\u2500 value\n    \u2502                   \u2502               \u2502   \u2514\u2500\u2500  bool[]\n    \u2502                   \u2502               \u2514\u2500\u2500 score\n    \u2502                   \u2502                   \u2514\u2500\u2500  f32[]\n    \u2502                   \u251c\u2500\u2500 cache\n    \u2502                   \u2502   \u2514\u2500\u2500 Trie\n    \u2502                   \u2514\u2500\u2500 score\n    \u2502                       \u2514\u2500\u2500  f32[]\n    \u251c\u2500\u2500 args\n    \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2514\u2500\u2500  i32[]\n    \u251c\u2500\u2500 retval\n    \u2502   \u2514\u2500\u2500 (const) None\n    \u2514\u2500\u2500 score\n        \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/switch/switch_combinator.py</code> <pre><code>@dataclass\nclass SwitchCombinator(JAXGenerativeFunction, SupportsBuiltinSugar):\n\"\"\"&gt; `SwitchCombinator` accepts multiple generative functions as input and\n    implements `GenerativeFunction` interface semantics that support branching\n    control flow patterns, including control flow patterns which branch on\n    other stochastic choices.\n    !!! info \"Existence uncertainty\"\n        This pattern allows `GenJAX` to express existence uncertainty over random choices -- as different generative function branches need not share addresses.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def branch_1():\n            x = genjax.normal(0.0, 1.0) @ \"x1\"\n        @genjax.gen\n        def branch_2():\n            x = genjax.bernoulli(0.3) @ \"x2\"\n        # Creating a `SwitchCombinator` via the preferred `new` class method.\n        switch = genjax.SwitchCombinator.new(branch_1, branch_2)\n        key = jax.random.PRNGKey(314159)\n        jitted = jax.jit(genjax.simulate(switch))\n        _ = jitted(key, (0, ))\n        tr = jitted(key, (1, ))\n        print(console.render(tr))\n        ```\n    \"\"\"\nbranches: List[JAXGenerativeFunction]\ndef flatten(self):\nreturn (self.branches,), ()\n@typecheck\n@classmethod\ndef new(cls, *args: JAXGenerativeFunction) -&gt; \"SwitchCombinator\":\n\"\"\"The preferred constructor for `SwitchCombinator` generative function\n        instances. The shorthand symbol is `Switch = SwitchCombinator.new`.\n        Arguments:\n            *args: JAX generative functions which will act as branch callees for the invocation of branching control flow.\n        Returns:\n            instance: A `SwitchCombinator` instance.\n        \"\"\"\nreturn SwitchCombinator([*args])\n# Optimized abstract call for tracing.\ndef __abstract_call__(self, branch, *args):\nfirst_branch = self.branches[0]\nreturn first_branch.__abstract_call__(*args)\n# Method is used to create a branch-agnostic type\n# which is acceptable for JAX's typing across `lax.switch`\n# branches.\ndef _create_sum_pytree(self, key, choices, args):\ncovers = []\nfor gen_fn in self.branches:\ntrace_shape = get_trace_data_shape(gen_fn, key, args)\ncovers.append(trace_shape)\nreturn DataSharedSumTree.new(choices, covers)\ndef get_trace_type(self, *args):\nsubtypes = []\nfor gen_fn in self.branches:\nsubtypes.append(gen_fn.get_trace_type(*args[1:]))\nreturn SumTraceType(subtypes)\ndef _simulate(self, branch_gen_fn, key, args):\ntr = branch_gen_fn.simulate(key, args[1:])\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nbranch_index = args[0]\nchoice_map = SwitchChoiceMap(branch_index, choices)\nscore = tr.get_score()\nretval = tr.get_retval()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn trace\n@typecheck\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; SwitchTrace:\nswitch = args[0]\ndef _inner(br):\nreturn lambda key, *args: self._simulate(br, key, args)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, *args)\ndef _importance(self, branch_gen_fn, key, chm, args):\n(w, tr) = branch_gen_fn.importance(key, chm, args[1:])\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nbranch_index = args[0]\nchoice_map = SwitchChoiceMap(branch_index, choices)\nscore = tr.get_score()\nretval = tr.get_retval()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn (w, trace)\n@typecheck\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, SwitchTrace]:\nswitch = args[0]\ndef _inner(br):\nreturn lambda key, chm, *args: self._importance(br, key, chm, args)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, chm, *args)\ndef _update_fallback(\nself,\nkey: PRNGKey,\nprev: Trace,\nnew: ChoiceMap,\nargdiffs: Tuple,\n):\n# Create a skeleton discard instance.\ndiscard_option: SwitchChoiceMap = prev.strip()\ndiscard_option = SwitchChoiceMap(\ndiscard_option.index,\nlist(map(lambda s: mask(False, s), discard_option.submaps)),\n)\ndef _inner_update(br, key):\n# Get the branch index (at tracing time) and use the branch\n# to update.\nconcrete_branch_index = self.branches.index(br)\nprev_subtrace = prev.get_subtrace(concrete_branch_index)\n(retval_diff, w, tr, maybe_discard) = br.update(\nkey, prev_subtrace, new, argdiffs[1:]\n)\n# Merge the skeleton discard with the actual one.\ndiscard_option.submaps[concrete_branch_index] = maybe_discard\n# Here, we create a DataSharedSumTree -- and we place the real trace\n# data inside of it.\nargs = tree_diff_primal(argdiffs)\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nchoice_map = SwitchChoiceMap(concrete_branch_index, choices)\n# Get all the metadata for update from the trace.\nscore = tr.get_score()\nretval = tr.get_retval()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn (retval_diff, w, trace, discard_option)\ndef _inner(br):\nreturn lambda key: _inner_update(br, key)\nbranch_functions = list(map(_inner, self.branches))\nswitch = tree_diff_primal(argdiffs[0])\nreturn jax.lax.switch(\nswitch,\nbranch_functions,\nkey,\n)\ndef _update_branch_switch(\nself,\nkey: PRNGKey,\nprev: Trace,\nnew: ChoiceMap,\nargdiffs: Tuple,\n):\ndef _inner_importance(br, key, prev, new, argdiffs):\nconcrete_branch_index = self.branches.index(br)\nnew = prev.strip().unsafe_merge(new)\nargs = tree_diff_primal(argdiffs)\n(w, tr) = br.importance(key, new, args[1:])\nupdate_weight = w - prev.get_score()\ndiscard = mask(True, prev.strip())\nretval = tr.get_retval()\nretval_diff = tree_diff_unknown_change(retval)\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nchoice_map = SwitchChoiceMap(concrete_branch_index, choices)\n# Get all the metadata for update from the trace.\nscore = tr.get_score()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn (retval_diff, update_weight, trace, discard)\ndef _inner(br):\nreturn lambda key, prev, new, argdiffs: _inner_importance(\nbr, key, prev, new, argdiffs\n)\nbranch_functions = list(map(_inner, self.branches))\nswitch = tree_diff_primal(argdiffs[0])\nreturn jax.lax.switch(\nswitch,\nbranch_functions,\nkey,\nprev,\nnew,\nargdiffs,\n)\n@typecheck\ndef update(\nself,\nkey: PRNGKey,\nprev: SwitchTrace,\nnew: ChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, SwitchTrace, ChoiceMap]:\nindex_argdiff = argdiffs[0]\nif static_check_no_change(index_argdiff):\nreturn self._update_fallback(key, prev, new, argdiffs)\nelse:\nreturn self._update_branch_switch(key, prev, new, argdiffs)\n@typecheck\ndef assess(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[Any, FloatArray]:\nswitch = args[0]\ndef _assess(branch_gen_fn, key, chm, args):\nreturn branch_gen_fn.assess(key, chm, args[1:])\ndef _inner(br):\nreturn lambda key, chm, *args: _assess(br, key, chm, args)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, chm, *args)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html","title":"Unfold combinator","text":"<p>GenJAX's <code>UnfoldCombinator</code> is a combinator which implements a scan-like pattern of control flow by utilizing <code>jax.lax.scan</code>.</p>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.UnfoldCombinator","title":"<code>UnfoldCombinator</code>  <code>dataclass</code>","text":"<p>             Bases: <code>JAXGenerativeFunction</code>, <code>SupportsBuiltinSugar</code></p> <p><code>UnfoldCombinator</code> accepts a kernel generative function, as well as a static maximum unroll length, and provides a scan-like pattern of generative computation.</p> <p>Kernel generative functions</p> <p>A kernel generative function is one which accepts and returns the same signature of arguments. Under the hood, <code>UnfoldCombinator</code> is implemented using <code>jax.lax.scan</code> - which has the same requirements.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n# A kernel generative function.\n@genjax.gen\ndef random_walk(prev):\nx = genjax.normal(prev, 1.0) @ \"x\"\nreturn x\n# Creating a `SwitchCombinator` via the preferred `new` class method.\nunfold = genjax.UnfoldCombinator.new(random_walk, 1000)\ninit = 0.5\nkey = jax.random.PRNGKey(314159)\ntr = jax.jit(genjax.simulate(unfold))(key, (999, init))\nprint(console.render(tr))\n</code></pre> <pre><code>\u2514\u2500\u2500 UnfoldTrace\n    \u251c\u2500\u2500 unfold\n    \u2502   \u2514\u2500\u2500 UnfoldCombinator\n    \u2502       \u251c\u2500\u2500 max_length\n    \u2502       \u2502   \u2514\u2500\u2500 (const) 1000\n    \u2502       \u2514\u2500\u2500 kernel\n    \u2502           \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502               \u2514\u2500\u2500 source\n    \u2502                   \u2514\u2500\u2500 &lt;function random_walk&gt;\n    \u251c\u2500\u2500 inner\n    \u2502   \u2514\u2500\u2500 BuiltinTrace\n    \u2502       \u251c\u2500\u2500 gen_fn\n    \u2502       \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n    \u2502       \u2502       \u2514\u2500\u2500 source\n    \u2502       \u2502           \u2514\u2500\u2500 &lt;function random_walk&gt;\n    \u2502       \u251c\u2500\u2500 args\n    \u2502       \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502       \u2514\u2500\u2500  f32[1000]\n    \u2502       \u251c\u2500\u2500 retval\n    \u2502       \u2502   \u2514\u2500\u2500  f32[1000]\n    \u2502       \u251c\u2500\u2500 choices\n    \u2502       \u2502   \u2514\u2500\u2500 Trie\n    \u2502       \u2502       \u2514\u2500\u2500 :x\n    \u2502       \u2502           \u2514\u2500\u2500 DistributionTrace\n    \u2502       \u2502               \u251c\u2500\u2500 gen_fn\n    \u2502       \u2502               \u2502   \u2514\u2500\u2500 Normal\n    \u2502       \u2502               \u251c\u2500\u2500 args\n    \u2502       \u2502               \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u2502               \u2502       \u251c\u2500\u2500  f32[1000]\n    \u2502       \u2502               \u2502       \u2514\u2500\u2500  f32[1000]\n    \u2502       \u2502               \u251c\u2500\u2500 value\n    \u2502       \u2502               \u2502   \u2514\u2500\u2500  f32[1000]\n    \u2502       \u2502               \u2514\u2500\u2500 score\n    \u2502       \u2502                   \u2514\u2500\u2500  f32[1000]\n    \u2502       \u251c\u2500\u2500 cache\n    \u2502       \u2502   \u2514\u2500\u2500 Trie\n    \u2502       \u2514\u2500\u2500 score\n    \u2502           \u2514\u2500\u2500  f32[1000]\n    \u251c\u2500\u2500 dynamic_length\n    \u2502   \u2514\u2500\u2500  i32[]\n    \u251c\u2500\u2500 args\n    \u2502   \u2514\u2500\u2500 tuple\n    \u2502       \u251c\u2500\u2500  i32[]\n    \u2502       \u2514\u2500\u2500  f32[]\n    \u251c\u2500\u2500 retval\n    \u2502   \u2514\u2500\u2500  f32[1000]\n    \u2514\u2500\u2500 score\n        \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/unfold_combinator.py</code> <pre><code>@dataclass\nclass UnfoldCombinator(JAXGenerativeFunction, SupportsBuiltinSugar):\n\"\"\"&gt; `UnfoldCombinator` accepts a kernel generative function, as well as a\n    static maximum unroll length, and provides a scan-like pattern of\n    generative computation.\n    !!! info \"Kernel generative functions\"\n        A kernel generative function is one which accepts and returns the same signature of arguments. Under the hood, `UnfoldCombinator` is implemented using `jax.lax.scan` - which has the same requirements.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        # A kernel generative function.\n        @genjax.gen\n        def random_walk(prev):\n            x = genjax.normal(prev, 1.0) @ \"x\"\n            return x\n        # Creating a `SwitchCombinator` via the preferred `new` class method.\n        unfold = genjax.UnfoldCombinator.new(random_walk, 1000)\n        init = 0.5\n        key = jax.random.PRNGKey(314159)\n        tr = jax.jit(genjax.simulate(unfold))(key, (999, init))\n        print(console.render(tr))\n        ```\n    \"\"\"\nmax_length: IntArray\nkernel: JAXGenerativeFunction\ndef flatten(self):\nreturn (self.kernel,), (self.max_length,)\n@typecheck\n@classmethod\ndef new(cls, kernel: JAXGenerativeFunction, max_length: Int) -&gt; \"UnfoldCombinator\":\n\"\"\"The preferred constructor for `UnfoldCombinator` generative function\n        instances. The shorthand symbol is `Unfold = UnfoldCombinator.new`.\n        Arguments:\n            kernel: A kernel `JAXGenerativeFunction` instance.\n            max_length: A static maximum possible unroll length.\n        Returns:\n            instance: An `UnfoldCombinator` instance.\n        \"\"\"\nreturn UnfoldCombinator(max_length, kernel)\n# To get the type of return value, just invoke\n# the scanned over source (with abstract tracer arguments).\ndef __abstract_call__(self, *args) -&gt; Any:\nstate = args[1]\nstatic_args = args[2:]\ndef _inner(carry, xs):\nstate = carry\nv = self.kernel.__abstract_call__(state, *static_args)\nreturn v, v\n_, stacked = jax.lax.scan(_inner, state, None, length=self.max_length)\nreturn stacked\ndef _optional_out_of_bounds_check(self, count: IntArray):\ndef _check():\ncheck_flag = jnp.less(self.max_length, count + 1)\ncheckify.check(\ncheck_flag,\nf\"\\nUnfoldCombinator received a length argument ({count}) longer than specified max length ({self.max_length})\",\n)\nglobal_options.optional_check(_check)\n@typecheck\ndef get_trace_type(self, *args, **kwargs) -&gt; VectorTraceType:\n_ = args[0]\nargs = args[1:]\ninner_type = self.kernel.get_trace_type(*args, **kwargs)\nreturn VectorTraceType(inner_type, self.max_length)\n@typecheck\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; UnfoldTrace:\nlength = args[0]\nself._optional_out_of_bounds_check(length)\nstate = args[1]\nstatic_args = args[2:]\nzero_trace = make_zero_trace(\nself.kernel,\nkey,\n(state, *static_args),\n)\ndef _inner_simulate(key, state, static_args, count):\nkey, sub_key = jax.random.split(key)\ntr = self.kernel.simulate(sub_key, (state, *static_args))\nstate = tr.get_retval()\nscore = tr.get_score()\nreturn (tr, state, count, count + 1, score)\ndef _inner_zero_fallback(key, state, _, count):\nstate = state\nscore = 0.0\nreturn (zero_trace, state, -1, count, score)\ndef _inner(carry, _):\ncount, key, state = carry\ncheck = jnp.less(count, length + 1)\nkey, sub_key = jax.random.split(key)\ntr, state, index, count, score = concrete_cond(\ncheck,\n_inner_simulate,\n_inner_zero_fallback,\nsub_key,\nstate,\nstatic_args,\ncount,\n)\nreturn (count, key, state), (tr, index, state, score)\n(_, _, state), (tr, _, retval, scores) = jax.lax.scan(\n_inner,\n(0, key, state),\nNone,\nlength=self.max_length,\n)\nunfold_tr = UnfoldTrace(\nself,\ntr,\nlength,\nargs,\nretval,\njnp.sum(scores),\n)\nreturn unfold_tr\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n):\nmaybe_idx_chm = IndexChoiceMap.convert(chm)\nreturn self.importance(key, maybe_idx_chm, args)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: IndexChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, UnfoldTrace]:\nlength = args[0]\nself._optional_out_of_bounds_check(length)\nstate = args[1]\nstatic_args = args[2:]\ndef _inner(carry, _):\ncount, key, state = carry\ndef _with_choicemap(key, count, state):\nsub_choice_map = chm.get_subtree(count)\nkey, sub_key = jax.random.split(key)\n(w, tr) = self.kernel.importance(\nsub_key, sub_choice_map, (state, *static_args)\n)\nreturn key, count + 1, tr, tr.get_retval(), tr.get_score(), w\ndef _with_empty_choicemap(key, count, state):\nsub_choice_map = EmptyChoiceMap()\nkey, sub_key = jax.random.split(key)\n(w, tr) = self.kernel.importance(\nsub_key, sub_choice_map, (state, *static_args)\n)\nreturn key, count, tr, state, 0.0, 0.0\ncheck = jnp.less(count, length + 1)\nkey, count, tr, state, score, w = concrete_cond(\ncheck, _with_choicemap, _with_empty_choicemap, key, count, state\n)\nreturn (count, key, state), (w, score, tr, state)\n(_, _, state), (w, score, tr, retval) = jax.lax.scan(\n_inner,\n(0, key, state),\nNone,\nlength=self.max_length,\n)\nunfold_tr = UnfoldTrace(\nself,\ntr,\nlength,\nargs,\nretval,\njnp.sum(score),\n)\nw = jnp.sum(w)\nreturn (w, unfold_tr)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: VectorChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, UnfoldTrace]:\nlength = args[0]\nself._optional_out_of_bounds_check(length)\nstate = args[1]\nstatic_args = args[2:]\ndef _inner(carry, slice):\ncount, key, state = carry\nchm = slice\nkey, sub_key = jax.random.split(key)\ndef _importance(key, chm, state):\nreturn self.kernel.importance(key, chm, (state, *static_args))\ndef _simulate(key, chm, state):\ntr = self.kernel.simulate(key, (state, *static_args))\nreturn (0.0, tr)\ncheck_count = jnp.less(count, length + 1)\n(w, tr) = concrete_cond(\ncheck_count,\n_importance,\n_simulate,\nsub_key,\nchm,\nstate,\n)\ncount, state, score, w = concrete_cond(\ncheck_count,\nlambda *args: (\ncount + 1,\ntr.get_retval(),\ntr.get_score(),\nw,\n),\nlambda *args: (count, state, 0.0, 0.0),\n)\nreturn (count, key, state), (w, score, tr, state)\n(_, _, state), (w, score, tr, retval) = jax.lax.scan(\n_inner,\n(0, key, state),\nchm,\nlength=self.max_length,\n)\nunfold_tr = UnfoldTrace(\nself,\ntr,\nlength,\nargs,\nretval,\njnp.sum(score),\n)\nw = jnp.sum(w)\nreturn (w, unfold_tr)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\n_: EmptyChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, UnfoldTrace]:\nlength = args[0]\nself._optional_out_of_bounds_check(length)\nunfold_tr = self.simulate(key, args)\nw = 0.0\nreturn (w, unfold_tr)\n@dispatch\ndef _update_fallback(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: ChoiceMap,\nlength: Diff,\nstate: Diff,\n*static_args: Diff,\n):\nmaybe_idx_chm = IndexChoiceMap.convert(chm)\nreturn self._update_fallback(\nkey, prev, maybe_idx_chm, length, state, *static_args\n)\n@dispatch\ndef _update_fallback(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: VectorChoiceMap,\nlength: Diff,\nstate: Diff,\n*static_args: Diff,\n):\nlength, state, static_args = tree_diff_primal((length, state, static_args))\ndef _inner(carry, slice):\ncount, key, state = carry\n(prev, chm) = slice\nkey, sub_key = jax.random.split(key)\n(retval_diff, w, tr, discard) = self.kernel.update(\nsub_key, prev, chm, (state, *static_args)\n)\ncheck = jnp.less(count, length + 1)\ncount, state, score, weight = concrete_cond(\ncheck,\nlambda *args: (count + 1, retval_diff, tr.get_score(), w),\nlambda *args: (count, state, 0.0, 0.0),\n)\nreturn (count, key, state), (state, score, weight, tr, discard)\n(_, _, state), (retval_diff, score, w, tr, discard) = jax.lax.scan(\n_inner,\n(0, key, state),\n(prev, chm),\nlength=self.max_length,\n)\nunfold_tr = UnfoldTrace(\nself,\ntr,\nlength,\n(length, state, *static_args),\ntree_diff_primal(retval_diff),\njnp.sum(score),\n)\nw = jnp.sum(w)\nreturn (retval_diff, w, unfold_tr, discard)\n@dispatch\ndef _update_specialized(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: EmptyChoiceMap,\nlength: Diff,\nstate: Any,\n*static_args: Any,\n):\nraise NotImplementedError\n# TODO: this does not handle when the new length\n# is less than the previous!\n@dispatch\ndef _update_specialized(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: IndexChoiceMap,\nlength: Diff,\nstate: Any,\n*static_args: Any,\n):\nstart_lower = jnp.min(chm.indices)\nprev_length = prev.get_args()[0]\n# TODO: `UnknownChange` is used here\n# to preserve the Pytree structure across the loop.\nstate_diff = tree_diff_unknown_change(\nconcrete_cond(\nstart_lower\n== 0,  # if the starting index is 0, we need to grab the state argument.\nlambda *args: state,\n# Else, we use the retval from the previous iteration in the trace.\nlambda *args: jtu.tree_map(\nlambda v: v[start_lower - 1],\nprev.get_retval(),\n),\n)\n)\nprev_inner_trace = prev.inner\ndef _inner(index, state):\n(key, w, state_diff, prev) = state\nsub_chm = chm.get_subtree(index)\nprev_slice = jtu.tree_map(lambda v: v[index], prev)\nkey, sub_key = jax.random.split(key)\n# Extending to an index greater than the previous length.\ndef _importance(key):\nstate_primal = tree_diff_primal(state_diff)\n(w, new_tr) = self.kernel.importance(\nkey, sub_chm, (state_primal, *static_args)\n)\nprimal_state = new_tr.get_retval()\nretval_diff = tree_diff_unknown_change(primal_state)\nreturn (retval_diff, w, new_tr)\n# Updating an existing index.\ndef _update(key):\nstatic_argdiffs = tree_diff_no_change(static_args)\n(retval_diff, w, new_tr, _) = self.kernel.update(\nkey, prev_slice, sub_chm, (state_diff, *static_argdiffs)\n)\n# TODO: c.f. message above on `UnknownChange`.\n# Preserve the diff type across the loop\n# iterations.\nprimal_state = tree_diff_primal(retval_diff)\nretval_diff = tree_diff_unknown_change(primal_state)\nreturn (retval_diff, w, new_tr)\ncheck = prev_length &lt; index\n(state_diff, idx_w, new_tr) = concrete_cond(\ncheck, _importance, _update, sub_key\n)\ndef _mutate(prev, new):\nnew = prev.at[index].set(new)\nreturn new\n# TODO: also handle discard.\nprev = jtu.tree_map(_mutate, prev, new_tr)\nw += idx_w\nreturn (key, w, state_diff, prev)\n# TODO: add discard.\nnew_upper = tree_diff_primal(length)\nnew_upper = jnp.where(\nnew_upper &gt;= self.max_length,\nself.max_length - 1,\nnew_upper,\n)\n(_, w, _, new_inner_trace) = jax.lax.fori_loop(\nstart_lower,\nnew_upper + 1,  # the bound semantics follow Python range semantics.\n_inner,\n(key, 0.0, state_diff, prev_inner_trace),\n)\n# Select the new return values.\nchecks = jnp.arange(0, self.max_length) &lt; new_upper + 1\ndef _where(v1, v2):\nextension = len(v1.shape) - 1\nnew_checks = checks.reshape(checks.shape + (1,) * extension)\nreturn jnp.where(new_checks, v1, v2)\nretval = jtu.tree_map(\n_where,\nnew_inner_trace.get_retval(),\nprev.get_retval(),\n)\nretval_diff = tree_diff_unknown_change(retval)\nargs = tree_diff_primal((length, state, *static_args))\n# TODO: is there a faster way to do this with the information I already have?\nnew_score = jnp.sum(\njnp.where(\njnp.arange(0, len(new_inner_trace.get_score())) &lt; new_upper + 1,\nnew_inner_trace.get_score(),\n0.0,\n)\n)\nnew_tr = UnfoldTrace(\nself,\nnew_inner_trace,\nnew_upper,\nargs,\nretval,\nnew_score,\n)\nreturn (retval_diff, w, new_tr, EmptyChoiceMap())\n@dispatch\ndef _update_specialized(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: VectorChoiceMap,\nlength: Diff,\nstate: Any,\n*static_args: Any,\n):\nraise NotImplementedError\n@dispatch\ndef _update_specialized(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: ChoiceMap,\nlength: Diff,\nstate: Any,\n*static_args: Any,\n):\nmaybe_idx_chm = IndexChoiceMap.convert(chm)\nreturn self._update_specialized(\nkey, prev, maybe_idx_chm, length, state, *static_args\n)\n@typecheck\ndef update(\nself,\nkey: PRNGKey,\nprev: UnfoldTrace,\nchm: ChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, UnfoldTrace, ChoiceMap]:\nlength = argdiffs[0]\nstate = argdiffs[1]\nstatic_args = argdiffs[2:]\nargs = tree_diff_primal(argdiffs)\nself._optional_out_of_bounds_check(args[0])  # length\ncheck_state_static_no_change = static_check_no_change((state, static_args))\nif check_state_static_no_change:\nstate = tree_diff_primal(state)\nstatic_args = tree_diff_primal(static_args)\nreturn self._update_specialized(\nkey,\nprev,\nchm,\nlength,\nstate,\n*static_args,\n)\nelse:\nreturn self._update_fallback(\nkey,\nprev,\nchm,\nlength,\nstate,\n*static_args,\n)\n@dispatch\ndef assess(\nself,\nkey: PRNGKey,\nchm: VectorChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[Any, FloatArray]:\nlength = args[0]\nself._optional_out_of_bounds_check(length)\nstate = args[1]\nstatic_args = args[2:]\ndef _inner(carry, slice):\ncount, key, state = carry\nchm = slice\ncheck = count == chm.get_index()\nkey, sub_key = jax.random.split(key)\n(retval, score) = self.kernel.assess(sub_key, chm, (state, *static_args))\ncheck = jnp.less(count, length + 1)\nindex = concrete_cond(\ncheck,\nlambda *args: count,\nlambda *args: -1,\n)\ncount, state, score = concrete_cond(\ncheck,\nlambda *args: (count + 1, retval, score),\nlambda *args: (count, state, 0.0),\n)\nreturn (count, key, state), (state, score, index)\n(_, _, state), (retval, score, _) = jax.lax.scan(\n_inner,\n(0, key, state),\nchm,\nlength=self.max_length,\n)\nscore = jnp.sum(score)\nreturn (retval, score)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#choice-maps-for-unfold","title":"Choice maps for <code>Unfold</code>","text":"<p><code>Unfold</code> produces <code>VectorChoiceMap</code> instances (a type of choice map shared with <code>MapCombinator</code>).</p> <p>To utilize <code>importance</code>, <code>update</code>, or <code>assess</code> with <code>Unfold</code>, it suffices to provide either a <code>VectorChoiceMap</code> for constraints, or an <code>IndexChoiceMap</code>. Both of these choice maps are documented below (documentation is mirrored at <code>MapCombinator</code>).</p>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.VectorChoiceMap","title":"<code>VectorChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass VectorChoiceMap(ChoiceMap):\ninner: Union[ChoiceMap, Trace]\ndef flatten(self):\nreturn (self.inner,), ()\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: EmptyChoiceMap,\n) -&gt; EmptyChoiceMap:\nreturn inner\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: ChoiceMap,\n) -&gt; ChoiceMap:\n# Static assertion: all leaves must have same first dim size.\nstatic_check_tree_leaves_have_matching_leading_dim(inner)\nreturn VectorChoiceMap(inner)\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: Dict,\n) -&gt; ChoiceMap:\nchm = choice_map(inner)\nreturn VectorChoiceMap.new(chm)\ndef is_empty(self):\nreturn self.inner.is_empty()\n@dispatch\ndef filter(\nself,\nselection: HierarchicalSelection,\n) -&gt; ChoiceMap:\nreturn VectorChoiceMap.new(self.inner.filter(selection))\n@dispatch\ndef filter(\nself,\nselection: IndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner)\nflags = jnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\n@dispatch\ndef filter(\nself,\nselection: ComplementIndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner.complement())\nflags = jnp.logical_not(\njnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\ndef get_selection(self):\nsubselection = self.inner.get_selection()\nreturn subselection\ndef has_subtree(self, addr):\nreturn self.inner.has_subtree(addr)\ndef get_subtree(self, addr):\nreturn self.inner.get_subtree(addr)\ndef get_subtrees_shallow(self):\nreturn self.inner.get_subtrees_shallow()\n@dispatch\ndef merge(self, other: \"VectorChoiceMap\") -&gt; Tuple[ChoiceMap, ChoiceMap]:\nnew, discard = self.inner.merge(other.inner)\nreturn VectorChoiceMap(new), VectorChoiceMap(discard)\n@dispatch\ndef merge(self, other: EmptyChoiceMap) -&gt; Tuple[ChoiceMap, ChoiceMap]:\nreturn self, other\n###########\n# Dunders #\n###########\ndef __hash__(self):\nreturn hash(self.inner)\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\nsub_tree = rich.tree.Tree(\"[bold](Vector)\")\nself.inner.__rich_tree__(sub_tree)\ntree.add(sub_tree)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.IndexChoiceMap","title":"<code>IndexChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass IndexChoiceMap(ChoiceMap):\nindices: IntArray\ninner: ChoiceMap\ndef flatten(self):\nreturn (self.indices, self.inner), ()\n@classmethod\ndef convert(cls, chm: ChoiceMap) -&gt; \"IndexChoiceMap\":\nindices = []\nsubtrees = []\nfor (k, v) in chm.get_subtrees_shallow():\nif isinstance(k, IntArray):\nindices.append(k)\nsubtrees.append(v)\nelse:\nraise Exception(\nf\"Failed to convert choice map of type {type(chm)} to IndexChoiceMap.\"\n)\ninner = tree_stack(subtrees)\nindices = jnp.array(indices)\nreturn IndexChoiceMap.new(indices, inner)\n@classmethod\n@dispatch\ndef new(cls, indices: IntArray, inner: ChoiceMap) -&gt; ChoiceMap:\n# Promote raw integers (or scalars) to non-null leading dim.\nindices = jnp.array(indices)\nif not indices.shape:\nindices = indices[:, None]\n# Verify that dimensions are consistent before creating an\n# `IndexChoiceMap`.\n_ = static_check_tree_leaves_have_matching_leading_dim((inner, indices))\n# if you try to wrap around an EmptyChoiceMap, do nothing.\nif isinstance(inner, EmptyChoiceMap):\nreturn inner\nreturn IndexChoiceMap(indices, inner)\n@classmethod\n@dispatch\ndef new(cls, indices: List, inner: ChoiceMap) -&gt; ChoiceMap:\nindices = jnp.array(indices)\nreturn IndexChoiceMap.new(indices, inner)\n@classmethod\n@dispatch\ndef new(cls, indices: Any, inner: Dict) -&gt; ChoiceMap:\ninner = choice_map(inner)\nreturn IndexChoiceMap.new(indices, inner)\ndef is_empty(self):\nreturn self.inner.is_empty()\n@dispatch\ndef filter(\nself,\nselection: HierarchicalSelection,\n) -&gt; ChoiceMap:\nreturn IndexChoiceMap(self.indices, self.inner.filter(selection))\ndef has_subtree(self, addr):\nif not isinstance(addr, Tuple) and len(addr) == 1:\nreturn False\n(idx, addr) = addr\nreturn jnp.logical_and(idx in self.indices, self.inner.has_subtree(addr))\n@dispatch\ndef filter(\nself,\nselection: IndexSelection,\n) -&gt; ChoiceMap:\nflags = jnp.isin(selection.indices, self.indices)\nfiltered_inner = self.inner.filter(selection.inner)\nmasked = mask(flags, filtered_inner)\nreturn IndexChoiceMap(self.indices, masked)\ndef has_subtree(self, addr):\nif not isinstance(addr, Tuple) and len(addr) == 1:\nreturn False\n(idx, addr) = addr\nreturn jnp.logical_and(idx in self.indices, self.inner.has_subtree(addr))\n@dispatch\ndef get_subtree(self, addr: Tuple):\nif not isinstance(addr, Tuple) and len(addr) == 1:\nreturn EmptyChoiceMap()\n(idx, addr) = addr\nsubtree = self.inner.get_subtree(addr)\nif isinstance(subtree, EmptyChoiceMap):\nreturn EmptyChoiceMap()\nelse:\n(slice_index,) = jnp.nonzero(idx == self.indices, size=1)\nsubtree = jtu.tree_map(lambda v: v[slice_index], subtree)\nreturn mask(idx in self.indices, subtree)\n@dispatch\ndef get_subtree(self, idx: IntArray):\n(slice_index,) = jnp.nonzero(idx == self.indices, size=1)\nslice_index = slice_index[0]\nsubtree = jtu.tree_map(lambda v: v[slice_index] if v.shape else v, self.inner)\nreturn mask(jnp.isin(idx, self.indices), subtree)\ndef get_selection(self):\nreturn self.inner.get_selection()\ndef get_subtrees_shallow(self):\nraise NotImplementedError\ndef merge(self, _: ChoiceMap):\nraise Exception(\"TODO: can't merge IndexChoiceMaps\")\ndef get_index(self):\nreturn self.indices\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\ndoc = gpp._pformat_array(self.indices, short_arrays=True)\nsub_tree = rich.tree.Tree(f\"[bold](Index,{doc})\")\nself.inner.__rich_tree__(sub_tree)\ntree.add(sub_tree)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#selections-for-vectorchoicemap","title":"Selections for <code>VectorChoiceMap</code>","text":"<p>(This section is also mirrored for <code>MapCombinator</code>)</p> <p>To <code>filter</code> from <code>VectorChoiceMap</code>, or <code>project</code> from <code>UnfoldTrace</code> both <code>HierarchicalSelection</code> and <code>IndexSelection</code> can be used.</p>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.UnfoldTrace","title":"<code>UnfoldTrace</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Trace</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/unfold_combinator.py</code> <pre><code>@dataclass\nclass UnfoldTrace(Trace):\nunfold: GenerativeFunction\ninner: Trace\ndynamic_length: IntArray\nargs: Tuple\nretval: Any\nscore: FloatArray\ndef flatten(self):\nreturn (\nself.unfold,\nself.inner,\nself.dynamic_length,\nself.args,\nself.retval,\nself.score,\n), ()\ndef get_args(self):\nreturn self.args\ndef get_choices(self):\nreturn VectorChoiceMap.new(self.inner)\ndef get_gen_fn(self):\nreturn self.unfold\ndef get_retval(self):\nreturn self.retval\ndef get_score(self):\nreturn self.score\n@dispatch\ndef project(\nself,\nselection: IndexSelection,\n) -&gt; FloatArray:\ninner_project = self.inner.project(selection.inner)\nreturn jnp.sum(\njnp.where(\nselection.indices &lt; self.dynamic_length + 1,\njnp.take(inner_project, selection.indices, mode=\"fill\", fill_value=0.0),\n0.0,\n)\n)\n@dispatch\ndef project(\nself,\nselection: HierarchicalSelection,\n) -&gt; FloatArray:\nreturn jnp.sum(\njnp.where(\njnp.arange(0, len(self.inner.get_score())) &lt; self.dynamic_length + 1,\nself.inner.project(selection),\n0.0,\n)\n)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.VectorChoiceMap","title":"<code>VectorChoiceMap</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass VectorChoiceMap(ChoiceMap):\ninner: Union[ChoiceMap, Trace]\ndef flatten(self):\nreturn (self.inner,), ()\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: EmptyChoiceMap,\n) -&gt; EmptyChoiceMap:\nreturn inner\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: ChoiceMap,\n) -&gt; ChoiceMap:\n# Static assertion: all leaves must have same first dim size.\nstatic_check_tree_leaves_have_matching_leading_dim(inner)\nreturn VectorChoiceMap(inner)\n@classmethod\n@dispatch\ndef new(\ncls,\ninner: Dict,\n) -&gt; ChoiceMap:\nchm = choice_map(inner)\nreturn VectorChoiceMap.new(chm)\ndef is_empty(self):\nreturn self.inner.is_empty()\n@dispatch\ndef filter(\nself,\nselection: HierarchicalSelection,\n) -&gt; ChoiceMap:\nreturn VectorChoiceMap.new(self.inner.filter(selection))\n@dispatch\ndef filter(\nself,\nselection: IndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner)\nflags = jnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\n@dispatch\ndef filter(\nself,\nselection: ComplementIndexSelection,\n) -&gt; ChoiceMap:\nfiltered = self.inner.filter(selection.inner.complement())\nflags = jnp.logical_not(\njnp.logical_and(\nselection.indices &gt;= 0,\nselection.indices\n&lt; static_check_tree_leaves_have_matching_leading_dim(self.inner),\n)\n)\ndef _take(v):\nreturn jnp.take(v, selection.indices, mode=\"clip\")\nreturn mask(flags, jtu.tree_map(_take, filtered))\ndef get_selection(self):\nsubselection = self.inner.get_selection()\nreturn subselection\ndef has_subtree(self, addr):\nreturn self.inner.has_subtree(addr)\ndef get_subtree(self, addr):\nreturn self.inner.get_subtree(addr)\ndef get_subtrees_shallow(self):\nreturn self.inner.get_subtrees_shallow()\n@dispatch\ndef merge(self, other: \"VectorChoiceMap\") -&gt; Tuple[ChoiceMap, ChoiceMap]:\nnew, discard = self.inner.merge(other.inner)\nreturn VectorChoiceMap(new), VectorChoiceMap(discard)\n@dispatch\ndef merge(self, other: EmptyChoiceMap) -&gt; Tuple[ChoiceMap, ChoiceMap]:\nreturn self, other\n###########\n# Dunders #\n###########\ndef __hash__(self):\nreturn hash(self.inner)\n###################\n# Pretty printing #\n###################\ndef __rich_tree__(self, tree):\nsub_tree = rich.tree.Tree(\"[bold](Vector)\")\nself.inner.__rich_tree__(sub_tree)\ntree.add(sub_tree)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html","title":"Distributions","text":""},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions","title":"<code>distributions</code>","text":"<p>This module provides:</p> <ul> <li> <p>Abstract base classes for declaring distributions as <code>GenerativeFunction</code> types. These classes include <code>Distribution</code> and <code>ExactDensity</code>. The latter assumes that the inheritor exposes exact density evaluation, while the former makes no such assumption.</p> </li> <li> <p>Several distributions from JAX's <code>scipy</code> module, as well as TensorFlow Distributions (<code>tfd</code>) from TensorFlow Probability (<code>tfp</code>) using the JAX backend.</p> </li> <li> <p>Custom distributions, including ones with exact posteriors (like discrete HMMs).</p> </li> <li> <p>A language (<code>coryx</code>) based on <code>oryx</code> for defining new distribution objects from inverse log determinant Jacobian transformations on existing distributions.</p> </li> <li> <p>A language (<code>gensp</code>) for defining distributions with estimated densities using inference.</p> </li> </ul>"},{"location":"genjax/library/generative_functions/distributions/index.html#the-distribution-abstract-base-class","title":"The <code>Distribution</code> abstract base class","text":""},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.Distribution","title":"<code>Distribution</code>  <code>dataclass</code>","text":"<p>             Bases: <code>JAXGenerativeFunction</code>, <code>SupportsBuiltinSugar</code></p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@dataclass\nclass Distribution(JAXGenerativeFunction, SupportsBuiltinSugar):\ndef flatten(self):\nreturn (), ()\ndef __abstract_call__(self, *args):\n# Abstract evaluation: value here doesn't matter, only the type.\nkey = jax.random.PRNGKey(0)\n(_, v) = self.random_weighted(key, *args)\nreturn v\n@typecheck\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\n# `get_trace_type` is compile time - the key value\n# doesn't matter, just the type.\nkey = jax.random.PRNGKey(0)\n(_, (_, ttype)) = jax.make_jaxpr(self.random_weighted, return_shape=True)(\nkey, *args\n)\nreturn tt_lift(ttype)\n@abc.abstractmethod\ndef random_weighted(self, *args, **kwargs):\npass\n@abc.abstractmethod\ndef estimate_logpdf(self, key, v, *args, **kwargs):\npass\n@typecheck\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; DistributionTrace:\n(w, v) = self.random_weighted(key, *args)\ntr = DistributionTrace(self, args, v, w)\nreturn tr\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: EmptyChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, DistributionTrace]:\ntr = self.simulate(key, args)\nreturn (0.0, tr)\n@dispatch\ndef importance(\nself,\nkey: PRNGKey,\nchm: ValueChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, DistributionTrace]:\nv = chm.get_leaf_value()\nw = self.estimate_logpdf(key, v, *args)\nscore = w\nreturn (w, DistributionTrace(self, args, v, score))\n# Precedence means preferred.\n@dispatch(precedence=1)\ndef importance(\nself,\nkey: PRNGKey,\nchm: Mask,\nargs: Tuple,\n) -&gt; Tuple[FloatArray, DistributionTrace]:\ndef _inactive():\nw = 0.0\ntr = self.simulate(key, args)\nreturn w, tr\ndef _active(v):\nw, tr = self.importance(key, v, args)\nreturn w, tr\nreturn chm.match(_inactive, _active)\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nprev: DistributionTrace,\nconstraints: EmptyChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, DistributionTrace, Any]:\nstatic_check_tree_leaves_diff(argdiffs)\nv = prev.get_retval()\nretval_diff = tree_diff_no_change(v)\n# If no change to arguments, no need to update.\nif static_check_no_change(argdiffs):\nreturn (retval_diff, 0.0, prev, EmptyChoiceMap())\n# Otherwise, we must compute an incremental weight.\nelse:\nargs = tree_diff_primal(argdiffs)\nfwd = self.estimate_logpdf(key, v, *args)\nbwd = prev.get_score()\nnew_tr = DistributionTrace(self, args, v, fwd)\nreturn (retval_diff, fwd - bwd, new_tr, EmptyChoiceMap())\n@dispatch\ndef update(\nself,\nkey: PRNGKey,\nprev: DistributionTrace,\nconstraints: ValueChoiceMap,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, DistributionTrace, Any]:\nstatic_check_tree_leaves_diff(argdiffs)\nargs = tree_diff_primal(argdiffs)\nv = constraints.get_leaf_value()\nfwd = self.estimate_logpdf(key, v, *args)\nbwd = prev.get_score()\nw = fwd - bwd\nnew_tr = DistributionTrace(self, args, v, fwd)\ndiscard = prev.get_choices()\nretval_diff = tree_diff_unknown_change(v)\nreturn (retval_diff, w, new_tr, discard)\n@dispatch(precedence=1)\ndef update(\nself,\nkey: PRNGKey,\nprev: DistributionTrace,\nconstraints: Mask,\nargdiffs: Tuple,\n) -&gt; Tuple[Any, FloatArray, DistributionTrace, Any]:\ndiscard_option = prev.strip()\ndef _none():\n(retdiff, w, new_tr, _) = self.update(key, prev, EmptyChoiceMap(), argdiffs)\ndiscard = mask(False, discard_option)\nprimal = tree_diff_primal(retdiff)\nretdiff = tree_diff_unknown_change(primal)\nreturn (retdiff, w, new_tr, discard)\ndef _some(val_chm):\n(retdiff, w, new_tr, _) = self.update(key, prev, val_chm, argdiffs)\ndiscard = mask(True, discard_option)\nprimal = tree_diff_primal(retdiff)\nretdiff = tree_diff_unknown_change(primal)\nreturn (retdiff, w, new_tr, discard)\nreturn constraints.match(_none, _some)\n@typecheck\ndef assess(\nself,\nkey: PRNGKey,\nevaluation_point: ValueChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[Any, FloatArray]:\nv = evaluation_point.get_leaf_value()\nscore = self.estimate_logpdf(key, v, *args)\nreturn (v, score)\n###################\n# Deserialization #\n###################\n@dispatch\ndef loads(\nself,\ndata: PickleDataFormat,\nbackend: PickleSerializationBackend,\n) -&gt; DistributionTrace:\nargs, value, score = backend.loads(data.payload)\nreturn DistributionTrace(self, args, value, score)\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#the-exactdensity-abstract-base-class","title":"The <code>ExactDensity</code> abstract base class","text":"<p>If you are attempting to create a new <code>Distribution</code>, you'll likely want to inherit from <code>ExactDensity</code> - which assumes that you have access to an exact logpdf method (a more restrictive assumption than <code>Distribution</code>). This is most often the case: all of the standard distributions (<code>scipy</code>, <code>tfd</code>) use <code>ExactDensity</code>.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.ExactDensity","title":"<code>ExactDensity</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Distribution</code></p> <p>Abstract base class which extends Distribution and assumes that the implementor provides an exact logpdf method (compared to one which returns an estimate of the logpdf).</p> <p>All of the standard distributions inherit from <code>ExactDensity</code>, and if you are looking to implement your own distribution, you should likely use this class.</p> <p><code>Distribution</code> implementors are <code>Pytree</code> implementors</p> <p>As <code>Distribution</code> extends <code>Pytree</code>, if you use this class, you must implement <code>flatten</code> as part of your class declaration.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@dataclass\nclass ExactDensity(Distribution):\n\"\"\"&gt; Abstract base class which extends Distribution and assumes that the\n    implementor provides an exact logpdf method (compared to one which returns\n    _an estimate of the logpdf_).\n    All of the standard distributions inherit from `ExactDensity`, and\n    if you are looking to implement your own distribution, you should\n    likely use this class.\n    !!! info \"`Distribution` implementors are `Pytree` implementors\"\n        As `Distribution` extends `Pytree`, if you use this class, you must implement `flatten` as part of your class declaration.\n    \"\"\"\n@abc.abstractmethod\ndef sample(self, key: PRNGKey, *args: Any, **kwargs) -&gt; Any:\n\"\"\"&gt; Sample from the distribution, returning a value from the event\n        space.\n        Arguments:\n            key: A `PRNGKey`.\n            *args: The arguments to the distribution invocation.\n        Returns:\n            v: A value from the support of the distribution.\n        Examples:\n            `genjax.normal` is a distribution with an exact density, which supports the `sample` interface. Here's an example of invoking `sample`.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            v = genjax.normal.sample(key, 0.0, 1.0)\n            print(console.render(v))\n            ```\n            Note that you often do want or need to invoke `sample` directly - you'll likely want to use the generative function interface methods instead:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            tr = genjax.normal.simulate(key, (0.0, 1.0))\n            print(console.render(tr))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef logpdf(self, v: Any, *args: Any, **kwargs) -&gt; FloatArray:\n\"\"\"&gt; Given a value from the support of the distribution, compute the\n        log probability of that value under the density (with respect to the\n        standard base measure).\n        Arguments:\n            v: A value from the support of the distribution.\n            *args: The arguments to the distribution invocation.\n        Returns:\n            logpdf: The log density evaluated at `v`, with density configured by `args`.\n        \"\"\"\ndef random_weighted(self, key, *args, **kwargs):\nv = self.sample(key, *args, **kwargs)\nw = self.logpdf(v, *args, **kwargs)\nreturn (w, v)\ndef estimate_logpdf(self, key, v, *args, **kwargs):\nw = self.logpdf(v, *args, **kwargs)\nreturn w\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#supported-distributions","title":"Supported distributions","text":"<p>Below, we list distribution generative function wrappers, all supported distributions, and their exported names.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#distributions-supported-via-tfpdistributions","title":"Distributions supported via <code>tfp.distributions</code>","text":"<p>To support TensorFlow Probability distributions (<code>tfp.distributions</code>), <code>genjax</code> exposes a <code>TFPDistribution</code> wrapper class which relies on interfaces defined for <code>tfp.distributions</code> objects to implement the <code>genjax.ExactDensity</code> interface.</p> <p>Below, we list all currently exported <code>TFPDistribution</code> instances.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.TFPDistribution","title":"<code>TFPDistribution</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ExactDensity</code></p> <p>A <code>GenerativeFunction</code> wrapper around TensorFlow Probability distributions.</p> <p>Implements the <code>ExactDensity</code> subclass of <code>genjax.Distribution</code> automatically using the interfaces defined for <code>tfp.distributions</code> objects.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/tensorflow_probability/__init__.py</code> <pre><code>@dataclass\nclass TFPDistribution(ExactDensity):\n\"\"\"\n    A `GenerativeFunction` wrapper around [TensorFlow Probability distributions](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions).\n    Implements the `ExactDensity` subclass of `genjax.Distribution` automatically using the interfaces defined for `tfp.distributions` objects.\n    \"\"\"\ndistribution: Any\ndef flatten(self):\nreturn (), (self.distribution,)\n@classmethod\ndef new(cls, tfp_d):\nnew = TFPDistribution(tfp_d)\nfunctools.update_wrapper(new, tfp_d)\nreturn new\ndef make_tfp_distribution(self, *args, **kwargs):\nreturn self.distribution(*args, **kwargs)\ndef sample(self, key, *args, **kwargs):\ndist = self.make_tfp_distribution(*args, **kwargs)\nreturn dist.sample(seed=key)\ndef logpdf(self, v, *args, **kwargs):\ndist = self.make_tfp_distribution(*args, **kwargs)\nreturn jnp.sum(dist.log_prob(v))\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions","title":"<code>distributions</code>","text":""},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_bates","title":"<code>tfp_bates = TFPDistribution.new(tfd.Bates)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Bates</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_chi","title":"<code>tfp_chi = TFPDistribution.new(tfd.Chi)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Chi</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_chi2","title":"<code>tfp_chi2 = TFPDistribution.new(tfd.Chi2)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Chi2</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_geometric","title":"<code>tfp_geometric = TFPDistribution.new(tfd.Geometric)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Geometric</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_gumbel","title":"<code>tfp_gumbel = TFPDistribution.new(tfd.Gumbel)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Gumbel</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_half_cauchy","title":"<code>tfp_half_cauchy = TFPDistribution.new(tfd.HalfCauchy)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.HalfCauchy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_half_normal","title":"<code>tfp_half_normal = TFPDistribution.new(tfd.HalfNormal)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.HalfNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_half_student_t","title":"<code>tfp_half_student_t = TFPDistribution.new(tfd.HalfStudentT)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.HalfStudentT</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_inverse_gamma","title":"<code>tfp_inverse_gamma = TFPDistribution.new(tfd.InverseGamma)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.InverseGamma</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_kumaraswamy","title":"<code>tfp_kumaraswamy = TFPDistribution.new(tfd.Kumaraswamy)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Kumaraswamy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_logit_normal","title":"<code>tfp_logit_normal = TFPDistribution.new(tfd.LogitNormal)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.LogitNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_moyal","title":"<code>tfp_moyal = TFPDistribution.new(tfd.Moyal)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Moyal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_multinomial","title":"<code>tfp_multinomial = TFPDistribution.new(tfd.Multinomial)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Multinomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_negative_binomial","title":"<code>tfp_negative_binomial = TFPDistribution.new(tfd.NegativeBinomial)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.NegativeBinomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_plackett_luce","title":"<code>tfp_plackett_luce = TFPDistribution.new(tfd.PlackettLuce)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.PlackettLuce</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_power_spherical","title":"<code>tfp_power_spherical = TFPDistribution.new(tfd.PowerSpherical)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.PowerSpherical</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_skellam","title":"<code>tfp_skellam = TFPDistribution.new(tfd.Skellam)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Skellam</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_student_t","title":"<code>tfp_student_t = TFPDistribution.new(tfd.StudentT)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.StudentT</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_normal","title":"<code>tfp_normal = TFPDistribution.new(tfd.Normal)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Normal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_mv_normal_diag","title":"<code>tfp_mv_normal_diag = TFPDistribution.new(tfd.MultivariateNormalDiag)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.MultivariateNormalDiag</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_mv_normal","title":"<code>tfp_mv_normal = TFPDistribution.new(tfd.MultivariateNormalFullCovariance)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.MultivariateNormalFullCovariance</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_categorical","title":"<code>tfp_categorical = TFPDistribution.new(tfd.Categorical)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Categorical</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_truncated_cauchy","title":"<code>tfp_truncated_cauchy = TFPDistribution.new(tfd.TruncatedCauchy)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.TruncatedCauchy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_truncated_normal","title":"<code>tfp_truncated_normal = TFPDistribution.new(tfd.TruncatedNormal)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.TruncatedNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_uniform","title":"<code>tfp_uniform = TFPDistribution.new(tfd.Uniform)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Uniform</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_von_mises","title":"<code>tfp_von_mises = TFPDistribution.new(tfd.VonMises)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.VonMises</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_von_mises_fisher","title":"<code>tfp_von_mises_fisher = TFPDistribution.new(tfd.VonMisesFisher)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.VonMisesFisher</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_weibull","title":"<code>tfp_weibull = TFPDistribution.new(tfd.Weibull)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Weibull</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.tfp_zipf","title":"<code>tfp_zipf = TFPDistribution.new(tfd.Zipf)</code>  <code>module-attribute</code>","text":"<p>A <code>TFPDistribution</code> generative function which wraps the <code>tfd.Zipf</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#custom-distributions-implemented-in-genjax","title":"Custom distributions implemented in <code>genjax</code>","text":""},{"location":"genjax/library/inference/index.html","title":"Inference","text":"<p><code>genjax</code> exposes several inference algorithms which are implemented utilizing the generative function interface.</p>"},{"location":"genjax/library/inference/is.html","title":"Importance sampling","text":"<p>This module exposes two variants of importance sampling, differing in their return signature.</p> <p>Sampling importance resampling runs importance sampling, and then resamples a single particle from the particle collection to return.</p>"},{"location":"genjax/library/inference/is.html#genjax.inference.importance_sampling.BootstrapImportanceSampling","title":"<code>BootstrapImportanceSampling</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> <p>Bootstrap importance sampling for generative functions.</p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass BootstrapImportanceSampling(Pytree):\n\"\"\"Bootstrap importance sampling for generative functions.\"\"\"\nnum_particles: IntArray\nmodel: GenerativeFunction\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@typecheck\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\n):\nreturn BootstrapImportanceSampling(num_particles, model)\ndef apply(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\n):\nsub_keys = jax.random.split(self.num_particles)\n(lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nmodel_args,\n)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn (trs, log_normalized_weights, log_ml_estimate)\n@typecheck\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax.inference.importance_sampling.CustomProposalImportanceSampling","title":"<code>CustomProposalImportanceSampling</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> <p>Custom proposal importance sampling for generative functions.</p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass CustomProposalImportanceSampling(Pytree):\n\"\"\"Custom proposal importance sampling for generative functions.\"\"\"\nnum_particles: IntArray\nmodel: GenerativeFunction\nproposal: GenerativeFunction\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@typecheck\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: GenerativeFunction,\n):\nreturn CustomProposalImportanceSampling(num_particles, model, proposal)\ndef apply(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\nproposal_args: Tuple,\n):\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\np_trs = jax.vmap(self.proposal.simulate, in_axes=(0, None))(\nsub_keys,\n(observations, *proposal_args),\n)\ndef _inner(key, proposal_chm, model_args):\nchm = proposal_chm.safe_merge(observations)\n(w, m_tr) = self.model.importance(\nkey,\nchm,\nmodel_args,\n)\nreturn (w, m_tr)\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n(lws, _) = jax.vmap(_inner, in_axes=(0, 0, None))(\nsub_keys, p_trs.strip(), model_args\n)\nlws = lws - p_trs.get_score()\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn (p_trs, log_normalized_weights, log_ml_estimate)\n@typecheck\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax.inference.importance_sampling.BootstrapSamplingImportanceResampling","title":"<code>BootstrapSamplingImportanceResampling</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass BootstrapSamplingImportanceResampling(Pytree):\nnum_particles: IntArray\nmodel: GenerativeFunction\ndef flatten(self):\nreturn (), (self.num_particles, self.model)\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\n):\nreturn BootstrapSamplingImportanceResampling(\nnum_particles,\nmodel,\n)\ndef apply(\nself,\nkey: PRNGKey,\nobs: ChoiceMap,\nmodel_args: Tuple,\n):\nkey, sub_key = jax.random.split(key)\nsub_keys = jax.random.split(sub_key, self.num_particles)\n(lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\nsub_keys, obs, model_args\n)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nind = jax.random.categorical(key, log_normalized_weights)\ntr = jtu.tree_map(lambda v: v[ind], trs)\nlnw = log_normalized_weights[ind]\nreturn (tr, lnw, log_ml_estimate)\n@typecheck\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax.inference.importance_sampling.CustomProposalSamplingImportanceResampling","title":"<code>CustomProposalSamplingImportanceResampling</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass CustomProposalSamplingImportanceResampling(Pytree):\nnum_particles: IntArray\nmodel: GenerativeFunction\nproposal: GenerativeFunction\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: GenerativeFunction,\n):\nreturn CustomProposalSamplingImportanceResampling(\nnum_particles,\nmodel,\nproposal,\n)\ndef apply(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\nproposal_args: Tuple,\n):\nkey, sub_key = jax.random.split(key)\nsub_keys = jax.random.split(sub_key, self.num_particles)\np_trs = jax.vmap(self.proposal.simulate, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nproposal_args,\n)\ndef _inner(key, proposal):\nconstraints = observations.safe_merge(proposal)\n(lws, _) = self.model.importance(key, constraints, model_args)\nreturn lws\nkey, sub_key = jax.random.split(key)\nsub_keys = jax.random.split(key, self.num_particles)\nlws = jax.vmap(_inner)(sub_keys, p_trs.strip())\nlws = lws - p_trs.get_score()\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nind = jax.random.categorical(key, log_normalized_weights)\ntr = jtu.tree_map(lambda v: v[ind], p_trs)\nlnw = log_normalized_weights[ind]\nreturn (tr, lnw, log_ml_estimate)\n@typecheck\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"}]}