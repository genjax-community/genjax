[
  {
    "objectID": "advanced/recurse_combinator/recurse_combinator.html",
    "href": "advanced/recurse_combinator/recurse_combinator.html",
    "title": "Online time series inference using pseudomarginal SMC",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\n\nIn this notebook, we’ll be combining several advanced ingredients of Gen and GenJAX to construct approximate densities induced by customized sequential Monte Carlo (SMC) targeting successively richer approximations to latent structure.\nThe notebook assumes several pre-requisites:\n\nUnderstanding generative functions, their interfaces, and the math that the interface functions compute.\nUnderstanding generative function combinators, and how genjax.Recurse can allow us to express bounded tree-like computations.\nUnderstanding the approximate density interface exposed by GenProx, and how we can equip generative functions with inference strategies to define approximate densities."
  },
  {
    "objectID": "advanced/coryx/coryx.html",
    "href": "advanced/coryx/coryx.html",
    "title": "Labeled Categorical",
    "section": "",
    "text": "This is a sketchbook for the design of an @dist DSL for GenJAX. The inversion + ILDJ functionality comes from Oryx (hence the title, coryx = core Oryx).\nimport genjax\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import vmap\nfrom jax import jit\nfrom jax import grad\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(0)\nInversion works for functions with known inverses like exp, as well as addition when one of the elements is a constant.\nThis example is from the Oryx docs.\n@genjax.dist\ndef new_dist(x, z):\n    v = genjax.rv(genjax.Normal)(x, z)\n    return jnp.exp(v / 2.0) + 2.0\n# I can sample from `new_dist`.\nv = new_dist.sample(key, 0.0, 2.0)\n\n# I can score a return value.\nq = new_dist.logpdf(3.0, 0.0, 2.0)\n(v, q)\n\n(Array(2.2860641, dtype=float32), Array(-0.71319103, dtype=float32))\n# Recreates the Oryx example.\ndef show_plot(mean, std):\n    vs = jit(vmap(new_dist.sample, in_axes=(0, None, None)))(\n        jax.random.split(jax.random.PRNGKey(0), 1000), mean, std\n    )\n    x = jnp.linspace(0.0, 8, 100)\n    scores = jnp.exp(jit(vmap(new_dist.logpdf, in_axes=(0, None, None)))(x, mean, std))\n    _, ax = plt.subplots(2, sharex=True)\n    ax[0].hist(vs, bins=\"auto\")\n    ax[1].plot(x, scores)\n    plt.show()\nshow_plot(0.0, 1.0)\nInvertible deterministic transforms should support relabeling when the label array has unique elements (and has #. elements equal to the cardinality of the event space).\nlabels = jnp.array([1.0, 0.0])\n\n\n@genjax.dist\ndef labeled_categorical(labels, logits):\n    v = genjax.rv(genjax.TFPCategorical)(logits)\n    return labels[v]\n# Sampling forward is okay.\nkey = jax.random.PRNGKey(0)\nlogits = jnp.array([0.3, 0.7])\nv = labeled_categorical.sample(key, labels, logits)\nv\n\nArray(1., dtype=float32)\n# Inversion currently breaks.\nscore = labeled_categorical.logpdf(0.0, labels, logits)\nscore\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_1989/4195692211.py:2 in &lt;module&gt;      │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_1989/4195692211.py'                  │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/generative_functions/distributions/coryx/lang │\n│ .py:192 in logpdf                                                                                │\n│                                                                                                  │\n│   189 │   │   def scorer(constraints):                                                           │\n│   190 │   │   │   return sow_transform(self.source, constraints)(*args)[1]                       │\n│   191 │   │                                                                                      │\n│ ❱ 192 │   │   inverses, ildj_correction = inverse_core.inverse_and_ildj(returner)(v)             │\n│   193 │   │   score = scorer(inverses) + ildj_correction                                         │\n│   194 │   │   return score                                                                       │\n│   195                                                                                            │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/generative_functions/distributions/coryx/core │\n│ .py:163 in wrapped                                                                               │\n│                                                                                                  │\n│   160 │   def wrapped(*args, **kwargs):                                                          │\n│   161 │   │   \"\"\"Function wrapper that takes in inverse arguments.\"\"\"                            │\n│   162 │   │   forward_args = trace_args if len(trace_args) else args                             │\n│ ❱ 163 │   │   jaxpr, (_, in_tree, _) = trace_util.stage(f, dynamic=False)(                       │\n│   164 │   │   │   *forward_args, **kwargs                                                        │\n│   165 │   │   )                                                                                  │\n│   166 │   │   flat_forward_args, _ = tree_util.tree_flatten(forward_args)                        │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/staging.py:64 in wrapped                 │\n│                                                                                                  │\n│    61 │   │   │   jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_fun, flat_avals)             │\n│    62 │   │   else:                                                                              │\n│    63 │   │   │   pvals = [pe.PartialVal.unknown(aval) for aval in flat_avals]                   │\n│ ❱  64 │   │   │   jaxpr, _, consts = pe.trace_to_jaxpr(flat_fun, pvals, instantiate=True)        │\n│    65 │   │   typed_jaxpr = jax_core.ClosedJaxpr(jaxpr, consts)                                  │\n│    66 │   │   return typed_jaxpr, (flat_args, in_tree, out_tree())                               │\n│    67                                                                                            │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/profiler.p │\n│ y:314 in wrapper                                                                                 │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/interpreters/pa │\n│ rtial_eval.py:756 in trace_to_jaxpr                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/linear_uti │\n│ l.py:165 in call_wrapped                                                                         │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/generative_functions/distributions/coryx/lang │\n│ .py:187 in returner                                                                              │\n│                                                                                                  │\n│   184 │                                                                                          │\n│   185 │   def logpdf(self, v, *args, **kwargs):                                                  │\n│   186 │   │   def returner(constraints):                                                         │\n│ ❱ 187 │   │   │   return sow_transform(self.source, constraints)(*args)[0]                       │\n│   188 │   │                                                                                      │\n│   189 │   │   def scorer(constraints):                                                           │\n│   190 │   │   │   return sow_transform(self.source, constraints)(*args)[1]                       │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/generative_functions/distributions/coryx/lang │\n│ .py:161 in _inner                                                                                │\n│                                                                                                  │\n│   158 │   │   else:                                                                              │\n│   159 │   │   │   handler = Sow.new([constraints])                                               │\n│   160 │   │   with cps.Interpreter.new(Bare, handler) as interpreter:                            │\n│ ❱ 161 │   │   │   flat_out = interpreter(                                                        │\n│   162 │   │   │   │   jaxpr, [Bare.new(v) for v in consts], list(map(Bare.new, flat_args))       │\n│   163 │   │   │   )                                                                              │\n│   164 │   │   flat_out = map(lambda v: v.get_val(), flat_out)                                    │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:227 in __call__      │\n│                                                                                                  │\n│   224 │   │   return eval_jaxpr_recurse(jaxpr.eqns, env, jaxpr.invars, args)                     │\n│   225 │                                                                                          │\n│   226 │   def __call__(self, jaxpr, consts, args):                                               │\n│ ❱ 227 │   │   return self._eval_jaxpr_continuation(jaxpr, consts, args)                          │\n│   228                                                                                            │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:224 in               │\n│ _eval_jaxpr_continuation                                                                         │\n│                                                                                                  │\n│   221 │   │   │   │   # selecting a rule, etc.                                                   │\n│   222 │   │   │   │   return safe_map(env.read, jaxpr.outvars)                                   │\n│   223 │   │                                                                                      │\n│ ❱ 224 │   │   return eval_jaxpr_recurse(jaxpr.eqns, env, jaxpr.invars, args)                     │\n│   225 │                                                                                          │\n│   226 │   def __call__(self, jaxpr, consts, args):                                               │\n│   227 │   │   return self._eval_jaxpr_continuation(jaxpr, consts, args)                          │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:203 in               │\n│ eval_jaxpr_recurse                                                                               │\n│                                                                                                  │\n│   200 │   │   │   │                                                                              │\n│   201 │   │   │   │   # Pass all the information over to the handler,                            │\n│   202 │   │   │   │   # which gets to choose how to interpret the primitive.                     │\n│ ❱ 203 │   │   │   │   return self.handler.handle(                                                │\n│   204 │   │   │   │   │   self.cell_type,                                                        │\n│   205 │   │   │   │   │   eqn.primitive,                                                         │\n│   206 │   │   │   │   │   args,                                                                  │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:143 in handle        │\n│                                                                                                  │\n│   140 │   │   │   # TODO: provide a better exception.                                            │\n│   141 │   │   │   except Exception as e:                                                         │\n│   142 │   │   │   │   raise e                                                                    │\n│ ❱ 143 │   │   │   return callable(cell_type, prim, args, cont, **kwargs)                         │\n│   144 │   │   else:                                                                              │\n│   145 │   │   │   return self.fallback(cell_type, prim, args, cont, **params)                    │\n│   146                                                                                            │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/generative_functions/distributions/coryx/lang │\n│ .py:148 in random_variable                                                                       │\n│                                                                                                  │\n│   145 │   │   w = gen_fn.logpdf(v, *args)                                                        │\n│   146 │   │   self.score += w                                                                    │\n│   147 │   │   v = cps.flatmap_outcells(cell_type, v)                                             │\n│ ❱ 148 │   │   return cont(*v)                                                                    │\n│   149                                                                                            │\n│   150                                                                                            │\n│   151 def sow_transform(source_fn, constraints, **kwargs):                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:194 in continuation  │\n│                                                                                                  │\n│   191 │   │   │   │                                                                              │\n│   192 │   │   │   │   # Create a continuation to pass to the rule.                               │\n│   193 │   │   │   │   def continuation(*args):                                                   │\n│ ❱ 194 │   │   │   │   │   return eval_jaxpr_recurse(                                             │\n│   195 │   │   │   │   │   │   eqns[1:],                                                          │\n│   196 │   │   │   │   │   │   env,                                                               │\n│   197 │   │   │   │   │   │   eqn.outvars,                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:203 in               │\n│ eval_jaxpr_recurse                                                                               │\n│                                                                                                  │\n│   200 │   │   │   │                                                                              │\n│   201 │   │   │   │   # Pass all the information over to the handler,                            │\n│   202 │   │   │   │   # which gets to choose how to interpret the primitive.                     │\n│ ❱ 203 │   │   │   │   return self.handler.handle(                                                │\n│   204 │   │   │   │   │   self.cell_type,                                                        │\n│   205 │   │   │   │   │   eqn.primitive,                                                         │\n│   206 │   │   │   │   │   args,                                                                  │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:145 in handle        │\n│                                                                                                  │\n│   142 │   │   │   │   raise e                                                                    │\n│   143 │   │   │   return callable(cell_type, prim, args, cont, **kwargs)                         │\n│   144 │   │   else:                                                                              │\n│ ❱ 145 │   │   │   return self.fallback(cell_type, prim, args, cont, **params)                    │\n│   146                                                                                            │\n│   147                                                                                            │\n│   148 @dataclasses.dataclass                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/core/interpreters/cps.py:126 in fallback      │\n│                                                                                                  │\n│   123 │   def fallback(self, cell_type: Type[Cell], prim: jc.Primitive, args, cont, **params):   │\n│   124 │   │   # Strip to JAX values.                                                             │\n│   125 │   │   args = static_map_unwrap(args)                                                     │\n│ ❱ 126 │   │   v = prim.bind(*args, **params)                                                     │\n│   127 │   │   # Lift back to `Cell` type.                                                        │\n│   128 │   │   v = flatmap_outcells(cell_type, v)                                                 │\n│   129 │   │   return cont(*v)                                                                    │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/core.py:34 │\n│ 3 in bind                                                                                        │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/core.py:34 │\n│ 6 in bind_with_trace                                                                             │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/interpreters/pa │\n│ rtial_eval.py:212 in process_primitive                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/interpreters/pa │\n│ rtial_eval.py:223 in default_process_primitive                                                   │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/core.py:37 │\n│ 9 in abstract_eval_                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/lax/utils. │\n│ py:67 in standard_abstract_eval                                                                  │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/lax/lax.py │\n│ :1558 in naryop_dtype_rule                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/.nox/jupyter/lib/python3.11/site-packages/jax/_src/lax/lax.py │\n│ :4653 in _check_same_dtypes                                                                      │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nTypeError: lax.lt requires arguments to have the same dtypes, got float32, int32."
  },
  {
    "objectID": "advanced/coryx/coryx.html#inversion-on-edge-cases",
    "href": "advanced/coryx/coryx.html#inversion-on-edge-cases",
    "title": "Labeled Categorical",
    "section": "Inversion on edge cases",
    "text": "Inversion on edge cases\n\n@genjax.dist\ndef edge_0(x):\n    v = genjax.rv(genjax.Normal)(x, 1.0)\n    q = genjax.rv(genjax.Normal)(x, 1.0)\n    return (v, v - q)\n\n\n# Sampling forward is okay.\nkey = jax.random.PRNGKey(0)\nv = edge_0.sample(key, 0.0)\nv\n\n\n# Inversion currently breaks.\nscore = edge_0.logpdf((0.3, 0.2), 1.0)\nscore"
  },
  {
    "objectID": "advanced/coryx/coryx.html#programs-which-should-fail",
    "href": "advanced/coryx/coryx.html#programs-which-should-fail",
    "title": "Labeled Categorical",
    "section": "Programs which should fail",
    "text": "Programs which should fail\n\n@genjax.dist\ndef new_dist(x):\n    v = genjax.rv(genjax.Normal)(x, 1.0)\n    return jnp.cos(v), jnp.sin(v)\n\n\n# Sampling forward is okay.\nkey = jax.random.PRNGKey(0)\nv = new_dist.sample(key, 1.0)\nv\n\n\n# Inversion should break.\nkey = jax.random.PRNGKey(314159)\nscore = new_dist.logpdf(v, 1.0)\nscore"
  },
  {
    "objectID": "advanced/smc_language/smc_language.html",
    "href": "advanced/smc_language/smc_language.html",
    "title": "The sequential Monte Carlo mini-language",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)"
  },
  {
    "objectID": "advanced/gen_sp/gen_sp.html",
    "href": "advanced/gen_sp/gen_sp.html",
    "title": "Probabilistic programming with estimated densities",
    "section": "",
    "text": "%config InlineBackend.figure_format = 'svg'\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import gensp\nfrom genjax import normal, tfp_uniform\nfrom math import pi as π\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nLet’s start by defining a model generative program. This example is from Alex Lew’s 2020 LAFI talk on a language called MetaPPL.\n@genjax.gen\ndef model():\n    x = normal(0.0, 10.0) @ \"x\"\n    y = normal(0.0, 10.0) @ \"y\"\n    z = normal(x**2 + y**2, 1.0) @ \"z\"\n    return z\nkey, sub_keys = genjax.slash(key, 300)\n_, trs = jax.vmap(model.simulate, in_axes=(0, None))(sub_keys, ())\nchm = trs.strip()\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nax.scatter(chm[\"x\"], chm[\"y\"], chm[\"z\"])\nplt.show()\nWhen we use gensp.chm_dist (which is shorthand for gensp.ChoiceMapDistribution) - we’re creating a new generative function whose internal proposal q can be changed.\ngensp_model = gensp.chm_dist(model, selection=genjax.select(\"x\", \"y\"), custom_q=None)\ngensp_model\n\n\n\n\nChoiceMapDistribution\n├── p\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── PytreeClosure(&lt;function model at 0x28da2a700&gt;)\n├── selection\n│   └── BuiltinSelection\n│       └── trie\n│           └── Trie\n│               ├── :x\n│               │   └── AllSelection\n│               └── :y\n│                   └── AllSelection\n└── custom_q\n    └── (const) None\nAbove, we’re keeping the default internal proposal q by not providing a custom_q. This means that gensp_model inherits q from model.\nThe resulting gensp_model object is something called a ChoiceMapDistribution - it’s an estimated distribution over the random choices at the addresses we pass in via selection.\nWe specified \"x\" and \"y\" - which means we are asking for a distribution which is the marginal of the full joint of model.\n\\[\nP(x, y) = \\int P(x, y, z) \\ dz\n\\]\nAs we know, this is the source of the intractability of Bayesian inference in the first place! So how do we get around this?\nkey, tr = gensp_model.simulate(key, ())\ntr\n\n\n\n\nDistributionTrace\n├── gen_fn\n│   └── ChoiceMapDistribution\n│       ├── p\n│       │   └── BuiltinGenerativeFunction\n│       │       └── source\n│       │           └── PytreeClosure(&lt;function model at 0x28da2a700&gt;)\n│       ├── selection\n│       │   └── BuiltinSelection\n│       │       └── trie\n│       │           └── Trie\n│       │               ├── :x\n│       │               │   └── AllSelection\n│       │               └── :y\n│       │                   └── AllSelection\n│       └── custom_q\n│           └── (const) None\n├── args\n│   └── tuple\n├── value\n│   └── BuiltinChoiceMap\n│       └── trie\n│           └── Trie\n│               ├── :x\n│               │   └── ValueChoiceMap\n│               │       └── value\n│               │           └──  f32[]\n│               └── :y\n│                   └── ValueChoiceMap\n│                       └── value\n│                           └──  f32[]\n└── score\n    └──  f32[]\nAnother way to produce an estimated density is to condition a normalized distribution (producing an unnormalized one), then estimatedly normalize.\nGenSP provides a way to express these densities using gensp.Target.\nconstraint = genjax.choice_map({\"z\": 4.0})\nconstraint\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        └── :z\n            └── ValueChoiceMap\n                └── value\n                    └── (const) 4.0\ntarget = gensp.target(model, (), constraint)\ntarget\n\n\n\n\nTarget\n├── p\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── PytreeClosure(&lt;function model at 0x28da2a700&gt;)\n├── args\n│   └── tuple\n└── constraints\n    └── BuiltinChoiceMap\n        └── trie\n            └── Trie\n                └── :z\n                    └── ValueChoiceMap\n                        └── value\n                            └── (const) 4.0\nA Target is an unnormalized distribution. The standard inference library in GenSP exposes inference algorithms which are themselves estimated densities, whose interfaces accept Target instances and perform estimated normalization.\ngensp_is = gensp.importance(50, None)\ngensp_is\n\n\n\n\nImportance\n├── num_particles\n│   └── (const) 50\n└── proposal\n    └── (const) None\nThe default variant of gensp.importance utilizes sampling importance resampling with no custom proposal. Let’s examine the quality of the posterior approximation.\nkey, sub_keys = genjax.slash(key, 1000)\n_, tr = jax.jit(jax.vmap(gensp_is.simulate, in_axes=(0, None)))(sub_keys, (target,))\nchm = tr.get_retval()\nchm\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        ├── :x\n        │   └── ValueChoiceMap\n        │       └── value\n        │           └──  f32[1000]\n        └── :y\n            └── ValueChoiceMap\n                └── value\n                    └──  f32[1000]\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\ncircle = plt.Circle((0, 0), 2.0, color=\"k\", fill=False, lw=3)\nax.scatter(chm[\"x\"], chm[\"y\"], marker=\".\")\nax.add_artist(circle)\nWe can improve the quality of the approximation by providing a custom proposal.\n@genjax.gen\ndef custom_q(target):\n    chm = target.constraints\n    z = chm[\"z\"]\n    θ = tfp_uniform(0.0, 2 * π) @ \"θ\"\n    x = normal(jnp.sqrt(z) * jnp.cos(θ), 0.2) @ \"x\"\n    y = normal(jnp.sqrt(z) * jnp.sin(θ), 0.2) @ \"y\"\nWhen run within the sampling importance resampling routine, the custom proposal is allowed to inspect the observed data (here, chm).\nNote that custom_q samples auxiliary randomness for the \"θ\" address - to correctly define the importance weight density ratio, we need to marginalize out \"θ\".\ngensp_custom_q = gensp.chm_dist(custom_q, selection=genjax.select(\"x\", \"y\"))\ngensp_is = gensp.importance(50, gensp_custom_q)\ngensp_is\n\n\n\n\nImportance\n├── num_particles\n│   └── (const) 50\n└── proposal\n    └── ChoiceMapDistribution\n        ├── p\n        │   └── BuiltinGenerativeFunction\n        │       └── source\n        │           └── PytreeClosure(&lt;function custom_q at 0x10729d760&gt;)\n        ├── selection\n        │   └── BuiltinSelection\n        │       └── trie\n        │           └── Trie\n        │               ├── :x\n        │               │   └── AllSelection\n        │               └── :y\n        │                   └── AllSelection\n        └── custom_q\n            └── (const) None\nkey, sub_keys = genjax.slash(key, 1000)\n_, tr = jax.jit(jax.vmap(gensp_is.simulate, in_axes=(0, None)))(sub_keys, (target,))\nchm = tr.get_retval()\nchm\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        ├── :x\n        │   └── ValueChoiceMap\n        │       └── value\n        │           └──  f32[1000]\n        └── :y\n            └── ValueChoiceMap\n                └── value\n                    └──  f32[1000]\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\ncircle = plt.Circle((0, 0), 2.0, color=\"k\", fill=False, lw=3)\nax.scatter(chm[\"x\"], chm[\"y\"], marker=\".\")\nax.add_artist(circle)\nFor both variants, we can estimate the log evidence and compare the value - the conditional log evidence ratio is often used for Bayesian model selection."
  },
  {
    "objectID": "advanced/gen_sp/gen_sp.html#trace-types-for-absolute-continuity-diagnosis",
    "href": "advanced/gen_sp/gen_sp.html#trace-types-for-absolute-continuity-diagnosis",
    "title": "Probabilistic programming with estimated densities",
    "section": "Trace types for absolute continuity diagnosis",
    "text": "Trace types for absolute continuity diagnosis\nIn the preceding sections, we were careful to identify exactly what subsets of random variables we wished to target with proposals and we lined addresses up accordingly. However, as the complexity of an estimated density tower increases - it would be useful to have automated tooling to check for support mismatches.\nGenJAX generative functions support a form of gradual typing based upon Lew et al, 2020 - providing a conservative representation of the support of the distribution which a generative function represents.\n\nmodel.get_trace_type()\n\n\n\n\nBuiltinTraceType\n├── trie\n│   └── Trie\n│       ├── :x\n│       │   └── ℝ ()\n│       ├── :y\n│       │   └── ℝ ()\n│       └── :z\n│           └── ℝ ()\n└── retval_type\n    └── ℝ ()\n\n\n\n\ncustom_q.get_trace_type(target)\n\n\n\n\nBuiltinTraceType\n├── trie\n│   └── Trie\n│       ├── :θ\n│       │   └── ℝ ()\n│       ├── :x\n│       │   └── ℝ ()\n│       └── :y\n│           └── ℝ ()\n└── retval_type\n    └── ϕ (empty)\n\n\n\nWhat happens if we transform custom_q to internally marginalize \"θ\"?\n\ngensp_custom_q.get_trace_type(target)\n\n\n\n\nBuiltinTraceType\n├── trie\n│   └── Trie\n│       ├── :x\n│       │   └── ℝ ()\n│       └── :y\n│           └── ℝ ()\n└── retval_type\n    └── ϕ (empty)\n\n\n\nThere’s an interface for trace types called on_support, which checks if a trace type is on the support of another trace type.\n\nt1 = gensp_custom_q.get_trace_type(target)\nt2 = model.get_trace_type()\ncheck, mismatch_tree = t1.on_support(t2)\nmismatch_tree\n\n\n\n\nTrie\n└── :z\n    └── tuple\n        ├── (const) None\n        └── ℝ ()\n\n\n\nThis interface returns two values: the first is a bool indicating if the first is on the support of the second, and the second is a “mismatch” tree which shows what addresses violate the check.\nEvery address which violates the check gets a tuple showing the trace type for the first type against the second type at that address - above, we see that the proposal doesn’t provide a choice at \"z\", the first element is None in the tuple.\nGenJAX’s GenSP implementation takes advantage of this gradual type system to perform static support checks at compile time, and return errors if mismatches occur. Observe:\n\n@genjax.gen\ndef no_match(target):\n    θ = normal(0.0, 1.0) @ \"θ\"\n\n\ntry:\n    gensp.static_check_supports(target, no_match)\nexcept Exception as e:\n    console.print(e)\n\nTrace type mismatch.\nGiven target: Target(p=BuiltinGenerativeFunction(source=PytreeClosure…\nProposal: BuiltinGenerativeFunction(source=PytreeClosure(callable=&lt;fu…\n\nAbsolute continuity failure at the following addresses:\nTrie(inner={'x': (Reals(shape=()), None), 'y': (Reals(shape=()), None…"
  },
  {
    "objectID": "hidden/scratch.html",
    "href": "hidden/scratch.html",
    "title": "Gen ⊗ JAX",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\n\nimport genjax\n\nconsole = genjax.pretty()\n\n\ndef emits_cc_gen_fn(v):\n    @genjax.gen\n    def model():\n        x = genjax.normal(jnp.sum(v), 1.0) @ \"x\"\n        return x\n\n    return model\n\n\n@genjax.gen\ndef model():\n    x = jnp.ones(5)\n    gen_fn = emits_cc_gen_fn(x)\n    v = gen_fn.inline()\n    return (v, gen_fn)\n\n\nkey = jax.random.PRNGKey(314159)\nkey, _ = jax.jit(genjax.simulate(model))(key, ())\n\n[None]\n[Traced&lt;ShapedArray(float32[5])&gt;with&lt;DynamicJaxprTrace(level=6/0)&gt;]\n[None]\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_43418/1931457155.py:26 in &lt;module&gt;    │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_43418/1931457155.py'                 │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/traceback_util.py │\n│ :166 in reraise_with_filtered_traceback                                                          │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:250 in    │\n│ cache_miss                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:158 in    │\n│ _python_pjit_helper                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/api.py:300 in     │\n│ infer_params                                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:499 in    │\n│ common_infer_params                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:961 in    │\n│ _pjit_jaxpr                                                                                      │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/linear_util.py:34 │\n│ 5 in memoized_fun                                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:914 in    │\n│ _create_pjit_jaxpr                                                                               │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/profiler.py:314   │\n│ in wrapper                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/interpreters/part │\n│ ial_eval.py:2155 in trace_to_jaxpr_dynamic                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/interpreters/part │\n│ ial_eval.py:2177 in trace_to_subjaxpr_dynamic                                                    │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/linear_util.py:18 │\n│ 8 in call_wrapped                                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/interface.py:49 in _inner                     │\n│                                                                                                  │\n│   46 def simulate(gen_fn, **kwargs) -&gt; Callable:                                                 │\n│   47 │   @functools.wraps(gen_fn.simulate)                                                       │\n│   48 │   def _inner(*args):                                                                      │\n│ ❱ 49 │   │   return gen_fn.simulate(*args, **kwargs)                                             │\n│   50 │                                                                                           │\n│   51 │   return _inner                                                                           │\n│   52                                                                                             │\n│ &lt;@beartype(genjax._src.generative_functions.builtin.builtin_gen_fn.BuiltinGenerativeFunction.sim │\n│ ulate) at 0x12778d940&gt;:52 in simulate                                                            │\n│                                                                                                  │\n│                                     ... 58 frames hidden ...                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/traceback_util.py │\n│ :166 in reraise_with_filtered_traceback                                                          │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:250 in    │\n│ cache_miss                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:158 in    │\n│ _python_pjit_helper                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/api.py:300 in     │\n│ infer_params                                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:499 in    │\n│ common_infer_params                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:961 in    │\n│ _pjit_jaxpr                                                                                      │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/linear_util.py:34 │\n│ 5 in memoized_fun                                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/pjit.py:914 in    │\n│ _create_pjit_jaxpr                                                                               │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/profiler.py:314   │\n│ in wrapper                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/interpreters/part │\n│ ial_eval.py:2155 in trace_to_jaxpr_dynamic                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/interpreters/part │\n│ ial_eval.py:2177 in trace_to_subjaxpr_dynamic                                                    │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/linear_util.py:18 │\n│ 8 in call_wrapped                                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/numpy/reductions. │\n│ py:209 in _reduce_sum                                                                            │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/numpy/reductions. │\n│ py:83 in _reduction                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/numpy/util.py:328 │\n│ in check_arraylike                                                                               │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'NoneType'&gt; at position 0."
  },
  {
    "objectID": "hidden/debugging_with_harvest.html",
    "href": "hidden/debugging_with_harvest.html",
    "title": "Gen ⊗ JAX",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport genjax\nfrom genjax import tag, grab, stab\n\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\n\n\ndef model():\n    v = genjax.tag(0.3, name=\"q\")\n    return jnp.exp(v)\n\n\nv, state = genjax.grab(model)()\nstate\n\n{'q': 0.3}\n\n\n\n\n@genjax.gen\ndef model():\n    v = genjax.tag(0.3, name=\"q\")\n    return jnp.exp(v)\n\n\n(key, tr), state = genjax.grab(model.simulate)(key, ())\nstate\n\n{}"
  },
  {
    "objectID": "hidden/genjax_expo_0.html",
    "href": "hidden/genjax_expo_0.html",
    "title": "Demo 0: GenJAX performance characteristics",
    "section": "",
    "text": "import time\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport dataclasses\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport pandas as pd\nimport seaborn as sns\nimport genjax\nfrom genjax.typing import FloatArray, IntArray, Union, PRNGKey, Tuple\nfrom genjax import GenerativeFunction, ChoiceMap, Selection, trace\n\nsns.set_theme(style=\"white\")\nplt.rcParams[\"figure.figsize\"] = [8, 8]\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\n\n# Device management.\ncpu_device = jax.devices(\"cpu\")[0]\ntry:\n    gpu_device = jax.devices(\"gpu\")[0]\nexcept:\n    gpu_device = jax.devices(\"cpu\")[0]\nThis notebook showcases key performance attributes of GenJAX.\nIt highlights performance improvements due to parallelism, as well as the efficacy of using JAX AD to support gradient moves - however, it does not contain comparisons with other language implementations of Gen.\nThe hardware characteristics of my experiment device is shown below.\n! neofetch --stdout\n\nmccoy@pop-os \n------------ \nOS: Pop!_OS 22.04 LTS x86_64 \nKernel: 6.0.12-76060006-generic \nUptime: 13 hours, 8 mins \nPackages: 2348 (dpkg) \nShell: zsh 5.8.1 \nResolution: 1920x1080 \nDE: GNOME 42.3.1 \nWM: Mutter \nWM Theme: Pop \nTheme: Pop-dark [GTK2/3] \nIcons: Pop [GTK2/3] \nTerminal: jupyter-lab \nCPU: AMD Ryzen 9 3900XT (24) @ 4.100GHz \nGPU: NVIDIA GeForce RTX 2080 \nMemory: 3401MiB / 32008MiB"
  },
  {
    "objectID": "hidden/genjax_expo_0.html#current-modeling-languages-static-combinators",
    "href": "hidden/genjax_expo_0.html#current-modeling-languages-static-combinators",
    "title": "Demo 0: GenJAX performance characteristics",
    "section": "Current modeling languages ~ static + combinators",
    "text": "Current modeling languages ~ static + combinators\nThe current set of modeling languages plus combinators is roughly equivalent to the (static modeling language plus combinators) subset of Gen.jl1.1 There are a few things that you can do beyond what is possible in the static modeling language (like use Python control flow evaluated at tracing time to perform codegen into the model) - but in general, these capabilities are syntactical sugar in nature.\nLet’s create a polynomial regression model with degree selection and outlier detection.\n\n# Branching model with two submodels.\nswitch = genjax.Switch(\n    list(\n        map(\n            genjax.gen,\n            [\n                lambda pv: trace(\"value\", genjax.Normal)(pv, 1.0),\n                lambda pv: trace(\"value\", genjax.Normal)(pv, 20.0),\n            ],\n        )\n    )\n)\n\n# A model vectorized over `x` which calls the branching model.\n@genjax.gen(genjax.Map, in_axes=(0, None, None))\ndef kernel(x, coefficients, basis_degrees):\n    is_outlier = trace(\"outlier\", genjax.Bernoulli)(0.05)\n    is_outlier = jnp.asarray(is_outlier, dtype=int)\n    polynomial_value = jnp.sum((x**basis_degrees) * coefficients)\n    y = trace(\"y\", switch)(is_outlier, polynomial_value)\n    return y\n\n\n# Here, `max_length` must be static at tracing time - it affects the shape\n# of arrays in certain parts of the computation.\n@genjax.gen\ndef model(xs):\n    max_degree = 5\n    coefficients = trace(\"alpha\", genjax.Map(genjax.TFPUniform, repeats=max_degree))(\n        -3.0, 3.0\n    )\n    basis_degrees = jnp.arange(max_degree)\n    ys = trace(\"ys\", kernel)(xs, coefficients, basis_degrees)\n    return ys\n\nLet’s sample a trace and examine it. Notice how the representation is a struct-of-array representation - even for submaps below MapTrace.\nIn GenJAX currently, combinators like Map don’t split the tree into separate instances - they provide indicators of broadcast semantics and raise the rank of the arrays from computations which they shadow. Something similar is true for Unfold.\n\ndata = jnp.arange(-2.0, 2.0, 0.05)\nkey, tr = jax.jit(model.simulate)(key, (data,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function model&gt;\n├── args\n│   └── tuple\n│       └──  f32[80]\n├── retval\n│   └──  f32[80]\n├── choices\n│   └── Trie\n│       ├── :alpha\n│       │   └── MapTrace\n│       │       ├── gen_fn\n│       │       │   └── MapCombinator\n│       │       │       ├── in_axes\n│       │       │       │   └── (const) None\n│       │       │       ├── repeats\n│       │       │       │   └── (const) 5\n│       │       │       └── kernel\n│       │       │           └── TFPDistribution\n│       │       │               └── distribution\n│       │       │                   └── (const) &lt;class \n│       │       │                       'tensorflow_probability.substrates.jax.distributions.uniform.Uniform'&gt;\n│       │       ├── indices\n│       │       │   └──  i32[5]\n│       │       ├── inner\n│       │       │   └── DistributionTrace\n│       │       │       ├── gen_fn\n│       │       │       │   └── TFPDistribution\n│       │       │       │       └── distribution\n│       │       │       │           └── (const) &lt;class \n│       │       │       │               'tensorflow_probability.substrates.jax.distributions.uniform.Uniform'&gt;\n│       │       │       ├── args\n│       │       │       │   └── tuple\n│       │       │       │       ├──  f32[5]\n│       │       │       │       └──  f32[5]\n│       │       │       ├── value\n│       │       │       │   └──  f32[5]\n│       │       │       └── score\n│       │       │           └──  f32[5]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :ys\n│           └── MapTrace\n│               ├── gen_fn\n│               │   └── MapCombinator\n│               │       ├── in_axes\n│               │       │   └── tuple\n│               │       │       ├── (const) 0\n│               │       │       ├── (const) None\n│               │       │       └── (const) None\n│               │       ├── repeats\n│               │       │   └── (const) None\n│               │       └── kernel\n│               │           └── BuiltinGenerativeFunction\n│               │               └── source\n│               │                   └── &lt;function kernel&gt;\n│               ├── indices\n│               │   └──  i32[80]\n│               ├── inner\n│               │   └── BuiltinTrace\n│               │       ├── gen_fn\n│               │       │   └── BuiltinGenerativeFunction\n│               │       │       └── source\n│               │       │           └── &lt;function kernel&gt;\n│               │       ├── args\n│               │       │   └── tuple\n│               │       │       ├──  f32[80]\n│               │       │       ├──  f32[80,5]\n│               │       │       └──  i32[80,5]\n│               │       ├── retval\n│               │       │   └──  f32[80]\n│               │       ├── choices\n│               │       │   └── Trie\n│               │       │       ├── :outlier\n│               │       │       │   └── DistributionTrace\n│               │       │       │       ├── gen_fn\n│               │       │       │       │   └── _Bernoulli\n│               │       │       │       ├── args\n│               │       │       │       │   └── tuple\n│               │       │       │       │       └──  f32[80]\n│               │       │       │       ├── value\n│               │       │       │       │   └──  bool[80]\n│               │       │       │       └── score\n│               │       │       │           └──  f32[80]\n│               │       │       └── :y\n│               │       │           └── SwitchTrace\n│               │       │               ├── gen_fn\n│               │       │               │   └── SwitchCombinator\n│               │       │               │       └── branches\n│               │       │               │           └── list\n│               │       │               │               ├── BuiltinGenerativeFunction\n│               │       │               │               │   └── source\n│               │       │               │               │       └── &lt;function &lt;lambda&gt;&gt;\n│               │       │               │               └── BuiltinGenerativeFunction\n│               │       │               │                   └── source\n│               │       │               │                       └── &lt;function &lt;lambda&gt;&gt;\n│               │       │               ├── chm\n│               │       │               │   └── IndexedChoiceMap\n│               │       │               │       ├── index\n│               │       │               │       │   └──  i32[80]\n│               │       │               │       └── submaps\n│               │       │               │           └── list\n│               │       │               │               ├── BuiltinTrace\n│               │       │               │               │   ├── gen_fn\n│               │       │               │               │   │   └── BuiltinGenerativeFunction\n│               │       │               │               │   │       └── source\n│               │       │               │               │   │           └── &lt;function &lt;lambda&gt;&gt;\n│               │       │               │               │   ├── args\n│               │       │               │               │   │   └── tuple\n│               │       │               │               │   │       └──  f32[80]\n│               │       │               │               │   ├── retval\n│               │       │               │               │   │   └──  f32[80]\n│               │       │               │               │   ├── choices\n│               │       │               │               │   │   └── Trie\n│               │       │               │               │   │       └── :value\n│               │       │               │               │   │           └── DistributionTrace\n│               │       │               │               │   │               ├── gen_fn\n│               │       │               │               │   │               │   └── _Normal\n│               │       │               │               │   │               ├── args\n│               │       │               │               │   │               │   └── tuple\n│               │       │               │               │   │               │       ├──  f32[80]\n│               │       │               │               │   │               │       └──  f32[80]\n│               │       │               │               │   │               ├── value\n│               │       │               │               │   │               │   └──  f32[80]\n│               │       │               │               │   │               └── score\n│               │       │               │               │   │                   └──  f32[80]\n│               │       │               │               │   ├── cache\n│               │       │               │               │   │   └── Trie\n│               │       │               │               │   └── score\n│               │       │               │               │       └──  f32[80]\n│               │       │               │               └── BuiltinTrace\n│               │       │               │                   ├── gen_fn\n│               │       │               │                   │   └── BuiltinGenerativeFunction\n│               │       │               │                   │       └── source\n│               │       │               │                   │           └── &lt;function &lt;lambda&gt;&gt;\n│               │       │               │                   ├── args\n│               │       │               │                   │   └── tuple\n│               │       │               │                   │       └──  f32[80]\n│               │       │               │                   ├── retval\n│               │       │               │                   │   └──  f32[80]\n│               │       │               │                   ├── choices\n│               │       │               │                   │   └── Trie\n│               │       │               │                   │       └── :value\n│               │       │               │                   │           └── DistributionTrace\n│               │       │               │                   │               ├── gen_fn\n│               │       │               │                   │               │   └── _Normal\n│               │       │               │                   │               ├── args\n│               │       │               │                   │               │   └── tuple\n│               │       │               │                   │               │       ├──  f32[80]\n│               │       │               │                   │               │       └──  f32[80]\n│               │       │               │                   │               ├── value\n│               │       │               │                   │               │   └──  f32[80]\n│               │       │               │                   │               └── score\n│               │       │               │                   │                   └──  f32[80]\n│               │       │               │                   ├── cache\n│               │       │               │                   │   └── Trie\n│               │       │               │                   └── score\n│               │       │               │                       └──  f32[80]\n│               │       │               ├── args\n│               │       │               │   └── tuple\n│               │       │               │       └──  f32[80]\n│               │       │               ├── retval\n│               │       │               │   └──  f32[80]\n│               │       │               └── score\n│               │       │                   └──  f32[80]\n│               │       ├── cache\n│               │       │   └── Trie\n│               │       └── score\n│               │           └──  f32[80]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nLet’s visualize some samples from our model.\n\ndef viz(ax, x, y, **kwargs):\n    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n\n\nf, axes = plt.subplots(4, 4, sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (data,))\n    x = data\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()"
  },
  {
    "objectID": "hidden/genjax_expo_0.html#inference-v0-importance-sampling",
    "href": "hidden/genjax_expo_0.html#inference-v0-importance-sampling",
    "title": "Demo 0: GenJAX performance characteristics",
    "section": "Inference v0: Importance sampling",
    "text": "Inference v0: Importance sampling\nImportance sampling (and SIR) are embarrassingly parallel. We can run an importance sampling routine and compare the performance characteristics on different devices.\n\n(genjax.ImportanceSampling, genjax.SamplingImportanceResampling)\n\n(\n    &lt;class 'genjax._src.inference.importance_sampling.ImportanceSampling'&gt;,\n    &lt;class 'genjax._src.inference.importance_sampling.SamplingImportanceResampling'&gt;\n)\n\n\n\nJust for enlightenment - let’s briefly look at the code for ImportanceSampling.\n\n@dataclasses.dataclass\nclass ImportanceSampling(genjax.Pytree):\n    num_particles: IntArray\n    model: GenerativeFunction\n    proposal: Union[None, GenerativeFunction] = None\n\n    def flatten(self):\n        return (), (self.num_particles, self.model, self.proposal)\n\n    @classmethod\n    def new(\n        cls,\n        num_particles: IntArray,\n        model: GenerativeFunction,\n        proposal: Union[None, GenerativeFunction] = None,\n    ):\n        return ImportanceSampling(\n            num_particles,\n            model,\n            proposal=proposal,\n        )\n\n    def _bootstrap_importance_sampling(\n        self,\n        key: PRNGKey,\n        observations: ChoiceMap,\n        model_args: Tuple,\n    ):\n        key, *sub_keys = jax.random.split(key, self.num_particles + 1)\n        sub_keys = jnp.array(sub_keys)\n        _, (lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\n            sub_keys,\n            observations,\n            model_args,\n        )\n        log_total_weight = jax.scipy.special.logsumexp(lws)\n        log_normalized_weights = lws - log_total_weight\n        log_ml_estimate = log_total_weight - jnp.log(self.num_particles)\n        return key, (trs, log_normalized_weights, log_ml_estimate)\n\n    def _proposal_importance_sampling(\n        self,\n        key: PRNGKey,\n        observations: ChoiceMap,\n        model_args: Tuple,\n        proposal_args: Tuple,\n    ):\n        key, *sub_keys = jax.random.split(key, self.num_particles + 1)\n        sub_keys = jnp.array(sub_keys)\n        _, p_trs = jax.vmap(self.proposal.simulate, in_axes=(0, None, None))(\n            sub_keys,\n            observations,\n            proposal_args,\n        )\n        observations = jax.tree_util.map(\n            lambda v: jnp.repeats(v, self.num_particles), observations\n        )\n        chm = p_trs.get_choices().merge(observations)\n        key, *sub_keys = jax.random.split(key, self.num_particles + 1)\n        sub_keys = jnp.array(sub_keys)\n        _, (lws, m_trs) = jax.vmap(self.model.importance, in_axes=(0, 0, None))(\n            sub_keys,\n            chm,\n            model_args,\n        )\n        lws = lws - p_trs.get_score()\n        log_total_weight = jax.scipy.special.logsumexp(lws)\n        log_normalized_weights = lws - log_total_weight\n        log_ml_estimate = log_total_weight - jnp.log(self.num_particles)\n        return key, (m_trs, log_normalized_weights, log_ml_estimate)\n\n    def apply(self, key, choice_map: ChoiceMap, *args):\n        # Importance sampling with custom proposal branch.\n        if len(args) == 2:\n            assert isinstance(args[0], tuple)\n            assert isinstance(args[1], tuple)\n            assert self.proposal is not None\n            model_args = args[0]\n            proposal_args = args[1]\n            return self._proposal_importance_sampling(\n                key, choice_map, model_args, proposal_args\n            )\n        # Bootstrap importance sampling branch.\n        else:\n            assert isinstance(args, tuple)\n            assert self.proposal is None\n            model_args = args[0]\n            return self._bootstrap_importance_sampling(key, choice_map, model_args)\n\n    def __call__(self, key, choice_map: ChoiceMap, *args):\n        return self.apply(key, choice_map, *args)\n\nBefore we consider invoking this, let’s write a ground truth process which we can parametrize by the number of data points.\n\ndef make_data(num_data_points):\n    key = jax.random.PRNGKey(0)\n    noise = 0.5 * jax.random.normal(key, shape=(num_data_points,))\n    data = np.arange(-2.0, 2.0, 4.0 / num_data_points)\n    y = 2.0 * data**2 + 2.0 + data**3\n    y[int(num_data_points / 2)] = 20.0\n    y[int(num_data_points / 4)] = 20.0\n    y[int(num_data_points / 5)] = 20.0\n    y = y + noise\n    return data, y\n\n\ndata, y = make_data(100)\nfig_data, ax_data = plt.subplots(dpi=280)\nviz(ax_data, data, y, color=\"blue\")\n\n\n\n\n\nobservations = genjax.choice_map(\n    {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n)\nobservations\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        └── :ys\n            └── VectorChoiceMap\n                ├── indices\n                │   └── (numpy) i64[100]\n                └── inner\n                    └── BuiltinChoiceMap\n                        └── trie\n                            └── Trie\n                                └── :y\n                                    └── Trie\n                                        └── :value\n                                            └── ValueChoiceMap\n                                                └── value\n                                                    └──  f32[100]\n\n\n\nHere, VectorChoiceMap communicates constraints to MapCombinator (as well as UnfoldCombinator, but that’s not featured in our model right now).\nThe indices describe, for each leaf element of the struct-of-array choice map shadowed by the VectorChoiceMap, what index of the Map invocation the value came from.\n\nobservations[\"ys\"].indices\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\nTo invoke importance sampling, we can instantiate an instance of ImportanceSampling.\n\n# 1000 particles.\ninf = genjax.ImportanceSampling.new(1000, model)\n\nHere, because I want a convenient way to show the inference results, I’m actually going to use sampling importance resampling - allowing me to sample a single sample from the approximate posterior induced by importance sampling.\n\ninf = genjax.SamplingImportanceResampling.new(1000, model)\ninf\n\n\n\n\nSamplingImportanceResampling\n├── num_particles\n│   └── (const) 1000\n├── model\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function model&gt;\n└── proposal\n    └── (const) None\n\n\n\nLet’s plot inference results using SIR to return single sample from the approximate posterior induced by importance sampling.\n\n# Evaluate a polynomial at x.\ndef polynomial_at_x(x, coefficients):\n    v = jnp.arange(len(coefficients))\n    basis = x**v\n    polynomial_value = jnp.sum(coefficients * basis)\n    return polynomial_value\n\n\npoly_eval = jax.jit(jax.vmap(polynomial_at_x, in_axes=(0, None)))\n\n\ndef plot_polynomial_values(ax, x, coefficients, **kwargs):\n    v = poly_eval(x, coefficients)\n    ax.scatter(x, v, **kwargs)\n\nBelow, we showcase 16 runs of SIR - we highlight outlier predictions in a green color, and the polynomial induced by the coefficients from the posterior sample is plotted in light gold.\n\n# Sampling importance resampling\ninf = genjax.sir(1000, model)\nx, y = make_data(100)\nobservations = genjax.choice_map(\n    {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n)\njitted = jax.jit(inf.apply)\n# Warmup.\nkey, (tr, _, _) = jitted(key, observations, (x,))\n%time key, (tr, _, _) = jitted(key, observations, (x, ))\n\n# Make plots.\nf, axes = plt.subplots(4, 4, sharex=True, sharey=True, dpi=280)\nf.suptitle(\"Samples from IS approximate posterior\")\nfor ax in axes.flatten():\n    key, (tr, _, _) = jitted(key, observations, (x,))\n    coefficients = tr[\"alpha\"].inner.get_retval()\n    outliers = tr[\"ys\"].indices[tr[\"ys\", \"outlier\"]]\n    viz(ax, x, y, marker=\".\")\n    viz(ax, x[outliers], y[outliers], marker=\".\", color=\"green\")\n    plot_polynomial_values(ax, data, coefficients, marker=\".\", alpha=0.2, color=\"gold\")\n\nplt.show()\n\nCPU times: user 529 µs, sys: 0 ns, total: 529 µs\nWall time: 387 µs\n\n\n\n\n\nNow, the outlier inferences aren’t the best - we’ll attempt to fix that in a moment…\nBut notice the runtime - at least for me, that feels comparable to Julia - and it makes sense, we’ve eliminated all the Python overhead in the hot inference code.\nBut that timing was utilizing the CPU - and internally importance sampling calls vmap, which supports multi-threading parallelism on devices which support it.\nLet’s move on to benchmarking, and now we’ll get to compare the CPU execution to the GPU.\n\nBenchmarking importance sampling\nLater on, we’ll add ingredients to our importance sampling routine (like rejuvenation) - so here, we’ll make the benchmark code re-usable.\n\ndef benchmark(key, inf, *args, iters=100, device=jax.devices()[0]):\n    jitted = jax.jit(inf, device=device)\n    # warmup\n    key, _ = jitted(key, *args)\n    times = []\n    for i in range(0, iters):\n        start = time.time()\n        key, v = jitted(key, *args)\n        v.block_until_ready()  # force\n        stop = time.time()\n        times.append(stop - start)\n    return key, np.array(times)\n\n\n# 1000 particles.\ninf = genjax.ImportanceSampling.new(1000, model)\n\nBelow is our complete benchmark code - which runs our variant of inference on both CPU and GPU devices, while scaling up the number of datapoints.\n\n# Benchmark function.\ndef _importance_sampling(key, *args):\n    inf = genjax.ImportanceSampling.new(1000, model)\n    key, (trs, _, _) = inf.apply(key, *args)\n    score = jnp.mean(trs.get_score())\n    return key, score\n\n\ndef benchmark_is(key, num_data_points, device=jax.devices()[0]):\n    x, y = make_data(num_data_points)\n    observations = genjax.choice_map(\n        {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n    )\n    key, times = benchmark(key, _importance_sampling, observations, (x,), device=device)\n    return key, times\n\n\n# Convenience which allows us to scale data and get times back\n# for execution on a specific JAX-managed device.\ndef test_device(key, data_counts, device):\n    times = []\n    for data_count in data_counts:\n        key, ts = %time benchmark_is(key, data_count, device=device)\n        m = np.mean(ts)\n        times.append(m)\n\n    return key, np.array(times)\n\n\n# Benchmarking.\ndata_counts = [100, 200, 300, 400, 500, 600]\nf, axes = plt.subplots(1, 2, sharey=True, dpi=280)\ncpu_ax = axes[0]\ngpu_ax = axes[1]\ncpu_ax.set_ylabel(\"Time (s)\")\ncpu_ax.set_xlabel(\"Num. data points\")\ngpu_ax.set_xlabel(\"Num. data points\")\nkey, cpu_times = test_device(key, data_counts, cpu_device)\nkey, gpu_times = test_device(key, data_counts, gpu_device)\ncpu_ax.scatter(data_counts, cpu_times)\ngpu_ax.scatter(data_counts, gpu_times)\nf.suptitle(\"IS Runtime (CPU vs. GPU)\")\n\nplt.show()\n\nCPU times: user 8.65 s, sys: 552 ms, total: 9.2 s\nWall time: 3.66 s\nCPU times: user 16.8 s, sys: 659 ms, total: 17.4 s\nWall time: 4.93 s\nCPU times: user 24.3 s, sys: 751 ms, total: 25.1 s\nWall time: 6.26 s\nCPU times: user 31.4 s, sys: 570 ms, total: 31.9 s\nWall time: 7.56 s\nCPU times: user 37.5 s, sys: 967 ms, total: 38.5 s\nWall time: 9 s\nCPU times: user 46.1 s, sys: 1.03 s, total: 47.1 s\nWall time: 10.7 s\nCPU times: user 3.05 s, sys: 84 ms, total: 3.13 s\nWall time: 1.98 s\nCPU times: user 3.41 s, sys: 116 ms, total: 3.52 s\nWall time: 2.17 s\nCPU times: user 3.71 s, sys: 76.2 ms, total: 3.79 s\nWall time: 2.33 s\nCPU times: user 3.72 s, sys: 168 ms, total: 3.89 s\nWall time: 2.46 s\nCPU times: user 3.91 s, sys: 108 ms, total: 4.02 s\nWall time: 2.61 s\nCPU times: user 3.95 s, sys: 126 ms, total: 4.07 s\nWall time: 2.61 s\n\n\n\n\n\nNotice in the plot above - parallelization keeps the compute time constant! That’s what we expect with embarrassingly parallel computations, and in this case - it’s extremely beneficial."
  },
  {
    "objectID": "hidden/genjax_expo_0.html#interim-ad-interfaces-in-genjax",
    "href": "hidden/genjax_expo_0.html#interim-ad-interfaces-in-genjax",
    "title": "Demo 0: GenJAX performance characteristics",
    "section": "Interim: AD interfaces in GenJAX",
    "text": "Interim: AD interfaces in GenJAX\nLet’s also consider the AD interfaces – from importance sampling, we might want to consider rejuvenation moves which utilize gradients (like MALA).\nIn GenJAX, the interfaces are slightly different than Gen.jl – GenJAX automatically defines a function called unzip for any generative function which implements assess.\n\nkey, scorer, returner = model.unzip(key, genjax.EmptyChoiceMap)\n\nmodel.unzip requires a PRNG key and fixed choice map - and returns two functions.\nThe first scorer function accepts a choice map and arguments and returns a score (computed using assess). It’s a convenient function to apply jax.grad to.\nThe second returner function accepts a choice map and arguments and returns the retval (again computed using assess). It provides an interface to compute gradients with respect to the return value of the generative function.\nAt least – that’s how it should be! For now, JAX has a few sharp edges around differentiating functions with respect to Pytree arguments. So the signatures of scorer and returner are:\ndef scorer(differentiable, nondifferentiable):\n    ...\n    \ndef returner(differentiable, nondifferentiable):\n    ...\nGenJAX exports a few utilities which allows you to break Pytree instances (like choice maps) into components - the differentiable ones allowed gradients to flow, the non-differentiable ones (like integers, or booleans) do not.\nWe won’t we be looking at these lowest level interfaces today – but I’ll just mention that choice_grad is an automatically defined method on generative functions which utilizes the lowest level interfaces to provide support for similar functionality to Gen.jl:\n\n# A higher-level gradient API - it relies upon `unzip`,\n# but provides convenient access to first-order gradients.\ndef choice_grad(self, key, trace, selection):\n    fixed = selection.complement().filter(trace.strip())\n    evaluation_point = selection.filter(trace.strip())\n    key, scorer, _ = self.unzip(key, fixed)\n    grad, nograd = tree_grad_split(\n        (evaluation_point, trace.get_args()),\n    )\n    choice_gradient_tree, _ = jax.grad(scorer)(grad, nograd)\n    return key, choice_gradient_tree\n\nNote how this implementation of the gradient interfaces explicitly breaks the compositionality which Gen defines for choice_gradients and argument_gradients!\nThe gradient interfaces above conform to JAX typical idioms (setting up functions and jax.grading them). This also provides support for higher-order AD.\nIt’s a pragmatic decision - but ultimately one I’m unsettled by. I’d love to discuss this design decision further.\n\nImplementing MAP\nHere’s a rough version “always accept” MAP using the gradient interfaces (there’s some slight deviations between this code and the standard library version, for pedagogical reasons).\n\n@dataclasses.dataclass\nclass MapUpdate(genjax.Pytree):\n    selection: Selection\n    tau: FloatArray\n\n    def flatten(self):\n        return (self.tau,), (self.selection,)\n\n    def apply(self, key, trace):\n        args = trace.get_args()\n        gen_fn = trace.get_gen_fn()\n        key, forward_gradient_trie = gen_fn.choice_grad(key, trace, self.selection)\n        forward_values = self.selection.filter(trace.strip())\n        forward_values = forward_values.strip()\n        forward_values = jtu.tree_map(\n            lambda v1, v2: v1 + self.tau * v2,\n            forward_values,\n            forward_gradient_trie,\n        )\n        key, (_, _, new_trace, _) = gen_fn.update(key, trace, forward_values, args)\n        return key, (new_trace, True)\n\n    def __call__(self, key, trace):\n        return self.apply(key, trace)"
  },
  {
    "objectID": "hidden/genjax_expo_0.html#inference-v1-map-initialization-mhmala-chain",
    "href": "hidden/genjax_expo_0.html#inference-v1-map-initialization-mhmala-chain",
    "title": "Demo 0: GenJAX performance characteristics",
    "section": "Inference v1: MAP initialization + MH/MALA chain",
    "text": "Inference v1: MAP initialization + MH/MALA chain\nLet’s consider inference utilizing MAP initialization followed by MALA/MH moves.\n\nmap_move = genjax.map_update(genjax.select([\"alpha\"]), 5e-4)\nmala_move = genjax.mala(genjax.select([\"alpha\"]), 5e-4)\nmala_move\n\n\n\n\nMetropolisAdjustedLangevinAlgorithm\n├── selection\n│   └── BuiltinSelection\n│       └── trie\n│           └── Trie\n│               └── :alpha\n│                   └── AllSelection\n└── tau\n    └── (const) 0.0005\n\n\n\nFor MH, we’ll define a proposal which targets the \"outlier\" random variable.\n\n@genjax.gen(genjax.Map, in_axes=(0,))\ndef proposal_kernel(x):\n    is_outlier = trace(\"outlier\", genjax.Bernoulli)(0.1)\n    return is_outlier\n\n\n@genjax.gen\ndef proposal(chm, x):\n    v = trace(\"ys\", proposal_kernel)(x)\n    return v\n\n\nmh_move = genjax.mh(genjax.select([(\"ys\", \"outlier\")]), proposal)\nmh_move\n\n\n\n\nMetropolisHastings\n├── selection\n│   └── BuiltinSelection\n│       └── trie\n│           └── Trie\n│               └── :ys\n│                   └── Trie\n│                       └── :outlier\n│                           └── AllSelection\n└── proposal\n    └── BuiltinGenerativeFunction\n        └── source\n            └── &lt;function proposal&gt;\n\n\n\n\nChain idioms in JAX\nNow we can create two chain functions, one for the MAP initialization and one for the MALA/MH moves.\nWe’ll use jax.lax.scan to create chains of kernel applications. This is a control flow primitive which expresses a fold-like pattern of computation - and lowers to an efficient XLA loop nest.\nAs part of the jax.lax.scan call, we can return the current trace out and create a sequence – we’ll use this sequence to animate the moves below.\n\ndef map_chain(key, initial_trace, length):\n    def _inner(carry, xs):\n        key, current = carry\n        key, (new, _) = map_move(key, current)\n        return (key, new), new\n\n    (key, new), sequence = jax.lax.scan(\n        _inner, (key, initial_trace), None, length=length\n    )\n    return (key, new), sequence\n\n\ndef rejuv_chain(key, initial_trace, length=50):\n    (data,) = initial_trace.get_args()\n\n    def _inner(carry, xs):\n        key, current = carry\n        key, (new, _) = mh_move(key, current, (data,))\n        key, (new, _) = mala_move(key, new)\n        # Here, we pass the x-data - as a convenient way\n        # to use the broadcast axis length in the proposal.\n        key, (new, _) = mh_move(key, new, (data,))\n        return (key, new), new\n\n    (key, new), sequence = jax.lax.scan(\n        _inner, (key, initial_trace), None, length=length\n    )\n    return (key, new), sequence\n\n\ndef mixed_inf(key, initial_trace, length=50):\n    (key, new), map_sequence = map_chain(key, initial_trace, length)\n    (key, new), rejuv_sequence = rejuv_chain(key, new, length)\n    return (key, new), (map_sequence, rejuv_sequence)\n\nLet’s construct an animation using mixed_inf (MAP + MALA/MH).\nFirst, we’ll get the sequence of traces from inference.\n\n# Regenerate our sample curve with 100 points.\nx, y = make_data(100)\n\n# Run MAP, followed by MALA/MH chain.\ndef run_mixed(key, x, y, device=jax.devices()[0]):\n    observations = genjax.choice_map(\n        {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n    )\n    key, (_, initial) = jax.jit(model.importance)(key, observations, (x,))\n    (key, _), (map_s, rejuv_s) = mixed_inf(key, initial)\n    return key, (map_s, rejuv_s)\n\n\n# Get the two update sequences.\nkey, (map_s, rejuv_s) = jax.jit(run_mixed)(key, x, y)\n\nThen, we can construct an animation showing traces from the two chains wiggle into place.\n(The blue line should be a scatter plot! But I couldn’t figure out to make this work with the matplotlib.animation functionality)\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# create a figure and axes\nf, ax = plt.subplots(figsize=(6, 6))\n\n# set up the subplots as needed\nax.set_xlim((-2, 2))\nax.set_ylim((0, 50))\nax.set_xlabel(\"Data\")\nax.set_ylabel(\"Predicted\")\n\n# create objects that will change in the animation. These are\n# initially empty, and will be given new values for each frame\n# in the animation.\ntxt_title = ax.set_title(\"\")\nground_truth = ax.scatter(\n    x,\n    y,\n    cmap=\"viridis\",\n)  # ax.plot returns a list of 2D line objects\n(prediction,) = ax.plot([], [], \"gold\", lw=2)\n\n# animation function. This is called sequentially\ndef drawframe(n):\n    if n &lt; 50:\n        coefficients = map_s[\"alpha\"].inner.get_leaf_value()\n        coefficients = coefficients[n, :]\n        outliers = map_s[\"ys\", \"outlier\"]\n        outliers = outliers[n, :]\n\n    else:\n        k = n - 50\n        coefficients = rejuv_s[\"alpha\"].inner.get_leaf_value()\n        coefficients = coefficients[n, :]\n        outliers = rejuv_s[\"ys\", \"outlier\"]\n        outliers = outliers[n, :]\n\n    v = poly_eval(x, coefficients)\n    prediction.set_data(x, v)\n    data = np.vstack((x, y)).T\n    ground_truth.set_offsets(data)\n    ground_truth.set_array(outliers)\n    ax.set_xlabel(\"Data\")\n    ax.set_ylabel(\"Predicted\")\n    txt_title.set_text(\"(MAP + MALA/MH Chain) Frame = {0:4d}\".format(n))\n    return (ground_truth, prediction)\n\n\nfrom matplotlib import animation\n\n# blit=True re-draws only the parts that have changed.\nanim = animation.FuncAnimation(f, drawframe, frames=100, interval=50, blit=True)\nplt.close()\n\nfrom IPython.display import HTML\n\nHTML(anim.to_html5_video())\n\n/tmp/ipykernel_42331/3474476441.py:20: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  ground_truth = ax.scatter(\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nBenchmarking MAP + MALA/MH\nWe’ll also define a benchmark function for this inference variant.\n\n# Benchmark function.\ndef _mixed_inf(key, initial_trace):\n    (key, new), _ = mixed_inf(key, initial_trace, length=50)\n    score = new.get_score()\n    return key, score\n\n\ndef benchmark_mixed(key, num_data_points, device=jax.devices()[0]):\n    x, y = make_data(num_data_points)\n    observations = genjax.choice_map(\n        {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n    )\n    key, (_, initial) = jax.jit(model.importance)(key, observations, (x,))\n    key, times = benchmark(key, _mixed_inf, initial, device=device)\n    return key, times\n\n\n# Convenience which allows us to scale data and get times back\n# for execution on a specific JAX-managed device.\ndef test_device(key, data_counts, device):\n    times = []\n    for data_count in data_counts:\n        key, ts = %time benchmark_mixed(key, data_count, device=device)\n        m = np.mean(ts)\n        times.append(m)\n\n    return key, np.array(times)\n\n\n# Benchmarking.\ndata_counts = [100, 200, 300, 400, 500, 600]\nf, axes = plt.subplots(1, 2, sharey=True, dpi=280)\ncpu_ax = axes[0]\ngpu_ax = axes[1]\ncpu_ax.set_ylabel(\"Time (s)\")\ncpu_ax.set_xlabel(\"Num. data points\")\ngpu_ax.set_xlabel(\"Num. data points\")\nkey, cpu_times = test_device(key, data_counts, cpu_device)\nkey, gpu_times = test_device(key, data_counts, gpu_device)\ncpu_ax.scatter(data_counts, cpu_times)\ngpu_ax.scatter(data_counts, gpu_times)\nf.suptitle(\"MAP + MALA/MH Runtime (CPU vs. GPU)\")\n\nplt.show()\n\nCPU times: user 11 s, sys: 548 ms, total: 11.6 s\nWall time: 9.83 s\nCPU times: user 10.7 s, sys: 633 ms, total: 11.4 s\nWall time: 11.3 s\nCPU times: user 11.5 s, sys: 830 ms, total: 12.3 s\nWall time: 12.3 s\nCPU times: user 12.9 s, sys: 941 ms, total: 13.9 s\nWall time: 13.8 s\nCPU times: user 13.9 s, sys: 981 ms, total: 14.9 s\nWall time: 14.9 s\nCPU times: user 14.9 s, sys: 1.1 s, total: 16 s\nWall time: 15.9 s\nCPU times: user 17.7 s, sys: 2.34 s, total: 20 s\nWall time: 14.3 s\nCPU times: user 18.2 s, sys: 2.36 s, total: 20.5 s\nWall time: 13.6 s\nCPU times: user 20.4 s, sys: 2.67 s, total: 23.1 s\nWall time: 15.5 s\nCPU times: user 21 s, sys: 3.03 s, total: 24.1 s\nWall time: 16.3 s\nCPU times: user 22.4 s, sys: 2.93 s, total: 25.4 s\nWall time: 17.9 s\nCPU times: user 23.1 s, sys: 2.87 s, total: 26 s\nWall time: 18.6 s"
  },
  {
    "objectID": "hidden/genjax_expo_0.html#inference-v2-importance-sampling-with-mhmala-rejuvenation",
    "href": "hidden/genjax_expo_0.html#inference-v2-importance-sampling-with-mhmala-rejuvenation",
    "title": "Demo 0: GenJAX performance characteristics",
    "section": "Inference v2: Importance sampling with MH/MALA rejuvenation",
    "text": "Inference v2: Importance sampling with MH/MALA rejuvenation\nWe can lift the previous kernel chain functions to map across the particle collection returned by importance sampling using jax.vmap\n\n# Sampling importance resampling w/ MALA/MH rejuvenation.\ndef sir_rejuv(key, *args):\n    inf = genjax.ImportanceSampling.new(1000, model)\n    key, (trs, lnw, lmle) = inf.apply(key, *args)\n    key, sub_keys = genjax.slash(key, 1000)\n    (_, new), _ = jax.vmap(rejuv_chain, in_axes=(0, 0))(sub_keys, trs)\n    key, sub_key = genjax.slash(key, 1)\n    ind = jax.random.categorical(sub_key, lnw)\n    tr = jtu.tree_map(lambda v: v[ind], trs)\n    return key, tr\n\nWe can re-use our earlier plotting recipe.\n\n# Sampling importance resampling\nx, y = make_data(100)\nobservations = genjax.choice_map(\n    {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n)\njitted = jax.jit(sir_rejuv)\n\n# Warmup.\nkey, tr = jitted(key, observations, (x,))\n%time key, tr = jitted(key, observations, (x, ))\n\n# Make plots.\nf, axes = plt.subplots(4, 4, sharex=True, sharey=True, dpi=280)\nf.suptitle(\"Samples from (SIR w/ MH/MALA rejuv) approximate posterior\")\nfor ax in axes.flatten():\n    key, tr = jitted(key, observations, (x,))\n    coefficients = tr[\"alpha\"].inner.get_retval()\n    outliers = tr[\"ys\"].indices[tr[\"ys\", \"outlier\"]]\n    viz(ax, x, y, marker=\".\")\n    viz(ax, x[outliers], y[outliers], marker=\".\", color=\"green\")\n    plot_polynomial_values(ax, data, coefficients, marker=\".\", alpha=0.2, color=\"gold\")\n\nplt.show()\n\nCPU times: user 166 ms, sys: 12.5 ms, total: 179 ms\nWall time: 153 ms\n\n\n\n\n\n\nBenchmarking IS + MALA/MH rejuvenation\nFinally, we can also benchmark this variant.\n\n# Inference function.\ndef _sir_rejuv(key, *args):\n    key, tr = sir_rejuv(key, *args)\n    score = tr.get_score()\n    return key, score\n\n\n# Benchmark function.\ndef benchmark_sir_rejuv(key, num_data_points, device=jax.devices()[0]):\n    x, y = make_data(num_data_points)\n    observations = genjax.choice_map(\n        {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n    )\n    key, times = benchmark(key, _sir_rejuv, observations, (x,), device=device)\n    return key, times\n\n\n# Convenience which allows us to scale data and get times back\n# for execution on a specific JAX-managed device.\ndef test_device(key, data_counts, device):\n    times = []\n    for data_count in data_counts:\n        key, ts = %time benchmark_sir_rejuv(key, data_count, device=device)\n        m = np.mean(ts)\n        times.append(m)\n\n    return key, np.array(times)\n\n\n# Benchmarking.\ndata_counts = [100, 200, 300, 400, 500, 600]\nf, axes = plt.subplots(1, 2, sharey=True, dpi=280)\ncpu_ax = axes[0]\ngpu_ax = axes[1]\ncpu_ax.set_ylabel(\"Time (s)\")\ncpu_ax.set_xlabel(\"Num. data points\")\ngpu_ax.set_xlabel(\"Num. data points\")\nkey, cpu_times = test_device(key, data_counts, cpu_device)\nkey, gpu_times = test_device(key, data_counts, gpu_device)\ncpu_ax.scatter(data_counts, cpu_times)\ngpu_ax.scatter(data_counts, gpu_times)\nf.suptitle(\"SIR + MH/MALA rejuv Runtime (CPU vs. GPU)\")\n\nplt.show()\n\nCPU times: user 3min 34s, sys: 1.93 s, total: 3min 36s\nWall time: 2min 11s\nCPU times: user 8min 29s, sys: 8.49 s, total: 8min 37s\nWall time: 3min 44s\nCPU times: user 13min 42s, sys: 12.2 s, total: 13min 54s\nWall time: 5min 34s\nCPU times: user 19min 42s, sys: 14.8 s, total: 19min 57s\nWall time: 7min 27s\nCPU times: user 24min 56s, sys: 16.8 s, total: 25min 13s\nWall time: 9min 32s\nCPU times: user 30min 39s, sys: 17.3 s, total: 30min 56s\nWall time: 11min 47s\nCPU times: user 25.7 s, sys: 1.73 s, total: 27.5 s\nWall time: 22.4 s\nCPU times: user 20.7 s, sys: 1.53 s, total: 22.2 s\nWall time: 17.2 s\nCPU times: user 24.1 s, sys: 1.66 s, total: 25.7 s\nWall time: 20.5 s\nCPU times: user 27.1 s, sys: 1.87 s, total: 28.9 s\nWall time: 23.6 s\nCPU times: user 30.2 s, sys: 1.69 s, total: 31.9 s\nWall time: 26.7 s\nCPU times: user 32.5 s, sys: 2.05 s, total: 34.6 s\nWall time: 29.3 s"
  },
  {
    "objectID": "hidden/jax_tutorial.html",
    "href": "hidden/jax_tutorial.html",
    "title": "JAX tutorial",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport dataclasses\ndef some_numerical_function(x):\n    return jnp.tanh(x) + 3.0\n(jax.jit, jax.vmap, jax.grad)\n\n(&lt;function jax._src.api.jit(fun: 'Callable', *, static_argnums: 'Union[int, Iterable[int], None]' = None, static_argnames: 'Union[str, Iterable[str], None]' = None, device: 'Optional[xc.Device]' = None, backend: 'Optional[str]' = None, donate_argnums: 'Union[int, Iterable[int]]' = (), inline: 'bool' = False, keep_unused: 'bool' = False, abstracted_axes: 'Optional[Any]' = None) -&gt; 'stages.Wrapped'&gt;,\n &lt;function jax._src.api.vmap(fun: 'F', in_axes: 'Union[int, Sequence[Any]]' = 0, out_axes: 'Any' = 0, axis_name: 'Optional[Hashable]' = None, axis_size: 'Optional[int]' = None, spmd_axis_name: 'Optional[Hashable]' = None) -&gt; 'F'&gt;,\n &lt;function jax._src.api.grad(fun: 'Callable', argnums: 'Union[int, Sequence[int]]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False, reduce_axes: 'Sequence[AxisName]' = ()) -&gt; 'Callable'&gt;)\nfn = jax.make_jaxpr(some_numerical_function)\nfn(5.0)\n\n\n{ lambda ; a:f32[]. let b:f32[] = tanh a; c:f32[] = add b 3.0 in (c,) }\nfn = jax.jit(some_numerical_function)\nfn(5.0)\n\nArray(3.9999092, dtype=float32, weak_type=True)\nfn = jax.jit(jax.vmap(jax.grad(some_numerical_function)))\nfn(jnp.array([3.0, 5.0]))\n\nArray([0.0098661 , 0.00018167], dtype=float32)\njaxpr = jax.make_jaxpr(jax.grad(some_numerical_function))(5.0)\njaxpr\n\n\n{ lambda ; a:f32[]. let\n    b:f32[] = tanh a\n    c:f32[] = sub 1.0 b\n    _:f32[] = add b 3.0\n    d:f32[] = mul 1.0 c\n    e:f32[] = mul d b\n    f:f32[] = add_any d e\n  in (f,) }"
  },
  {
    "objectID": "hidden/jax_tutorial.html#pytrees",
    "href": "hidden/jax_tutorial.html#pytrees",
    "title": "JAX tutorial",
    "section": "Pytrees",
    "text": "Pytrees\n\nimport genjax\nfrom typing import Any\n\n\n@dataclasses.dataclass\nclass SomeFoo(genjax.Pytree):\n    x: float\n    y: Any\n\n    def flatten(self):\n        return (self.x, self.y), ()\n\n\ndef some_numerical_function(foo):\n    def _inner():\n        return foo.x\n\n    return jnp.tanh(_inner()) + 3.0\n\n\nfoo = SomeFoo(5.0, SomeFoo(5.0, None))\nleaves, form = jax.tree_util.tree_flatten(foo)\nleaves\n\n[5.0, 5.0]\n\n\n\njax.jit(some_numerical_function)(SomeFoo(5.0, None))\n\nArray(3.9999092, dtype=float32, weak_type=True)\n\n\n\n# Importing Jax functions useful for tracing/interpreting.\nimport numpy as np\nfrom functools import wraps\n\nfrom jax import core\nfrom jax import lax\nfrom jax._src.util import safe_map\n\n\ndef interp(fn):\n    def eval_jaxpr(jaxpr, consts, *args):\n        # Mapping from variable -&gt; value\n        env = {}\n\n        def read(var):\n            # Literals are values baked into the Jaxpr\n            if type(var) is core.Literal:\n                return var.val\n            return env[var]\n\n        def write(var, val):\n            env[var] = val\n\n        # Bind args and consts to environment\n        safe_map(write, jaxpr.invars, args)\n        safe_map(write, jaxpr.constvars, consts)\n\n        # Loop through equations and evaluate primitives using `bind`\n        for eqn in jaxpr.eqns:\n            # Read inputs to equation from environment\n            invals = safe_map(read, eqn.invars)\n            # `bind` is how a primitive is called\n            outvals = eqn.primitive.bind(*invals, **eqn.params)\n            # Primitives may return multiple outputs or not\n            if not eqn.primitive.multiple_results:\n                outvals = [outvals]\n            # Write the results of the primitive into the environment\n            safe_map(write, eqn.outvars, outvals)\n        # Read the final result of the Jaxpr from the environment\n        return safe_map(read, jaxpr.outvars)\n\n    def _inner(*args):\n        jaxpr = jax.make_jaxpr(fn)(*args)\n        v = eval_jaxpr(jaxpr.jaxpr, jaxpr.consts, *args)\n        return v\n\n    return _inner\n\n\njax.jit(interp(lambda x: x + 5.0))(5.0)\n\n[Array(10., dtype=float32, weak_type=True)]\n\n\n\n@genjax.gen\ndef model():\n    x = genjax.Normal(0.0, 1.0) @ \"x\"\n    return x\n\n\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\ntr\n\nBuiltinTrace(gen_fn=BuiltinGenerativeFunction(source=&lt;function model at 0x16b0c42c0&gt;), args=(), retval=Array(-0.10823099, dtype=float32), choices=Trie(inner={'x': DistributionTrace(gen_fn=_Normal(), args=(0.0, 1.0), value=Array(-0.10823099, dtype=float32), score=Array(-0.9247955, dtype=float32))}), cache=Trie(inner={}), score=Array(-0.9247955, dtype=float32))\n\n\n\ndef fn(index):\n    x = jnp.ones(5)\n    y = x.at[index].set(3.0)\n    return y[index]\n\n\njax.jit(fn)(3)\n\nArray(3., dtype=float32)\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport genjax\n\nconsole = genjax.pretty()\n\n\n@genjax.gen\ndef add_normal_noise(x):\n    noise1 = genjax.trace(\"noise1\", genjax.Normal)(0.0, 1.0)\n    noise2 = genjax.trace(\"noise2\", genjax.Normal)(0.0, 1.0)\n    return (key, x + noise1 + noise2)\n\n\n@genjax.gen\ndef my_map():\n    mapped = genjax.Map(add_normal_noise, in_axes=(0,))\n    arr = jnp.ones(100)\n    mapped(arr) @ \"map\"\n\n\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.simulate(my_map)(key, ())\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function my_map&gt;\n├── args\n│   └── tuple\n├── retval\n│   └── (const) None\n├── choices\n│   └── Trie\n│       └── :map\n│           └── VectorTrace\n│               ├── gen_fn\n│               │   └── MapCombinator\n│               │       ├── in_axes\n│               │       │   └── tuple\n│               │       │       └── (const) 0\n│               │       ├── repeats\n│               │       │   └── (const) None\n│               │       └── kernel\n│               │           └── BuiltinGenerativeFunction\n│               │               └── source\n│               │                   └── &lt;function add_normal_noise&gt;\n│               ├── indices\n│               │   └── (numpy) i64[100]\n│               ├── inner\n│               │   └── BuiltinTrace\n│               │       ├── gen_fn\n│               │       │   └── BuiltinGenerativeFunction\n│               │       │       └── source\n│               │       │           └── &lt;function add_normal_noise&gt;\n│               │       ├── args\n│               │       │   └── tuple\n│               │       │       └──  f32[100]\n│               │       ├── retval\n│               │       │   └── tuple\n│               │       │       ├──  u32[100,2]\n│               │       │       └──  f32[100]\n│               │       ├── choices\n│               │       │   └── Trie\n│               │       │       ├── :noise1\n│               │       │       │   └── DistributionTrace\n│               │       │       │       ├── gen_fn\n│               │       │       │       │   └── _Normal\n│               │       │       │       ├── args\n│               │       │       │       │   └── tuple\n│               │       │       │       │       ├──  f32[100]\n│               │       │       │       │       └──  f32[100]\n│               │       │       │       ├── value\n│               │       │       │       │   └──  f32[100]\n│               │       │       │       └── score\n│               │       │       │           └──  f32[100]\n│               │       │       └── :noise2\n│               │       │           └── DistributionTrace\n│               │       │               ├── gen_fn\n│               │       │               │   └── _Normal\n│               │       │               ├── args\n│               │       │               │   └── tuple\n│               │       │               │       ├──  f32[100]\n│               │       │               │       └──  f32[100]\n│               │       │               ├── value\n│               │       │               │   └──  f32[100]\n│               │       │               └── score\n│               │       │                   └──  f32[100]\n│               │       ├── cache\n│               │       │   └── Trie\n│               │       └── score\n│               │           └──  f32[100]\n│               ├── args\n│               │   └── tuple\n│               │       └──  f32[100]\n│               ├── retval\n│               │   └── tuple\n│               │       ├──  u32[100,2]\n│               │       └──  f32[100]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport genjax\n\nconsole = genjax.pretty()\n\n\n@genjax.gen\ndef add_normal_noise(x):\n    noise1 = genjax.trace(\"noise1\", genjax.Normal)(0.0, 1.0)\n    noise2 = genjax.trace(\"noise2\", genjax.Normal)(0.0, 1.0)\n    return (key, x + noise1 + noise2)\n\n\n@genjax.gen\ndef my_map():\n    mapped = genjax.Map(add_normal_noise, in_axes=(0,))\n    arr = jnp.ones(100)\n    mapped(arr) @ \"map\"\n\n\nkey = jax.random.PRNGKey(314159)\ntr = genjax.simulate(my_map)(key, ())"
  },
  {
    "objectID": "internals/impl_builtin_language/impl_builtin_language.html",
    "href": "internals/impl_builtin_language/impl_builtin_language.html",
    "title": "Implementing the builtin modeling language",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import GenerativeFunction, ChoiceMap, Selection, trace\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nOne key property of the generative function interface is that it enables a separation between model and inference code - providing an abstraction layer that facilitates the development of modular model pieces, and then inference pieces that abstract over the implementation of the interface.\nNow, implementing the interface on objects, and composing them in various ways (by e.g. specializing the implementation of the interface functions to support any intended composition) is a valid way to construct new generative functions. In fact, this is the pattern which generative function combinators follow - they accept generative functions as input, and produce new generative functions whose implementations are specialized to represent some specific pattern of computation.\nExplicitly constructing generative functions using languages of objects, however, can often feel unwieldy. Part of the way that GenJAX (and Gen.jl) alleviates this restriction is by exposing languages which construct generative functions from programs. This drastically increases the expressivity available to the programmer.\nIn GenJAX, here’s an example of the BuiltinGenerativeFunction language:\n@genjax.gen\ndef model(x):\n    y = genjax.trace(\"y\", genjax.Normal)(x, 1.0)\n    z = genjax.trace(\"z\", genjax.Normal)(y + x, 1.0)\n    return z\nWhen we apply one of the interface functions to this object, we get the associated data types that we expect.\nkey, tr = model.simulate(key, (1.0,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function model&gt;\n├── args\n│   └── tuple\n│       └── (const) 1.0\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :y\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├── (const) 1.0\n│       │       │       └── (const) 1.0\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :z\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └── (const) 1.0\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\nHow exactly do we do this? In this notebook, you’re going to find out. You’ll also get a chance to explore some of the capabilities which JAX exposes to library designers. Ideally, you’ll also get a sense of some of the limitations of JAX (and GenJAX) - which are restricted to support programs which are amenable to GPU/TPU acceleration."
  },
  {
    "objectID": "internals/impl_builtin_language/impl_builtin_language.html#the-magic-of-jax",
    "href": "internals/impl_builtin_language/impl_builtin_language.html#the-magic-of-jax",
    "title": "Implementing the builtin modeling language",
    "section": "The magic of JAX",
    "text": "The magic of JAX\nLet’s examine the generative function object:\n\nmodel\n\n\n\n\nBuiltinGenerativeFunction\n└── source\n    └── &lt;function model&gt;\n\n\n\nAll the decorator genjax.gen does is wrap the function into this object. It holds a reference to the function we defined above.\nBut clearly, we need to somehow get inside that function - because we’re recording data onto the BuiltinTrace which come from intermediate results of the execution of the function.\nThat’s where JAX comes in - JAX provides a way to trace pure, numerical Python programs - enabling us to construct program transformations which return new functions that compute different semantics from the original function.11 Program tracing is an approach which has its roots in automatic differentiation. If you’re interesting in this technique, I cannot recommend Autodidax: JAX core from scratch enough. It will introduce you to enough interesting PL ideas to keep you occupied for months, if not years.\nLet’s utilize one of JAX’s interpreters to construct an intermediate representation of the function which our generative function object holds reference to:\n\njaxpr = jax.make_jaxpr(model.source)(1.0)\njaxpr\n\n{ lambda ; a:f32[]. let\n    b:key&lt;fry&gt;[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap b\n    c:f32[] = trace[addr=y gen_fn=_Normal() tree_in=PyTreeDef((*, *))] a 1.0\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[] = add c d\n    f:key&lt;fry&gt;[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap f\n    g:f32[] = trace[addr=z gen_fn=_Normal() tree_in=PyTreeDef((*, *))] e 1.0\n  in (g,) }\n\n\n\nSo jax.make_jaxpr takes a function f :: A -&gt; B and returns a function f :: A -&gt; Jaxpr, where Jaxpr is the program representation above.\nWhen we run this function using Python’s interpreter, JAX lifts the input to something called a Tracer, JAX keeps an internal stack of interpreters which redirect infix operations on Tracer instances and modify their behavior. Additionally, JAX exposes new primitives (like all the NumPy primitives) which wrap a function called bind. bind takes in Tracer arguments, looks through them (and the interpreter stack), selects the interpreter which should handle the call - and then the interpreter is allowed to process_primitive - invoking the semantics which the interpreter defines for that primitive.\njax.make_jaxpr uses the above process to walk the program, and construct the above intermediate representation.\nNow, the point of having this representation is that we can transform it further! We can lower it to other representations (including things like XLA - the linear algebra accelerator that JAX utilizes to go high performance). We could also write another interpreter which walks this representation, invokes other primitives with bind, etc - deferring further transformation to the next interpreter in line.\nThis (admittedly rough description) above is the secret behind JAX’s compositional transformations."
  },
  {
    "objectID": "internals/impl_builtin_language/impl_builtin_language.html#new-semantics-via-program-transformations",
    "href": "internals/impl_builtin_language/impl_builtin_language.html#new-semantics-via-program-transformations",
    "title": "Implementing the builtin modeling language",
    "section": "New semantics via program transformations",
    "text": "New semantics via program transformations\nLet’s examine the representation once more.\n\njaxpr\n\n{ lambda ; a:f32[]. let\n    b:key&lt;fry&gt;[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap b\n    c:f32[] = trace[addr=y gen_fn=_Normal() tree_in=PyTreeDef((*, *))] a 1.0\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[] = add c d\n    f:key&lt;fry&gt;[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap f\n    g:f32[] = trace[addr=z gen_fn=_Normal() tree_in=PyTreeDef((*, *))] e 1.0\n  in (g,) }\n\n\n\nYou’ll notice that there is an intrinsic called trace here - which looks suspiciously similar to genjax.trace above.\ntrace is a custom primitive that GenJAX defines - by defining a new primitive, we can place a stub in the intermediate representation, which we can further transform to implement the semantics we wish to express.\n\nA high level view\nNow, we need to transform it! Here’s where some serious design decisions enter into the picture.\nOne thing you might notice about the Jaxpr above is that the the arity of the function is fixed, and so is the arity of the return value. But when we call simulate on our model - we get out something which is not a h :: f32[] (it’s actually a jax.Pytree with a lot more data - so we’d expect a lot more return values in the Jaxpr2.2 JAX flattens/unflattens Pytree instances on each side of the IR boundary - the IR is strongly typed, but only natively supports a few base types, and a few composite array types.\nWhat gives?\nHere’s where JAX’s support for compositional application of interpreters comes into play.\nInstead of attempting to modify the IR above to change the arity of everything (a process which the authors expect would be quite painful, and buggy) - we can write another interpreter which walks the IR and evaluates it, but that interpreter can keep track of the state that we want to put into the BuiltinTrace at the end of the interface invocation.\nThen, we can stage out that interpreter to support JIT compilation, etc. I’ll describe the process below in pseudo-types:\nWe start with f :: A -&gt; B, and we stage it to get a new function f' :: Type[A] -&gt; Jaxpr, then we write an interpreter I with signature I :: (Jaxpr, A) -&gt; (B, State). The application of I itself can also be staged.\nSo this is really nice - we don’t have to munge the IR manually, we just get to write an interpreter to do the transformation for us. That’s the power that JAX provides for us!\n\n\nInterpreter design decisions\nWith the high-level view in mind, we’ll examine two of the interface implementations. The first is simulate - likely the easiest implementation to understand3. The second is update.3 For this notebook, we’re going to ignore the inference math that we wish to support!\nNow, in GenJAX, the interpreter is written to be re-usable for each of the interface functions. Because we’ve chosen to re-use the interpreter (and parametrize the transformation semantics by configuring the interpreter in other ways – besides the implementation), you’re going to see some complexity right out the gate.\nThe reason why this complexity is there is because we wish to expose incremental computing optimizations in update. To support this customization, the interpreter can best be described as a propagation interpreter - similar to Julia’s abstract interpretation machinery (if you’re familiar). A propagation interpreter treats the Jaxpr as an undirected graph - and performs interpretation by iterating until a fixpoint condition is satisfied.\nThe high level pattern from the previous section is still true! But if you’ve written interpreters for something like Structure and Interpretation of Computer Programs before - this interpreter might be a slight shock to the system.\nHere’s a boiled down form of the simulate_transform:\n\ndef simulate_transform(f, **kwargs):\n    def _inner(key, args):\n        # Step 1: stage out the function to a `Jaxpr`.\n        closed_jaxpr, (flat_args, in_tree, out_tree) = stage(f)(key, *args, **kwargs)\n        jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n\n        # Step 2: create a `Simulate` instance, which we parametrize\n        # the propagation interpreter with.\n        #\n        # `Bare` is an instance of something called a `Cell` - the\n        # objects which the propagation interpreter reasons about.\n        handler = Simulate()\n        final_env, ret_state = propagate(\n            Bare,\n            bare_propagation_rules,\n            jaxpr,\n            [Bare.new(v) for v in consts],\n            list(map(Bare.new, flat_args)),\n            [Bare.unknown(var.aval) for var in jaxpr.outvars],\n            handler=handler,\n        )\n\n        # Step 3: when the interpreter finishes, we read the values\n        # out of its environment.\n        flat_out = safe_map(final_env.read, jaxpr.outvars)\n        flat_out = map(lambda v: v.get_val(), flat_out)\n        key_and_returns = jtu.tree_unflatten(out_tree, flat_out)\n        key, *retvals = key_and_returns\n        retvals = tuple(retvals)\n\n        # Here's the handler state - remember the signature from\n        # above `I :: (Jaxpr, A) -&gt; (B, State)`, these fields\n        # below are the `State`.\n        score = handler.score\n        chm = handler.choice_state\n        cache = handler.cache_state\n\n        # This returns all the things which we want to put\n        # into `BuiltinTrace`.\n        return key, (f, args, retvals, chm, score), cache\n\n    return _inner\n\nAnd, just to show you that this is the key behind how we implement simulate, I’ve copied the BuiltinGenerativeFunction class method for simulate below:\n\ndef simulate(self, key, args, **kwargs):\n    assert isinstance(args, Tuple)\n    key, (f, args, r, chm, score), cache = simulate_transform(self.source, **kwargs)(\n        key, args\n    )\n    return key, BuiltinTrace(self, args, r, chm, cache, score)\n\nWe’ll discuss propagate in a moment - but a few high-level things.\nNote that the simulate method can be staged out / used with JAX’s interfaces:\n\njitted = jax.jit(model.simulate)\nkey, tr = jitted(key, (1.0,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function model&gt;\n├── args\n│   └── tuple\n│       └──  f32[]\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :y\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├──  f32[]\n│       │       │       └──  f32[]\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :z\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └──  f32[]\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nThat’s because simulate_transform and the interpreter implementation itself for propagate are all JAX traceable.\nThe only difference between the BuiltinTrace which we first generated at the top of the notebook and this one is that jax.jit will lift the 1.0 argument to a Tracer type, versus the non-jitted interpreter which just uses the Python float value.\nAnd again, we can also stage out our simulate implementation and get a Jaxpr back:\n\njax.make_jaxpr(model.simulate)(key, (1.0,))\n\n{ lambda ; a:u32[2] b:f32[]. let\n    c:key&lt;fry&gt;[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap c\n    d:key&lt;fry&gt;[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap d\n    e:key&lt;fry&gt;[] = random_wrap[impl=fry] a\n    f:key&lt;fry&gt;[2] = random_split[count=2] e\n    g:u32[2,2] = random_unwrap f\n    h:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] g\n    i:u32[2] = squeeze[dimensions=(0,)] h\n    j:u32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=(1, 1)] g\n    k:u32[2] = squeeze[dimensions=(0,)] j\n    l:key&lt;fry&gt;[] = random_wrap[impl=fry] k\n    m:u32[] = random_bits[bit_width=32 shape=()] l\n    n:u32[] = shift_right_logical m 9\n    o:u32[] = or n 1065353216\n    p:f32[] = bitcast_convert_type[new_dtype=float32] o\n    q:f32[] = sub p 1.0\n    r:f32[] = sub 1.0 -0.9999999403953552\n    s:f32[] = mul q r\n    t:f32[] = add s -0.9999999403953552\n    u:f32[] = reshape[dimensions=None new_sizes=()] t\n    v:f32[] = max -0.9999999403953552 u\n    w:f32[] = erf_inv v\n    x:f32[] = mul 1.4142135381698608 w\n    y:f32[] = mul 1.0 x\n    z:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    ba:f32[] = add z y\n    bb:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    bc:f32[] = sub ba bb\n    bd:f32[] = div bc 1.0\n    be:f32[] = abs bd\n    bf:f32[] = integer_pow[y=2] be\n    bg:f32[] = log 6.283185307179586\n    bh:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bg\n    bi:f32[] = add bf bh\n    bj:f32[] = mul -1.0 bi\n    bk:f32[] = log 1.0\n    bl:f32[] = sub 2.0 bk\n    bm:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bl\n    bn:f32[] = div bj bm\n    bo:f32[] = reduce_sum[axes=()] bn\n    bp:f32[] = add 0.0 bo\n    bq:f32[] = add ba b\n    br:key&lt;fry&gt;[] = random_wrap[impl=fry] i\n    bs:key&lt;fry&gt;[2] = random_split[count=2] br\n    bt:u32[2,2] = random_unwrap bs\n    bu:u32[1,2] = slice[\n      limit_indices=(1, 2)\n      start_indices=(0, 0)\n      strides=(1, 1)\n    ] bt\n    bv:u32[2] = squeeze[dimensions=(0,)] bu\n    bw:u32[1,2] = slice[\n      limit_indices=(2, 2)\n      start_indices=(1, 0)\n      strides=(1, 1)\n    ] bt\n    bx:u32[2] = squeeze[dimensions=(0,)] bw\n    by:key&lt;fry&gt;[] = random_wrap[impl=fry] bx\n    bz:u32[] = random_bits[bit_width=32 shape=()] by\n    ca:u32[] = shift_right_logical bz 9\n    cb:u32[] = or ca 1065353216\n    cc:f32[] = bitcast_convert_type[new_dtype=float32] cb\n    cd:f32[] = sub cc 1.0\n    ce:f32[] = sub 1.0 -0.9999999403953552\n    cf:f32[] = mul cd ce\n    cg:f32[] = add cf -0.9999999403953552\n    ch:f32[] = reshape[dimensions=None new_sizes=()] cg\n    ci:f32[] = max -0.9999999403953552 ch\n    cj:f32[] = erf_inv ci\n    ck:f32[] = mul 1.4142135381698608 cj\n    cl:f32[] = mul 1.0 ck\n    cm:f32[] = add bq cl\n    cn:f32[] = sub cm bq\n    co:f32[] = div cn 1.0\n    cp:f32[] = abs co\n    cq:f32[] = integer_pow[y=2] cp\n    cr:f32[] = log 6.283185307179586\n    cs:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cr\n    ct:f32[] = add cq cs\n    cu:f32[] = mul -1.0 ct\n    cv:f32[] = log 1.0\n    cw:f32[] = sub 2.0 cv\n    cx:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cw\n    cy:f32[] = div cu cx\n    cz:f32[] = reduce_sum[axes=()] cy\n    da:f32[] = add bp cz\n  in (bv, b, cm, b, 1.0, ba, bo, bq, 1.0, cm, cz, da) }\n\n\n\nGiving us our pure, array math code. You can’t help but admit that that’s pretty elegant!"
  },
  {
    "objectID": "internals/impl_builtin_language/impl_builtin_language.html#how-does-propagate-work",
    "href": "internals/impl_builtin_language/impl_builtin_language.html#how-does-propagate-work",
    "title": "Implementing the builtin modeling language",
    "section": "How does propagate work?",
    "text": "How does propagate work?\nNow, in this section - we’re going to talk about the nitty gritty of propagate itself. What exactly is this interpreter doing? Let’s examine the context surrounding the call to propagate:\ndef simulate_transform(f, **kwargs):\n    def _inner(key, args):\n        closed_jaxpr, (flat_args, in_tree, out_tree) = stage(f)(\n            key, *args, **kwargs\n        )\n        jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n        handler = Simulate()\n        final_env, ret_state = propagate(\n            # A lattice type\n            Bare,\n            \n            # Lattice propagation rules\n            bare_propagation_rules,\n            \n            # The Jaxpr which we wish to interpret\n            jaxpr,\n            \n            # Trace-time constants\n            [Bare.new(v) for v in consts],\n            \n            # Input cells\n            list(map(Bare.new, flat_args)),\n            \n            # Output cells\n            [Bare.unknown(var.aval) for var in jaxpr.outvars],\n            \n            # How we handle `trace`.\n            handler=handler,\n        )\n        ...\n\n    return _inner\nFirst, we stage our model function into a Jaxpr - when we perform the staging process, everything (e.g. custom datatypes which are Pytree implementors) gets flattened out to array leaves.\nAfter we stage, we collect all the data which we want to use to initialize our interpreter’s environment with - but we encounter our first bit of complexity.\nWhat is Bare? And what is a Cell? Let’s start with the latter question: a Cell is an abstract type which represents a lattice value.\nTo understand what a lattice value is - it’s worth gaining a high-level picture of what propagate attempts to do. propagate is an interpreter based on mixed concrete/abstract interpretation - it treats the Jaxpr as a graph - where the operations are nodes in the graph, and the SSA values (e.g. the named registers like ci, cj, etc) are edges.\nThe interpreter will iterate over the graph - attempting to update information about the edges by applying propagation rules (hence the name, propagate) which we define (bare_propagation_rules, above).\nA propagation rule accepts a list of input cells (the SSA edges which flow into the operation) and a list of output cells. It returns a new modified list of input cells, and a new modified list of output cells, as well as a state value (in this notebook, we won’t discuss the state value - it’s unneeded for the interfaces we will describe).\nThe way the interpreter works is that it keeps a queue of nodes and an environment which maps SSA values to lattice values. We pop a node off the queue, grab the existing lattice values for input SSA values and output SSA values, attempt to update them using a propagation rule, and then store the update in the environment. In addition, after we attempt to update the cells - we determine if the update has changed the information level of any of the cells. If the information level has changed for any cell (as measured using the partial order on lattice values), we add any nodes which the SSA value associated with that cell flows into back onto the queue.\nThis process describes an iterative algorithm which attempts to compute an information fixpoint - defined by a state transition function (which operates on the state of all cells in the Jaxpr - the environment) which we get to customize using propagation rules.\nI’m not going to inline any of the implementation of this interpreter into this notebook. I’ll refer the reader to the implementation of the interpreter.44 Note that the ideas behind this interpreter are quite widespread - but the original implementation (which the GenJAX authors modified) came from Oryx, and that implementation initially came from Roy Frostig (as far as we can tell).\n\nWhat happens in simulate?\nGreat - so how do we utilize this interpreter idea to implement the simulate_transform described above?"
  },
  {
    "objectID": "internals/developers/jax_intro.html",
    "href": "internals/developers/jax_intro.html",
    "title": "Developer’s intro to JAX",
    "section": "",
    "text": "import jax"
  },
  {
    "objectID": "internals/developers/jax_intro.html#jax-basics",
    "href": "internals/developers/jax_intro.html#jax-basics",
    "title": "Developer’s intro to JAX",
    "section": "JAX basics",
    "text": "JAX basics\n\nCommon transformations\n\n\nPytrees"
  },
  {
    "objectID": "internals/developers/jax_intro.html#primitives-and-interpreters",
    "href": "internals/developers/jax_intro.html#primitives-and-interpreters",
    "title": "Developer’s intro to JAX",
    "section": "Primitives and interpreters",
    "text": "Primitives and interpreters\n\nimport jax.core as jc\n\n\nPrimitives\nJAX allows you to define primitives, and give them custom type interpreters, implementation semantics, and lowering.\n\nfoo_p = jc.Primitive(\"foo\")\n\nThe way JAX developers typically expose primitives is by defining a wrapper function which wraps the primitive and calls Primitive.bind.\nLet’s do something slightly naive now, and we’ll refactor it later when we learn a bit more about JAX.\n\ndef foo(*args):\n    return foo_p.bind(*args)\n\nWe can attempt to use this primitive now in JAX code.\n\ndef f(x):\n    return foo(x)\n\n\njax.make_jaxpr(f)(5.0)\n\nNotImplementedError: Abstract evaluation for 'foo' not implemented\n\n\nIt’s upset - because JAX needs to know how to pass symbolic tracer values through the primitive to trace it.\nHere’s how we define this for a new primitive.\n\ndef abstract_foo(*args):\n    return args\n\n\nPrimitive abstract base classes\nGenJAX supports convenient base classes for primitives that automatically implement several of the methods I’ve covered above. I (McCoy) originally found this pattern in Oryx, and it was too good to not adopt.\nSome of the patterns here will make more sense after the Interpreters section, where I discuss “initial” and “final” style primitives / tracing.\n\n\n\nInterpreters\nInterpreters can be written in two styles - one is significantly easier to understand, but offers somewhat poor debugging (to users), the second is more complicated - but has nicer debugging.\nHere, we’ll cover the first style - later on in development, we may switch some of the interpreters over to the second style, so I’ll mention it at the end."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html",
    "href": "concepts/introduction/intro_to_genjax.html",
    "title": "Introduction to Gen and GenJAX",
    "section": "",
    "text": "%config InlineBackend.figure_format = 'svg'\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import GenerativeFunction, ChoiceMap, Selection, trace\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)"
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#what-is-genjax",
    "href": "concepts/introduction/intro_to_genjax.html#what-is-genjax",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is GenJAX?",
    "text": "What is GenJAX?\nHere are a few high-level ways to think about GenJAX:\n\nA probabilistic programming1 system based on the concepts of Gen.\nA Bayesian modelling and inference compiler with support for device acceleration (courtesy of JAX).\nA base layer for experiments in model and inference DSL design.\n\n1 New to probabilistic programming? Don’t fret, read on!There’s already a well-supported implementation of Gen in Julia. Why is a JAX port interesting?\nThere are a number of compelling technical and social reasons to explore Gen’s probabilistic programming paradigm on top of JAX, here are a few:\n\nJAX’s excellent accelerator support - our implementation natively supports several common accelerator idioms - like automatic struct-of-array representations, and the ability to automatically batch model/inference programs onto accelerators.\nJAX’s excellent support for compositional AD removes implementation and maintenance complexity for Gen’s gradient interfaces - previously a difficult challenge in other implementations. In addition, JAX’s support for convenient, higher-order AD opens up new opportunities to explore during inference design with gradient interfaces.\nJAX exposes compositional code transformations to library authors, and, as library authors, we can utilize code transformations to implement state-of-the-art optimizations for models and inference expressed in our system.\nA lot of domain experts and modelers are working in Python! Some of them even use JAX (hopefully more each year). Presenting an interface to Gen which is familiar, and takes advantage of JAX’s native ecosystem is a compelling social reason.\n\nLet’s truncate the list there for now.\nFor the JAX literati, one final (hopefully tantalizing) takeaway: by construction, all GenJAX modeling + inference code is JAX traceable - and thus, vmapable, jitable, etc."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#what-is-probabilistic-programming",
    "href": "concepts/introduction/intro_to_genjax.html#what-is-probabilistic-programming",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is probabilistic programming?",
    "text": "What is probabilistic programming?\nPerhaps you may be coming to this notebook without any prior knowledge in probabilistic programming…\nThat’s okay! Ideally, the ideas in this notebook should be self-contained2.2 You may miss why generative functions (see below) are designed the way they are on a first read - but you’ll still get the punchline if you follow the notebook to the end.\n\nA Bayesian viewpoint\nHere’s one practical take on what probabilistic programming is all about: programming language design for expressing and solving Bayesian inference problems3.3 In the Probabilistic Computing lab at MIT, we also consider differentiable programming to be contained within the set of concerns of probabilistic programming. We won’t cover differentiable programming interfaces in this notebook.\nProbabilistic programming is a broad field, and there are corners which may not be covered by this practical take. We’ll just assume that people are interested in Bayes, and how to represent Bayes on computers in nice ways. For our purposes in this notebook, we’ll stick as much as we can to the basics.\n\n\nWhat are we actually computing with?\nThe objects which we program with expose a mixture of generative and differentiable interfaces - the interfaces are designed to support common (as well as quite advanced) classes of Bayesian inference algorithms. Gen provides automation for the tricky math which these algorithms require.\nWe separate the design of inference (whose implementation uses the interfaces), from the implementation of the interfaces on computational objects. This allows us to build languages of objects which satisfy the interfaces - and allows their compositional usage and interoperability.\nIn Gen, the objects which implement the interfaces are called generative functions."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#what-is-a-generative-function",
    "href": "concepts/introduction/intro_to_genjax.html#what-is-a-generative-function",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is a generative function?",
    "text": "What is a generative function?\nGenerative functions are the key concept of Gen’s probabilistic programming paradigm. Generative functions are computational objects defined by a set of associated data types and methods. These types and methods describe compositional interfaces that are useful for Bayesian inference computations.\nGen’s formal description of generative functions consist of two objects:\n\n\\(P(\\tau, r; x)\\) - a normalized measure over tree-like data (choice maps) and untraced randomness4 \\(r\\), parametrized by arguments \\(x\\).\n\\(f(\\tau; x)\\) - a deterministic function from the above measure’s sample space to a space of data types.\n\n4 More on this later. It’s safe to say “I have no idea what that is” for now, and expect us to explain later or in another notebook.We can informally think of the sampling semantics of these objects as consisting of two steps:\n\nFirst, sample a choice map from \\(P\\).\nThen, compute the return value using \\(f\\).\n\nIn many of the generative function interfaces, we won’t just be interested in the final sampled return value. We’ll also be interested in what happened along the way: we’ll record the intermediate and final results of these steps in Trace objects - data structures which contain the recordings of values, along with probabilistic metadata like the score of random choices selected along the way.\nBelow, we provide an example of an (admittedly, not too interesting) GenJAX generative function5.5 There’s not just one generative function class - users can and are encouraged to design new types of generative functions which capture repeated modeling patterns. An excellent example of this modularity in Gen’s design is generative function combinators.\nThis generative function is part of a function-like language (the BuiltinGenerativeFunction language) - pay close attention to the hierarchical compositionality of generative functions in this language under an abstraction (genjax.trace) similar to a function call. We’ll discuss the addresses (\"sub\" and \"m0\") a bit later.\n\n@genjax.gen\ndef g(x):\n    m0 = genjax.trace(\"m0\", genjax.bernoulli)(x)  # unsweetened\n    return m0\n\n\n@genjax.gen\ndef h(x):\n    m0 = g(x) @ \"sub\"  # sweetened\n    return m0\n\n\nh\n\n\n\n\nBuiltinGenerativeFunction\n└── source\n    └── PytreeClosure\n        ├── callable\n        │   └── &lt;function h&gt;\n        └── environment\n            └── list\n\n\n\nThis generative function holds a Python Callable object. For this generative function language, the interface methods (see the list under Generative function interface below) which are useful for modeling and inference are given semantics via JAX’s tracing and program transformation infrastructure.\nLet’s examine some of these operations now.\n\nconsole.inspect(h.simulate, methods=True)\n\n╭─ &lt;bound method BuiltinGenerativeFunction.simulate of BuiltinGenerativeFuncti─╮\n│ def BuiltinGenerativeFunction.simulate(key: jaxtyping.UInt[Array, '...'], ar │\n│                                                                              │\n│ &gt; Given a `PRNGKey` and arguments, execute the generative function, returnin │\n│                                                                              │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                 │\n╰──────────────────────────────────────────────────────────────────────────────╯\n\n\n\nThis is our first glimpse of the generative function interface (GFI), the secret sauce which Gen is based around.\n\n\n\n\n\n\nJAX interfaces\n\n\n\nThere’s a few methods here which are not part of the GFI, but are worth mentioning because they deal with data interfaces to JAX:\n\nflatten - which allows us to treat generative functions as Pytree implementors.\nunflatten - same as above.\n\nThese are used to register the implementor type as a Pytree, which is roughly a tree-like Python structure which JAX can zip/unzip at runtime API boundaries.\n\n\nLet’s study the simulate method first: we’ll explore its semantics, and see the types of data it produces.\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── PytreeClosure\n│               ├── callable\n│               │   └── &lt;function h&gt;\n│               └── environment\n│                   └── list\n├── args\n│   └── tuple\n│       └── (const) 0.3\n├── retval\n│   └──  bool[]\n├── choices\n│   └── Trie\n│       └── :sub\n│           └── BuiltinTrace\n│               ├── gen_fn\n│               │   └── BuiltinGenerativeFunction\n│               │       └── source\n│               │           └── PytreeClosure\n│               │               ├── callable\n│               │               │   └── &lt;function g&gt;\n│               │               └── environment\n│               │                   └── list\n│               ├── args\n│               │   └── tuple\n│               │       └── (const) 0.3\n│               ├── retval\n│               │   └──  bool[]\n│               ├── choices\n│               │   └── Trie\n│               │       └── :m0\n│               │           └── DistributionTrace\n│               │               ├── gen_fn\n│               │               │   └── Bernoulli\n│               │               ├── args\n│               │               │   └── tuple\n│               │               │       └── (const) 0.3\n│               │               ├── value\n│               │               │   └──  bool[]\n│               │               └── score\n│               │                   └──  f32[]\n│               ├── cache\n│               │   └── Trie\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nIf you’re familiar with other “trace-based” probabilistic systems - this should look familiar.\nThis object instance is a piece of data which has captured information about the execution of the function. Specifically, the subtraces of other generative function calls which occur in genjax.trace statements.\nIt also captures the score - the log probability of the normalized measure which the model program represents, evaluated at the random choices which the generative call execution produced.\nIf you were paying attention above, the score is \\(\\log P(\\tau, r; x)\\).\n\nHow is simulate implemented for this language?\nFor this generative function language, we implement simulate using a code transformation! Here’s the transformed code.\n\njaxpr = jax.make_jaxpr(genjax.simulate(h))(key, (0.3,))\njaxpr\n\n{ lambda ; a:u32[2] b:f32[]. let\n    c:key&lt;fry&gt;[] = random_wrap[impl=fry] a\n    d:key&lt;fry&gt;[2] = random_split[count=2] c\n    e:u32[2,2] = random_unwrap d\n    f:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] e\n    g:u32[2] = squeeze[dimensions=(0,)] f\n    h:u32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=(1, 1)] e\n    i:u32[2] = squeeze[dimensions=(0,)] h\n    j:key&lt;fry&gt;[] = random_wrap[impl=fry] i\n    k:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:bool[] = pjit[\n      jaxpr={ lambda ; m:key&lt;fry&gt;[] n:f32[]. let\n          o:f32[] = pjit[\n            jaxpr={ lambda ; p:key&lt;fry&gt;[] q:f32[] r:f32[]. let\n                s:f32[] = convert_element_type[\n                  new_dtype=float32\n                  weak_type=False\n                ] q\n                t:f32[] = convert_element_type[\n                  new_dtype=float32\n                  weak_type=False\n                ] r\n                u:u32[] = random_bits[bit_width=32 shape=()] p\n                v:u32[] = shift_right_logical u 9\n                w:u32[] = or v 1065353216\n                x:f32[] = bitcast_convert_type[new_dtype=float32] w\n                y:f32[] = sub x 1.0\n                z:f32[] = sub t s\n                ba:f32[] = mul y z\n                bb:f32[] = add ba s\n                bc:f32[] = reshape[dimensions=None new_sizes=()] bb\n                bd:f32[] = max s bc\n              in (bd,) }\n            name=_uniform\n          ] m 0.0 1.0\n          be:bool[] = lt o n\n        in (be,) }\n      name=_bernoulli\n    ] j k\n    bf:f32[] = convert_element_type[new_dtype=float32 weak_type=True] l\n    bg:f32[] = sub bf 0.0\n    bh:bool[] = ne bg 0.0\n    bi:f32[] = pjit[\n      jaxpr={ lambda ; bj:bool[] bk:f32[] bl:f32[]. let\n          bm:f32[] = select_n bj bl bk\n        in (bm,) }\n      name=_where\n    ] bh bg 1.0\n    bn:f32[] = pjit[\n      jaxpr={ lambda ; bj:bool[] bk:f32[] bl:f32[]. let\n          bm:f32[] = select_n bj bl bk\n        in (bm,) }\n      name=_where\n    ] bh b 1.0\n    bo:f32[] = log bn\n    bp:f32[] = mul bi bo\n    bq:f32[] = pjit[\n      jaxpr={ lambda ; bj:bool[] bk:f32[] bl:f32[]. let\n          bm:f32[] = select_n bj bl bk\n        in (bm,) }\n      name=_where\n    ] bh bp 0.0\n    br:f32[] = sub 1.0 bg\n    bs:f32[] = neg b\n    bt:bool[] = ne br 0.0\n    bu:f32[] = pjit[\n      jaxpr={ lambda ; bj:bool[] bk:f32[] bl:f32[]. let\n          bm:f32[] = select_n bj bl bk\n        in (bm,) }\n      name=_where\n    ] bt br 1.0\n    bv:f32[] = pjit[\n      jaxpr={ lambda ; bj:bool[] bk:f32[] bl:f32[]. let\n          bm:f32[] = select_n bj bl bk\n        in (bm,) }\n      name=_where\n    ] bt bs 1.0\n    bw:f32[] = log1p bv\n    bx:f32[] = mul bu bw\n    by:f32[] = pjit[\n      jaxpr={ lambda ; bj:bool[] bk:f32[] bl:f32[]. let\n          bm:f32[] = select_n bj bl bk\n        in (bm,) }\n      name=_where\n    ] bt bx 0.0\n    bz:f32[] = add bq by\n    ca:bool[] = lt bg 0.0\n    cb:bool[] = gt bg 1.0\n    cc:bool[] = convert_element_type[new_dtype=bool weak_type=False] ca\n    cd:bool[] = convert_element_type[new_dtype=bool weak_type=False] cb\n    ce:bool[] = or cc cd\n    cf:f32[] = pjit[\n      jaxpr={ lambda ; cg:bool[] ch:f32[] ci:f32[]. let\n          cj:f32[] = select_n cg ci ch\n        in (cj,) }\n      name=_where\n    ] ce -inf bz\n    ck:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cf\n    cl:f32[] = reduce_sum[axes=()] ck\n    cm:f32[] = add 0.0 cl\n    cn:f32[] = add 0.0 cm\n  in (g, b, l, b, l, b, l, cl, cm, cn) }\n\n\n\nThat’s a lot of code! This code is pure, numerical, and ready for acceleration. By utilizing JAX and a staging transformation, we’ve stripped out all Python overhead.\nThis is how we’ve implemented simulate for this particular generative function language.66 In general, Gen doesn’t require that we follow the same “code transformation” implementation for other generative function languages. GenJAX, however, is a bit special - because we restrict the user to remain within the JAX traceable subset of Python - any generative function interface implementation must also be JAX traceable. This is a JAX requirement, not a Gen one."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#generative-function-interface",
    "href": "concepts/introduction/intro_to_genjax.html#generative-function-interface",
    "title": "Introduction to Gen and GenJAX",
    "section": "Generative function interface",
    "text": "Generative function interface\nThere are a few more generative function interface methods worth discussing.\nIn this notebook, instead of carefully walking through the math which these interface methods compute, we’ll defer that discussion to another notebook. Below, we give an informal discussion of what each of the interface methods computes, and roughly describe what algorithm families are supported by their usage.\n\nThe generative function interface in GenJAX\nGenJAX’s generative functions define an interface which support compositional usage of generative functions within other generative functions. The interface functions here closely mirror the interfaces defined in Gen.jl.\nIn the following, we use the following abbreviations:\n\nIS - importance sampling\nSMC - sequential Monte Carlo\nMCMC - Markov chain Monte Carlo\nVI - variational inference\n\n\n\n\n\n\n\n\n\nInterface\nType\nInference algorithm support\n\n\n\n\nsimulate\nGenerative\nIS, SMC\n\n\nimportance\nGenerative\nIS, SMC, VI\n\n\nupdate\nGenerative and incremental\nMCMC, SMC\n\n\nassess\nGenerative and differentiable\nMCMC, IS, SMC\n\n\nunzip\nDifferentiable\nDifferentiable and involutive MCMC and SMC, VI\n\n\n\nThis interface supports several methods - I’ve roughly described them and split them into the two categories Generative and Differentiable below:\n\nGenerative\n\nsimulate - sample from normalized trace measure, and return the score.\nimportance - given constraints for some addresses, sample from unnormalized trace measure and return an importance weight.\nupdate - given an existing trace, and a set of constraints and argument change values, update the trace to be consistent with the set of constraints under execution with the new arguments, and return an incremental importance weight.\nassess - given a complete choice map and arguments, return the normalized log probability.\n\n\n\nDifferentiable\n\nassess - same as above.\nunzip - given a set of fixed constraints, return two callables. The first callable score accepts constraints which fill in the complement of the fixed constraints and arguments, and returns the normalized log probability of all the constraints. The second callable retval accepts constraints and arguments, and returns the return value for the generative function call consistent with the constraints and given arguments.\n\nunzip produces two functions which can be compositionally used with jax.grad to evaluate gradients used by both differentiable and involutive MCMC and SMC."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#more-about-generative-functions",
    "href": "concepts/introduction/intro_to_genjax.html#more-about-generative-functions",
    "title": "Introduction to Gen and GenJAX",
    "section": "More about generative functions",
    "text": "More about generative functions\nHere are a few more bits of information which should help you gain context with these objects.\n\nDistributions are generative functions\nIn GenJAX, distributions are generative functions.\n\nkey, tr = genjax.simulate(genjax.normal)(key, (0.0, 1.0))\ntr\n\n\n\n\nDistributionTrace\n├── gen_fn\n│   └── Normal\n├── args\n│   └── tuple\n│       ├── (const) 0.0\n│       └── (const) 1.0\n├── value\n│   └──  f32[]\n└── score\n    └──  f32[]\n\n\n\nThis should bring a sigh of relief! Ah, distributions are generative functions - the concepts can’t be too exotic.\nDistributions implement the interface in a conceptually simple way. They don’t have internal compositional choice structure (like the function-like BuiltinGenerativeFunction language above).\nDistributions themselves expose two interfaces:\n\nlogpdf - exact density evaluation.\nsample - exact sampling.\n\nWe can use these two interfaces to implement all the generative function interfaces for distributions.\n\n\nAssociated data types\n\nChoice maps are the tree-like recordings of random choices in a trace.\nSelection is an object which allows querying a trace/choice map - selecting certain choices.\n\n\n@genjax.gen\ndef h(x):\n    m1 = genjax.bernoulli(x) @ \"m0\"\n    m2 = genjax.bernoulli(x) @ \"m1\"\n    return m1 + m2\n\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── PytreeClosure\n│               ├── callable\n│               │   └── &lt;function h&gt;\n│               └── environment\n│                   └── list\n├── args\n│   └── tuple\n│       └── (const) 0.3\n├── retval\n│   └──  bool[]\n├── choices\n│   └── Trie\n│       ├── :m0\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── Bernoulli\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       └── (const) 0.3\n│       │       ├── value\n│       │       │   └──  bool[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :m1\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── Bernoulli\n│               ├── args\n│               │   └── tuple\n│               │       └── (const) 0.3\n│               ├── value\n│               │   └──  bool[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\n\nselection = genjax.select(\"m1\")\nselected = selection.filter(tr.get_choices())\nselected\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        └── :m1\n            └── ValueChoiceMap\n                └── value\n                    └──  bool[]"
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#what-can-i-do-with-them",
    "href": "concepts/introduction/intro_to_genjax.html#what-can-i-do-with-them",
    "title": "Introduction to Gen and GenJAX",
    "section": "What can I do with them?",
    "text": "What can I do with them?\nNow, we’ve informally seen the interfaces and datatypes associated with generative functions.\nStudying the interfaces (and improvements thereof), as well as the computational objects which satisfy them can be an entire PhD’s worth of effort.\nIn the remainder of this notebook, let’s see how we can do machine learning with them.\nLet’s consider a modeling problem where we wish to perform generalized regression with outliers between two variates, taking a family of polynomials as potential curves.\nOne such model for this data generating process is shown below.\n\n# Two branches for a branching submodel.\n@genjax.gen\ndef model_y(x, coefficients):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    y = genjax.normal(polynomial_value, 0.3) @ \"value\"\n    return y\n\n\n@genjax.gen\ndef outlier_model(x, coefficients):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    y = genjax.normal(polynomial_value, 30.0) @ \"value\"\n    return y\n\n\n# The branching submodel.\nswitch = genjax.Switch(model_y, outlier_model)\n\n# A mapped kernel function which calls the branching submodel.\n@genjax.gen(genjax.Map, in_axes=(0, None))\ndef kernel(x, coefficients):\n    is_outlier = genjax.bernoulli(0.1) @ \"outlier\"\n    is_outlier = jnp.asarray(is_outlier, dtype=int)\n    y = switch(is_outlier, x, coefficients) @ \"y\"\n    return y\n\n\n@genjax.gen\ndef model(xs):\n    coefficients = genjax.mv_normal(np.zeros(3), 2.0 * np.identity(3)) @ \"alpha\"\n    ys = kernel(xs, coefficients) @ \"ys\"\n    return ys\n\nThere’s a few implementation patterns which you might pick up on by studying this model.\n\nTo implement control flow, we use higher-order functions called combinators. These accept generative functions as input, and return generative functions as output.\nAny JAX compatible code is allowed in the body of a generative function.\n\nCourtesy of the interface, we get to design our model generative function in pieces.\nNow, let’s examine the sampled observation address (\"ys\", \"y\") from a sample trace from our model.\n\ndata = jnp.arange(0, 10, 0.5)\nkey, tr = jax.jit(model.simulate)(key, (data,))\ntr[\"ys\", \"y\"]\n\n\n\n\nTaggedChoiceMap\n├── index\n│   └──  i32[20]\n└── submaps\n    └── list\n        ├── BuiltinTrace\n        │   ├── gen_fn\n        │   │   └── BuiltinGenerativeFunction\n        │   │       └── source\n        │   │           └── PytreeClosure\n        │   │               ├── callable\n        │   │               │   └── &lt;function model_y&gt;\n        │   │               └── environment\n        │   │                   └── list\n        │   ├── args\n        │   │   └── tuple\n        │   │       ├──  f32[20]\n        │   │       └──  f32[20,3]\n        │   ├── retval\n        │   │   └──  f32[20]\n        │   ├── choices\n        │   │   └── Trie\n        │   │       └── :value\n        │   │           └── DistributionTrace\n        │   │               ├── gen_fn\n        │   │               │   └── Normal\n        │   │               ├── args\n        │   │               │   └── tuple\n        │   │               │       ├──  f32[20]\n        │   │               │       └──  f32[20]\n        │   │               ├── value\n        │   │               │   └──  f32[20]\n        │   │               └── score\n        │   │                   └──  f32[20]\n        │   ├── cache\n        │   │   └── Trie\n        │   └── score\n        │       └──  f32[20]\n        └── BuiltinTrace\n            ├── gen_fn\n            │   └── BuiltinGenerativeFunction\n            │       └── source\n            │           └── PytreeClosure\n            │               ├── callable\n            │               │   └── &lt;function outlier_model&gt;\n            │               └── environment\n            │                   └── list\n            ├── args\n            │   └── tuple\n            │       ├──  f32[20]\n            │       └──  f32[20,3]\n            ├── retval\n            │   └──  f32[20]\n            ├── choices\n            │   └── Trie\n            │       └── :value\n            │           └── DistributionTrace\n            │               ├── gen_fn\n            │               │   └── Normal\n            │               ├── args\n            │               │   └── tuple\n            │               │       ├──  f32[20]\n            │               │       └──  f32[20]\n            │               ├── value\n            │               │   └──  f32[20]\n            │               └── score\n            │                   └──  f32[20]\n            ├── cache\n            │   └── Trie\n            └── score\n                └──  f32[20]\n\n\n\nHere, I’m just showing the subtrace from the switching model invocation - but it is already quite large and unwieldy!\nBesides, it also doesn’t tell us much about the values we truly care about here - the sampled values.\nFrom this model, we can get these in two ways.\nThe first way: we can just look at the trace return value.\n\ntr.get_retval()\n\nArray([   1.7448314 ,    0.41097283,   -1.9558886 ,   -5.498426  ,\n         -9.625496  ,  -15.6570425 ,  -22.74233   ,  -30.96311   ,\n        -30.182352  ,  -64.04883   ,  -62.844086  ,  -75.89779   ,\n        -90.39219   , -105.64662   , -121.877945  , -140.97981   ,\n       -159.82722   , -180.46971   , -207.86064   , -224.98926   ],      dtype=float32)\n\n\n\nThe second way: we can get them out of the choice map of the trace directly.\n\nchm = tr.get_choices()\nchm[\"ys\", \"y\", \"value\"]\n\nArray([   1.7448314 ,    0.41097283,   -1.9558886 ,   -5.498426  ,\n         -9.625496  ,  -15.6570425 ,  -22.74233   ,  -30.96311   ,\n        -30.182352  ,  -64.04883   ,  -62.844086  ,  -75.89779   ,\n        -90.39219   , -105.64662   , -121.877945  , -140.97981   ,\n       -159.82722   , -180.46971   , -207.86064   , -224.98926   ],      dtype=float32)\n\n\n\nNow, let’s construct a small visualization function to show us the samples.\n\ndef viz(ax, x, y, **kwargs):\n    (data,) = tr.get_args()\n    chm = tr.get_choices()\n    ys = np.array(chm[\"ys\", \"y\", \"value\"])\n    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(8, 8), sharex=True, sharey=True)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (data,))\n    x = data\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nThese are the (\"ys\", \"y\", \"value\") samples for 9 traces from our model, against the points from the data we passed.\nWe just walked through one of the main elements of probabilistic programming: setting up a program, which represents a joint distribution over random variates, some of which we’ll identify with data we expect to see in the world.\nWe can adjust the noise settings of our model to produce wider priors over possible sets of points - and we may want to do this if our data is noisy!\nFor now, let’s keep the settings as is, and explore inference in GenJAX."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#your-first-inference-program",
    "href": "concepts/introduction/intro_to_genjax.html#your-first-inference-program",
    "title": "Introduction to Gen and GenJAX",
    "section": "Your first inference program",
    "text": "Your first inference program\nNow, let’s say we have some data.\n\nx = np.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\ny = 2.0 * x + 1.5 + x**2\ny[2] = 50.0\n\n\nfig_data, ax_data = plt.subplots(figsize=(6, 6))\nviz(ax_data, x, y, color=\"blue\")\n\n\n\n\nIn Bayesian inference, if we wish to consider the conditional distribution \\(P(\\tau, r; x | \\text{data})\\) induced from a model \\(P(\\tau, \\text{data}, r; x)\\) - Bayes’ rule gives us a way to compute it.\n\\[\nP(\\tau, r; x | \\text{data}) = \\frac{P(\\tau, \\text{data}, r; x)}{\\int P(\\tau, \\text{data}, r; x) \\ d\\tau}\n\\]\nThe problem is that we often cannot compute the denominator (the evidence integral) easily. Instead, we turn to approximate Bayesian inference.\nDepending on how we wish to use the LHS conditional (which is called the posterior in Bayesian inference) - we have different options available to us.\nIf we wish to approximately sample from the posterior, to get an empirical sense of its shape and properties, we will often utilize techniques which provide exact samplers for another distribution which gets asymptotically close to the target posterior if we increase certain hyperparameters.\nOne such algorithm is importance sampling, and that’s what we’ll write today.\nHere’s importance sampling (with a single sample) without a custom proposal in GenJAX.\n\nobservations = genjax.choice_map(\n    {\n        \"ys\": genjax.vector_choice_map(\n            jnp.arange(0, len(y)), genjax.choice_map({(\"y\", \"value\"): y})\n        )\n    }\n)\nmodel_args = (x,)\nkey, (w, tr) = model.importance(key, observations, model_args)\n\nWe’re introduced to another interface method!\nimportance accepts a PRNG key, a choice map representing observations (sometimes called constraints), and model arguments. It returns a new evolved PRNG key, and a tuple contained a log importance weight and a trace.\nThe trace is consistent with the arguments and constraints passed into the invocation.\n\nobservations[\"ys\", \"y\", \"value\"]\n\narray([ 2.19,  3.39, 50.  ,  6.26, 11.39, 12.75, 16.5 , 25.5 , 36.5 ])\n\n\n\n\ntr[\"ys\", \"y\", \"value\"]\n\nArray([ 2.19,  3.39, 50.  ,  6.26, 11.39, 12.75, 16.5 , 25.5 , 36.5 ],      dtype=float32)\n\n\n\nLet’s examine the weight now, and compare it to the score.\n\n(w, tr.get_score())\n\n(\n    Array(-10115.872, dtype=float32),\n    Array([  -28.476847,   -55.859825, -8679.652   ,  -117.18727 ,\n        -201.13243 ,  -218.27744 ,  -256.18158 ,  -302.58957 ,\n        -300.92776 ], dtype=float32)\n)\n\n\n\nNotice that these two quantities are different.\nRemember: the score is the normalized log density of the choice map measure evaluated at the complete set of trace constraints. We’ll refer to complete traces by \\(\\tau\\).\nThe log importance weight w is slightly different.\n\nImportance sampling, informally\nLet’s discuss how importance sampling works first7.7 In this notebook, I’ll defer discussing formal proofs concerning the asymptotic consistence of posterior estimators derived from importance sampling.\nThis will provide us with an understanding as to why w is different from the score.\nMore importantly, we’ll understand how we can use importance to solve the inference task of approximately sampling from the posterior over coefficients (and ultimately, over curves) from our generative function.\n\n\n\n\n\n\nImportance sampling is typically presented by focusing on posterior expectations \\(E_{x \\sim P(x | y)}[f(x)]\\).\nIn our case, we want to sample \\(x \\sim P(x | y)\\). To do this, we’ll actually be considering a different procedure called sampling importance resampling or SIR for short.\nImportantly, importance sampling is a subroutine in SIR.\nWe’ll discuss why importance sampling works here, and provide references to why SIR works to solve our problem.\n\n\n\nLet’s start by considering two distributions which we can sample from, and evaluate densities.\nBelow, I’m plotting the densities of two distributions - a 1D Gaussian mixture and a 1D Gaussian8.8 GenJAX allows usage of TensorFlow Distributions as generative functions. Here, we’re just using the logpdf interface from distributions which expose exact logpdf evaluation - but genjax exports a wrapper which implements the complete generative function interface.\n\nmix = genjax.tfp_mixture(genjax.tfp_categorical, [genjax.tfp_normal, genjax.tfp_normal])\nmix_args = ([0.5, 0.5], [(-3.0, 0.8), (1.0, 0.3)])\nd = genjax.tfp_normal\nd_args = (0.0, 1.0)\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nevaluation_points = np.arange(-5, 5, 0.01)\n\n\ndef plot_logpdf(ax, logpdf_fn, evaluation_points, **kwargs):\n    logpdfs = jax.vmap(logpdf_fn)(evaluation_points)\n    ax.scatter(evaluation_points, jnp.exp(logpdfs), marker=\".\", **kwargs)\n\n\nd_logpdf = lambda v: d.logpdf(v, *d_args)\nmix_logpdf = lambda v: mix.logpdf(v, *mix_args)\n\nplot_logpdf(ax, d_logpdf, evaluation_points, color=\"red\", label=\"1D Gaussian PDF\")\nplot_logpdf(\n    ax,\n    mix_logpdf,\n    evaluation_points,\n    color=\"blue\",\n    label=\"1D Gaussian mixture PDF\",\n)\nax.legend()\n\n\n\n\nTo gain context on importance sampling, imagine that the distribution which produces the blue curve is difficult to sample from - but it exposes a logpdf interface which we can use to evaluate the density at any point on the support of the distribution.\nNow, suppose you hand me the distribution which made the red curve - and it is easy to sample from, and it also exposes a logpdf interface.\nOne thing we could do is sample from the red curve and then “correct” for the fact that we’re sampling from the wrong distribution.\nThis is the key intuition behind importance sampling.\nNow, I’m going to write a procedure and ask you to just go with it … for a moment.\n\ndef importance_sample(hard, easy):\n    def _inner(key, hard_args, easy_args):\n        key, sub_key = jax.random.split(key)\n        sample = easy.sample(sub_key, *easy_args)\n        easy_logpdf = easy.logpdf(sample, *easy_args)\n        hard_logpdf = hard.logpdf(sample, *hard_args)\n        importance_weight = hard_logpdf - easy_logpdf\n        return key, (importance_weight, sample)\n\n    return _inner\n\n\nhard = genjax.tfp_mixture(\n    genjax.tfp_categorical, [genjax.tfp_normal, genjax.tfp_normal]\n)\neasy = genjax.tfp_normal\njitted = jax.jit(importance_sample(hard, easy))\nkey, (importance_weight, sample) = jitted(key, mix_args, d_args)\n\n\n(importance_weight, sample)\n\n(Array(-4.0058784, dtype=float32), Array(-0.78240925, dtype=float32))\n\n\n\nNow, we can easily run this procedure many times in parallel.\n\njitted = jax.jit(jax.vmap(importance_sample(hard, easy), in_axes=(0, None, None)))\n\n\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n_, (importance_weight, sample) = jitted(sub_keys, mix_args, d_args)\n\n\nimportance_weight\n\nArray([-6.2341752e+00,  6.2633801e-01, -3.3759670e+00, -5.9383879e+00,\n       -9.7561479e-01, -5.3102112e+00, -5.1128283e+00, -2.1514070e+00,\n        4.5841384e-01, -4.0724092e+00, -1.8354319e+00, -5.5122299e+00,\n       -6.2061067e+00, -2.1719730e-01, -3.5693016e+00, -6.1405115e+00,\n       -5.3810544e+00, -6.2284150e+00,  7.8095734e-01, -3.8515360e+00,\n       -2.8184617e-01, -1.8381605e+00, -7.5211000e+00,  1.0577252e+00,\n        1.0453278e+00,  1.0218762e+00,  9.8658347e-01,  9.5912194e-01,\n       -1.9758319e+00,  1.0580635e+00, -4.1381865e+00, -2.3692460e+00,\n        1.0351460e+00, -2.5306203e+00,  6.6269362e-01,  6.0733199e-02,\n       -1.1496689e+00, -6.2260799e+00, -6.1138239e+00,  4.6291578e-01,\n       -4.1340222e+00,  8.6949825e-01, -2.1810038e+00, -2.9703891e+00,\n       -7.1519613e-03, -1.2844566e+00, -2.3207369e+00, -2.3978152e+00,\n       -1.0548264e+00, -5.6061153e+00, -5.9159632e+00, -3.1040215e-01,\n       -1.5413404e-01, -1.2319851e-01, -1.4652884e+00, -5.0214028e-01,\n        6.9174856e-01, -5.8656144e+00, -5.3170371e+00,  6.0201246e-01,\n       -1.6653061e-02, -3.0496669e+00, -2.3092539e+00, -2.4331427e+00,\n        1.0071430e+00, -5.6184292e+00, -1.1847696e+00, -3.5587354e+00,\n       -5.8828015e+00, -3.7577522e+00, -3.6165695e+00, -5.5585594e+00,\n       -1.7351907e+00, -6.7679203e-01,  1.9070494e-01, -3.9766405e+00,\n        5.7461774e-01, -1.0918140e-02, -2.6561151e+00,  4.9570715e-01,\n        6.3730204e-01,  9.7846591e-01, -3.9247353e+00,  1.0440333e+00,\n       -6.2158747e+00, -2.4731362e+00, -4.5478611e+00,  1.0888841e+00,\n       -7.3868978e-01, -4.2448354e+00,  9.5782554e-01,  3.7766671e-01,\n       -6.2152562e+00, -5.2021756e+00, -5.1431012e+00, -1.7538964e+00,\n       -4.6911263e+00, -7.5708902e-01, -4.1147037e+00,  9.9778765e-01],      dtype=float32)\n\n\n\nWe’re just sampling from easy, then scoring the samples with importance_weight according to the log ratio easy_logpdf(sample) - hard_logpdf(sample).\nHere’s the trick - from our collection of samples and weights, let’s normalize the weights into a distribution and sample a single sample to return using it.\n\ndef sampling_importance_resampling(hard, easy, n_samples):\n    def _inner(key, hard_args, easy_args):\n        fn = importance_sample(hard, easy)\n        key, *sub_keys = jax.random.split(key, n_samples + 1)\n        sub_keys = jnp.array(sub_keys)\n        vmapped = jax.vmap(fn, in_axes=(0, None, None))\n        _, (ws, samples) = vmapped(sub_keys, hard_args, easy_args)\n        logits = ws\n        key, sub_key = jax.random.split(key)\n        index = genjax.tfp_categorical.sample(sub_key, logits)\n        final_sample = samples[index]\n        return key, final_sample\n\n    return _inner\n\n\nhard = genjax.tfp_mixture(\n    genjax.tfp_categorical, [genjax.tfp_normal, genjax.tfp_normal]\n)\neasy = genjax.tfp_normal\njitted = jax.jit(sampling_importance_resampling(hard, easy, 100))\nkey, sample = jitted(key, mix_args, d_args)\n\n\nsample\n\nArray(-2.556716, dtype=float32)\n\n\n\nLet’s run this procedure a bunch of times and plot the points on the x-axis of our plot above.\n\ndef plot_on_x(ax, x, **kwargs):\n    ax.scatter(x, np.zeros_like(x), **kwargs)\n\n\nkey, *sub_keys = jax.random.split(key, 1000 + 1)\nsub_keys = jnp.array(sub_keys)\nfn = sampling_importance_resampling(hard, easy, 1000)\njitted = jax.jit(jax.vmap(fn, in_axes=(0, None, None)))\n_, samples = jitted(sub_keys, mix_args, d_args)\nplot_on_x(ax, samples, color=\"gold\", marker=\".\", alpha=0.05)\nfig\n\n\n\n\nNotice what happens with the SIR samples (in gold)?\nThey accumulate around the places you’d expect to see if you were sampling from the hard distribution!\nThat’s what importance sampling and sampling importance sampling give us - we provide a “hard” distribution with a logpdf interface, and another “easy” distribution with sample and logpdf interface9, and SIR returns an exact sampler for a distribution which approximates the hard distribution.9 There are more constraints. The second distribution must be absolutely continuous in measure with respect to the first. Let’s defer this discussion to a formal treatment of importance sampling.\n\n\nBack to our generative function\nNow that we’ve seen the ingredients and implementation of importance sampling and sampling importance resampling - let’s return to our original problem.\n\nfig_data\n\n\n\n\nIf you studied the previous section careful - one question might jump out at you: what is the “easy” distribution for model.importance?\n\nBuiltin proposals\nGenerative functions defined using the BuiltinGenerativeFunction language come with builtin proposals - it’s a distribution (which we’ll refer to as \\(Q\\)) induced from the prior, with sampling and score defined ancestrally.\nGive observation constraints \\(u\\), the importance weight which model.importance computes is10:10 This definition again considers “untraced randomness” \\(r\\). If you wish to ignore this in the math, just remove the \\(Q(r; x, \\tau)\\) term. Even in the presence of untraced randomness, the weights which Gen computes are asymptotically consistent in expectation over \\(Q(r; x, \\tau)\\)\n\\[\n\\begin{align}\n\\log w &= \\log P(\\tau, r; x) - \\log Q(\\tau; u, x)Q(r; x, \\tau) \\\\\n\\end{align}\n\\]\nFor the BuiltinGenerativeFunction language, we implement \\(Q\\) by invoking the generative function - when we arrive at a constrained address, we recursively called submodel.importance - accumulate the log weight, as well as the log score.\nNow, if an address has no constraints - we get 0.0 for the weight (think about why this is by looking at the above equation and asking what happens when \\(Q\\) has to generate a full \\(\\tau\\)). However, we still get a score.\n\n\nSequential importance resampling in GenJAX\nHere’s SIR using builtin proposals (just a single call to model.importance) in GenJAX11:11 To implement a variant with custom proposals, all we need to do is first proposal.simulate, merge the proposal choice map with the constraints, then model.importance followed by a final weight adjustment w = w - proposal_tr.get_score() - easy peasy.\n\ndef sampling_importance_resampling(model, n_samples):\n    def _inner(key, observations, model_args):\n        key, *sub_keys = jax.random.split(key, n_samples + 1)\n        sub_keys = jnp.array(sub_keys)\n        vmapped = jax.vmap(model.importance, in_axes=(0, None, None))\n        _, (lws, trs) = vmapped(sub_keys, observations, model_args)\n        key, sub_key = jax.random.split(key)\n        index = genjax.tfp_categorical.sample(sub_key, lws)\n        final_tr = jtu.tree_map(lambda v: v[index], trs)\n        return key, final_tr\n\n    return _inner\n\nOne difference between our first implementation (on just distributions) above and this one is that Trace instances are structured objects (but all of them are Pytree implementors) - meaning we need to index into the leaves when we wish to return a single sampled trace.\n\nmodel_args = (x,)\njitted = jax.jit(\n    jax.vmap(sampling_importance_resampling(model, 100), in_axes=(0, None, None))\n)\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n_, samples = jitted(sub_keys, observations, model_args)\ncoefficients = samples[\"alpha\"]\n\nSo now we have an approximate sampler for the posterior and we can use it to look at properties of the posterior - like what sort of curves are likely given our data and our model prior.\n… and by the way, to get a representative set of samples from the posterior for this model, on an Apple M2 device - only takes about 0.05 seconds12.12 Just remember: we’re running this notebook on CPU - but the resulting specialized inference code can easily be moved to accelerators, courtesy of the fact that all our code is JAX traceable.\n\n%%timeit\n_, samples = jitted(sub_keys, observations, model_args)\n\n44.5 ms ± 2.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndef polynomial_at_x(x, coefficients):\n    basis_values = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(coefficients * basis_values)\n    return polynomial_value\n\n\njitted = jax.jit(jax.vmap(polynomial_at_x, in_axes=(None, 0)))\n\n\ndef plot_polynomial_values(ax, x, coefficients, **kwargs):\n    v = jitted(x, coefficients)\n    ax.scatter(np.repeat(x, len(v)), v, **kwargs)\n\n\ncoefficients = samples[\"alpha\"]\nevaluation_points = np.arange(0, 5, 0.01)\nfor data in evaluation_points:\n    plot_polynomial_values(\n        ax_data, data, coefficients, marker=\".\", color=\"gold\", alpha=0.005\n    )\nfig_data\n\n\n\n\nIntuitively, this makes a lot of sense. Our prior over polynomials considers a wide range of curves - but, if our approximate sampling process is trusted, we’re correctly seeing what we should expect to happen if we observed this data - polynomials with the coefficients shown above tend to be sampled more under the posterior.\nWe can also ask for an estimate of the posterior probability that any particle point was an outlier.\nFor example, below is the set of samples projected onto the (\"ys\", \"outlier\") address for the point which we manually set to be quite far from the curve.\n\nposterior_outlier = samples[\"ys\", \"outlier\"][:, 2]\nposterior_outlier\n\nArray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True], dtype=bool)\n\n\n\n\nnp.sum(posterior_outlier) / len(posterior_outlier)\n\nArray(0.99, dtype=float32)\n\n\n\nThat seems to make sense! We pulled that point quite far away from ground truth curve - so we’d expect that point 2 is considered an outlier under the true posterior."
  },
  {
    "objectID": "concepts/introduction/intro_to_genjax.html#summary",
    "href": "concepts/introduction/intro_to_genjax.html#summary",
    "title": "Introduction to Gen and GenJAX",
    "section": "Summary",
    "text": "Summary\nWe’ve covered a lot of ground in this notebook. Please reflect, re-read, and post issues!\n\nWe discussed the Gen probabilistic programming framework, and discussed GenJAX - an implementation of Gen on top of JAX.\nWe discussed generative functions - the main computational object of Gen.\nWe discussed how to create generative functions using generative function languages, and several of GenJAX’s builtin capabilities for constructing generative functions.\nWe discussed how to use generative functions to represent joint probability distributions, which can be used to construct models of phenomena.\nWe created a generative function to model a data-generating process based on sampling and evaluating random polynomials at input data - to represent a typical regression task.\nWe discussed how to formulate questions about induced conditional distributions under a probabilistic model as a Bayesian inference problem.\nWe discussed importance sampling and sampling importance resampling, two central techniques in approximate Bayesian inference.\nWe created a sampling importance resampling routine and applied it to produce approximate posterior samples from the posterior in the our polynomial generating model.\nWe investigated the approximate posterior samples, and visually inspected that they match the inferences that we might draw - both for the polynomials we expected to produce the data, as well as what data points might be outliers.\n\nThis is just the beginning! There’s a lot more to learn, but plenty to bite off with this initial notebook."
  },
  {
    "objectID": "concepts/incremental_computation/incremental.html",
    "href": "concepts/incremental_computation/incremental.html",
    "title": "Incremental computation for generative functions",
    "section": "",
    "text": "%config InlineBackend.figure_format = 'svg'\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dataclasses\nimport genjax\nfrom genjax import Diff\nfrom typing import List, Tuple, Any\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nThe update interface method for generative functions defines an update operation on traces produced by generative functions.\nupdate allows the user to provide new constraints, as well as new arguments, and returns an updated trace which is consistent with the new constraints, as well as an incremental importance weight which measures the difference between the new and old constraints under the model. update is used to implement many types of iterative MCMC inference families.\nThe specification of update only requires that a modeling language support the above behavior - nonetheless, modeling languages can implement update with custom optimizations to improve the cost of repeatedly calling update (e.g. an iterative MCMC inference procedure).\nWhile we’ll be focused on the distribution and builtin languages, this system is also applicable to the combinator implementations of update. In another notebook, we’ll see how the incremental computing system can be used to efficiently compute update for UnfoldCombinator."
  },
  {
    "objectID": "concepts/incremental_computation/incremental.html#what-is-update-used-for",
    "href": "concepts/incremental_computation/incremental.html#what-is-update-used-for",
    "title": "Incremental computation for generative functions",
    "section": "What is update used for?",
    "text": "What is update used for?\nBefore we discuss how update can be optimized by a generative function implementor, it’s worth constructing a simple example which shows how update is used, and to show why optimizing update is worthwhile.\nOne common usage of update is in MCMC algorithm kernels. MCMC is often repeatedly applied to generate a chain of samples: any optimization opportunities that we identify and take advantage of will provide runtime gains which are multiplied over the length of the chain.\nLet’s example this scenario using a pedagogical example - remember that the potential optimization pattern (based upon random variable dependency information) we’ll describe extends to all generative functions.\n\nPedagogical example\nConsider the following generative function:\n\n@genjax.gen\ndef model(x):\n    a = genjax.trace(\"a\", genjax.Normal)(x, 1.0)\n    b = genjax.trace(\"b\", genjax.Normal)(x, 1.0)\n    c = genjax.trace(\"c\", genjax.Normal)(a + b, 1.0)\n    return c\n\nThe variable dependency graph is shown below.\n\n\n\n\nflowchart LR\n  x[Argument] --&gt; a[a]\n  x --&gt; b[b]\n  a --&gt; c[c]\n  b --&gt; c\n  c --&gt; r[Return]\n\n\n\n\n\nNow, when we simulate a trace from this model - we get choices for \"a\", \"b\", and \"c\".\n\nkey, tr = model.simulate(key, (2.0,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function model&gt;\n├── args\n│   └── tuple\n│       └── (const) 2.0\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :a\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├── (const) 2.0\n│       │       │       └── (const) 1.0\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       ├── :b\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├── (const) 2.0\n│       │       │       └── (const) 1.0\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :c\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └── (const) 1.0\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nIterative inference techniques like Metropolis-Hastings (and other MCMC methods) start with an initial trace, propose an update to the trace using a proposal, and then compute a criterion for accepting or rejecting the update.\nIn Metropolis-Hastings, the criterion involves an accept-reject ratio computation - which requires computing the probability of transitioning from the current trace to the new trace, as well as the probability of transitioning from the new trace back to the current trace, under a kernel defined by the algorithm.\nThe library implementation of Metropolis-Hastings is shown below - MetropolisHastings.apply shows the main content of the algorithm (it’s safe to ignore other methods for now).\n\n@dataclasses.dataclass\nclass MetropolisHastings(genjax.MCMCKernel):\n    selection: genjax.Selection\n    proposal: genjax.GenerativeFunction\n\n    def flatten(self):\n        return (), (self.selection, self.proposal)\n\n    def apply(self, key, trace: genjax.Trace, proposal_args: Tuple):\n        model = trace.get_gen_fn()\n        model_args = trace.get_args()\n        proposal_args_fwd = (trace.get_choices(), *proposal_args)\n        key, proposal_tr = self.proposal.simulate(key, proposal_args_fwd)\n        fwd_weight = proposal_tr.get_score()\n        diffs = jtu.tree_map(Diff.no_change, model_args)\n        key, (_, weight, new, discard) = model.update(\n            key, trace, proposal_tr.get_choices(), diffs\n        )\n        proposal_args_bwd = (new, *proposal_args)\n        key, (bwd_weight, _) = self.proposal.importance(key, discard, proposal_args_bwd)\n        alpha = weight - fwd_weight + bwd_weight\n        key, sub_key = jax.random.split(key)\n        check = jnp.log(random.uniform(sub_key)) &lt; alpha\n        return (\n            key,\n            jax.lax.cond(\n                check,\n                lambda *args: (new, True),\n                lambda *args: (trace, False),\n            ),\n        )\n\n    def reversal(self):\n        return self\n\nThis computation involves update - which incrementally updates a trace to be consistent with new arguments and constraints, and computes an importance weight (the difference between the trace’s new score and the old score).\n\n\n\n\n\n\nIn the invocation of update, there’s an interesting not-yet-explained argument: diffs - a tuple of Diff values, which represent changes to the original arguments of the call which produced the trace which we are attempting to update. We’ll come back to these values in a moment.\n\n\n\nIf we naively evaluate the required log probability by re-evaluating the entire model - we’re performing extra computation. We can see this by considering a specific target address - let’s consider \"a\". If the update changes \"a\", what other generative function calls do we need to visit to compute the correct update - both to the trace, and the importance weight?\nThe graph below shows the answer.\n\n\n\n\nflowchart LR\n  x[Argument] --&gt; a[a]\n  x --&gt; b[b]\n  a --&gt; c[c]\n  b --&gt; c\n  c --&gt; r[Return]\n  style a fill:#f9f,stroke:#333,stroke-width:4px\n  style c fill:#f9f,stroke:#333,stroke-width:4px\n\n\n\n\n\nAn update to \"a\" requires that we re-evaluate the log probability at \"c\" because the return value of the generative function call at \"a\" flows into the generative function call at \"c\" - but we do not need to re-visit \"b\" because none of the values which flow into \"b\" have changed.\nWhen computing the weight difference, unchanged sites thus contribute nothing.22 The important idea is that tracking what values have changed allows us to identify what parts of the computation graph are required - and what parts do not need to be re-visited or re-computed."
  },
  {
    "objectID": "concepts/incremental_computation/incremental.html#change-information",
    "href": "concepts/incremental_computation/incremental.html#change-information",
    "title": "Incremental computation for generative functions",
    "section": "Change information",
    "text": "Change information\nThe specification of update doesn’t require that an implementation track or use the change information - but generative function implementations can choose to optimize their update implementation.\nWith that in mind, several of the languages which GenJAX exposes can be instructed to perform optimized update computations using Diff values.\nA Diff value consists of a base value v and a value of Change type, which represents the change to the base value. The new argument value for update is given by \\(\\text{v} \\oplus dv\\) where dv :: Change.\nThe \\(\\oplus\\) operation must be appropriately defined for the change type lattice - we implement this operation for common change types in GenJAX, but users can define their own change types for Pytree data classes.\n\ngenjax.Diff.new(5.0, genjax.NoChange)\n\n\n\n\nDiff\n├── val\n│   └── (const) 5.0\n└── change\n    └── (const) &lt;genjax._src.core.diff_rules._NoChange object at 0x1468d9bd0&gt;\n\n\n\n\nDiffs for distributions\nLet’s explore the basics with distributions.\n\nkey, dist_tr = genjax.Normal.simulate(key, (0.0, 1.0))\ndist_tr\n\n\n\n\nDistributionTrace\n├── gen_fn\n│   └── _Normal\n├── args\n│   └── tuple\n│       ├── (const) 0.0\n│       └── (const) 1.0\n├── value\n│   └──  f32[]\n└── score\n    └──  f32[]\n\n\n\n\n# dist_tr.update is equivalent to model.update(key, tr, ...)\nkey, (ret_diff, w, tr, d) = dist_tr.update(\n    key,\n    genjax.EmptyChoiceMap(),\n    (\n        genjax.Diff.new(1.0, genjax.UnknownChange),\n        genjax.Diff.new(1.0, genjax.NoChange),\n    ),\n)\n\nThe return values do not change.\n\n(dist_tr.get_retval(), ret_diff.val)\n\n(Array(-0.72876865, dtype=float32), Array(-0.72876865, dtype=float32))\n\n\n\nThe weight is non-zero because the arguments have changed, implying that we must re-evaluate the log probability.\n\nw\n\nArray(-1.2287686, dtype=float32)\n\n\n\nWhat does the code look like when there is no new constraint and both the arguments do not change?\n\n# dist_tr.update is equivalent to model.update(key, tr, ...)\njaxpr = jax.make_jaxpr(dist_tr.update)(\n    key,\n    genjax.EmptyChoiceMap(),\n    (\n        genjax.Diff.new(0.0, genjax.NoChange),\n        genjax.Diff.new(1.0, genjax.NoChange),\n    ),\n)\njaxpr\n\n{ lambda a:f32[] b:f32[]; c:u32[2] d:f32[] e:f32[]. let\n    \n  in (c, a, 0.0, 0.0, 1.0, a, b, False, a) }\n\n\n\nAs expected, no computation is required - so the flattened arguments are just forwarded to the return."
  },
  {
    "objectID": "concepts/incremental_computation/incremental.html#cache-change-aware-memoization",
    "href": "concepts/incremental_computation/incremental.html#cache-change-aware-memoization",
    "title": "Incremental computation for generative functions",
    "section": "cache: change aware memoization",
    "text": "cache: change aware memoization\nThe BuiltinGenerativeFunction language exposes a primitive called cache that interacts with the change tracking system to support memoization of deterministic computations (even deterministic computations which depend on random choices)."
  },
  {
    "objectID": "concepts/gradient_interfaces/gradient_interfaces.html",
    "href": "concepts/gradient_interfaces/gradient_interfaces.html",
    "title": "Differentiable programming with GenJAX",
    "section": "",
    "text": "%config InlineBackend.figure_format = 'svg'\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport dataclasses\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import trace, slash, TFPUniform, Normal\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nThe generative function interface exposes functionality which allows usage of generative functions for differentiable programming. These interfaces are designed to work seamlessly with jax.grad - allowing (even higher-order) gradient computations which are useful for inference algorithms which require gradients and gradient estimators. In this notebook, we’ll describe some of these interfaces - as well as their (current, but not forever) limitations. We’ll walk through an implementation of MAP estimation, as well as the Metropolis-adjusted Langevin algorithm (MALA) using these interfaces."
  },
  {
    "objectID": "concepts/gradient_interfaces/gradient_interfaces.html#gradient-interfaces",
    "href": "concepts/gradient_interfaces/gradient_interfaces.html#gradient-interfaces",
    "title": "Differentiable programming with GenJAX",
    "section": "Gradient interfaces",
    "text": "Gradient interfaces\nBecause JAX features best-in-class support for higher-order AD, GenJAX exposes interfaces that compose natively with JAX’s interfaces for gradients. The primary interface method which provides jax.grad-compatible functions from generative functions is an interface called unzip.\nunzip allows a user to provide a key, and a fixed choice map - and it returns a new key and two closures:\n\nThe first closure is a “score” closure which accepts a choice map as the first argument, and arguments which match the non-PRNGKey signature types of the generative function. The score closure returns the exact joint score of the generative function. It computes the exact joint score using an interface called assess.1\nThe second closure is a “retval” closure which accepts a choice map as the first argument, and arguments which match the non-PRNGKey signature types of the generative function. The retval closure executes the generative function constrained using the union of the fixed choice map, and the user provided choice map, and returns the return value of the execution. Here, the return value is also provided by invoking the assess interface.\n\n1 Caveat: assess is not required to return the exact joint score, only an estimate. However, if jax.grad is used on estimates - the resulting thing is not a correct gradient estimator. See the important callout below!So really, unzip is syntactic sugar over another interface called assess.\n\nassess for exact density evaluation\nassess is a generative function interface method which computes log joint density estimates from generative functions. assess requires that a user provide a choice map which completely fills all choices encountered during execution. Otherwise, it errors.22 And these errors are thrown at JAX trace time, so you’ll get an exception before runtime.\nIf a generative function also draws from untraced randomness - assess computes an estimate whose expectation over the distribution of untraced randomness gives the correct log joint density.\n\n\n\n\n\n\nCorrectness of gradient estimators\n\n\n\nWhen used on generative functions which include untraced randomness, naively applying jax.grad to the closures returned by interfaces described in this notebook do not compute gradient estimators which are unbiased with respect to the true gradients.\nShort: don’t use these with untraced randomness. We’re working on alternatives."
  },
  {
    "objectID": "concepts/gradient_interfaces/gradient_interfaces.html#a-running-example",
    "href": "concepts/gradient_interfaces/gradient_interfaces.html#a-running-example",
    "title": "Differentiable programming with GenJAX",
    "section": "A running example",
    "text": "A running example\nLet’s consider the following model, which we’ll cover in different variations.\n\n# If you don't specify broadcast `in_axes`, you\n# should specify number of IID samples via `repeats`.\n@genjax.gen(genjax.Map, repeats=100)\ndef sample_x(x_mu, var):\n    position = trace(\"pos\", Normal)(x_mu, 1.0)\n    return position\n\n\n@genjax.gen(genjax.Map, in_axes=(0, None, None))\ndef sample_y(x, a, b):\n    position = trace(\"pos\", Normal)(a * x + b, 1.0)\n    return position\n\n\n@genjax.gen\ndef model():\n    x_mu = trace(\"x_mu\", TFPUniform)(-3.0, 3.0)\n    a = trace(\"a\", TFPUniform)(-4.0, 4.0)\n    b = trace(\"b\", TFPUniform)(-3.0, 3.0)\n    x = trace(\"x\", sample_x)(x_mu, 1.0)\n    y = trace(\"y\", sample_y)(x, a, b)\n    return y\n\nAnd, most importantly, visualizations.\n\ndef viz(ax, x, y, **kwargs):\n    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\ntrs = []\nfor ax in axes.flatten():\n    key, tr = jitted(key, ())\n    x = tr[\"x\", \"pos\"]\n    y = tr[\"y\", \"pos\"]\n    trs.append(tr)\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nA nice diffuse prior over points."
  },
  {
    "objectID": "concepts/gradient_interfaces/gradient_interfaces.html#map-estimation",
    "href": "concepts/gradient_interfaces/gradient_interfaces.html#map-estimation",
    "title": "Differentiable programming with GenJAX",
    "section": "MAP estimation",
    "text": "MAP estimation\nWhen it comes to looking at the interfaces, a good first step is gradient-based maximum a posteriori probability (MAP) estimation. Let’s write this using the lowest level interface unzip first:\nNow, often we may have a trace in hand, and we just want the first-order gradient with respect to certain random choices (specified by a genjax.Selection). This is a relatively common occurrence - so there’s a higher-level API choice_grad which gives us exactly this thing.3 Here’s MapUpdate using choice_grad.3 It’s not compositional with jax.grad - but if we need that power, we can just drop back down to use unzip.\n\n@dataclasses.dataclass\nclass MapUpdate(genjax.Pytree):\n    selection: genjax.Selection\n    tau: genjax.typing.FloatArray\n\n    def flatten(self):\n        return (self.tau,), (self.selection,)\n\n    def apply(self, key, trace):\n        args = trace.get_args()\n        gen_fn = trace.get_gen_fn()\n        key, forward_gradient_trie = gen_fn.choice_grad(key, trace, self.selection)\n        forward_values = self.selection.filter(trace)\n        forward_values = forward_values.strip()\n        forward_values = jtu.tree_map(\n            lambda v1, v2: v1 + self.tau * v2,\n            forward_values,\n            forward_gradient_trie,\n        )\n        argdiffs = tuple(map(genjax.Diff.no_change, args))\n        key, (_, _, new_trace, _) = gen_fn.update(key, trace, forward_values, argdiffs)\n        return key, (new_trace, True)\n\n    def __call__(self, key, trace):\n        return self.apply(key, trace)\n\n\nmap_update = MapUpdate\n\nSimple, concise - works with any generative function whose choices specified by MapUpdate.selection support gradients on the joint logpdf.\nFrom the Pytree interface, any instance of MapUpdate has a static selection, and a tau (which determines the gradient step size) which can be dynamic.44 If this is your first time seeing the Pytree interface, note that it’s defined by the flatten interface - which allows us to specify runtime vs. trace time data in Pytree structures.\nBecause MapUpdate is a Pytree, in inference code, we’d just construct MapUpdate before calling it - and we can do this on either side of the JAX API boundary.55 E.g. outside of a jax.jit transform, inside - it’s all okay.\n\nupdate = map_update(genjax.select([\"x_mu\", \"a\", \"b\"]), 1e-4)\nupdate\n\n\n\n\nMapUpdate\n├── selection\n│   └── BuiltinSelection\n│       └── trie\n│           └── Trie\n│               ├── :x_mu\n│               │   └── AllSelection\n│               ├── :a\n│               │   └── AllSelection\n│               └── :b\n│                   └── AllSelection\n└── tau\n    └── (const) 0.0001\n\n\n\nLet’s take a sampled piece of data, extract the (\"x\", \"pos\") and (\"y\", \"pos\") addresses, and then use MAP optimization to estimate the mode of the posterior.\n\ntr = trs[2]\nselection = genjax.select([\"x\", \"y\"])\nchm = selection.filter(tr.strip())\nchm\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        ├── :x\n        │   └── VectorChoiceMap\n        │       ├── indices\n        │       │   └──  i32[100]\n        │       └── inner\n        │           └── BuiltinChoiceMap\n        │               └── trie\n        │                   └── Trie\n        │                       └── :pos\n        │                           └── ValueChoiceMap\n        │                               └── value\n        │                                   └──  f32[100]\n        └── :y\n            └── VectorChoiceMap\n                ├── indices\n                │   └──  i32[100]\n                └── inner\n                    └── BuiltinChoiceMap\n                        └── trie\n                            └── Trie\n                                └── :pos\n                                    └── ValueChoiceMap\n                                        └── value\n                                            └──  f32[100]\n\n\n\n\nx = chm[\"x\", \"pos\"]\ny = chm[\"y\", \"pos\"]\nfig_data, ax_data = plt.subplots(figsize=(3, 3), dpi=140)\nviz(ax_data, x, y, marker=\".\")\n\n\n\n\nIf we apply MapUpdate, we take a single optimization step:\n\nkey, (_, tr) = jax.jit(model.importance)(key, chm, ())\nkey, (tr, _) = update(key, tr)\n\nWe can use scan to apply MapUpdate repeatedly.\n\ndef chain(key, tr):\n    def _inner(carry, _):\n        key, tr = carry\n        key, (tr, _) = update(key, tr)\n        return (key, tr), ()\n\n    (key, tr), _ = jax.lax.scan(_inner, (key, tr), None, length=2000)\n    return key, tr\n\n\njitted = jax.jit(chain)\nkey, tr = jitted(key, tr)\n\nNow, we can plot the polynomial described by \"a\" and \"b\", with evaluation points generated around the estimated \"x_mu\".\n\ndef polynomial_at_x(x, coefficients):\n    basis_values = jnp.array([1.0, x])\n    polynomial_value = jnp.sum(coefficients * basis_values)\n    return polynomial_value\n\n\njitted = jax.jit(jax.vmap(polynomial_at_x, in_axes=(0, None)))\n\n\ndef plot_polynomial_values(ax, x, coefficients, **kwargs):\n    v = jitted(x, coefficients)\n    ax.scatter(x, v, **kwargs)\n\n\na = tr[\"a\"]\nb = tr[\"b\"]\nx_mu = tr[\"x_mu\"]\nkey, sub_key = jax.random.split(key)\nevaluation_points = x_mu + jax.random.normal(sub_key, shape=(1000,))\ncoefficients = jnp.array([b, a])\nplot_polynomial_values(\n    ax_data,\n    evaluation_points,\n    coefficients,\n    marker=\".\",\n    color=\"gold\",\n    alpha=0.05,\n)\nfig_data"
  },
  {
    "objectID": "concepts/combinators/intro_to_combinators.html",
    "href": "concepts/combinators/intro_to_combinators.html",
    "title": "Introduction to generative function combinators",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nGen helps probabilistic programmers design and implement models and inference algorithms by automating the (often) complicated inference math. The generative function interface is the key abstraction layer which provides this automation. Generative function language designers can extend the interface to new generative function objects - providing domain-specific patterns and optimizations which users can automatically take advantage of.\nOne key class of generative function languages are combinators - higher-order functions which accept generative functions as input, and produce a new generative function type as an output.\nCombinators functionally transform the generative structure that we pass into them, expressing useful patterns - including chain-like computations, IID sampling patterns, or generative computations which form grammar-like structures.\nCombinators also expose optimization opportunities - by registering the patterns as generative functions, implementors (e.g. the library authors) can specialize the implementation of the generative function interface methods. Users of combinators can then take advantage of this interface specialization to express asymptotically optimal updates (useful in e.g. MCMC kernels), or optimized importance weight calculations.\nIn this notebook, we’ll be discussing Unfold - a combinator for expressing generative computations which are reminiscent of state-space (or Markov) models. To keep things simple, we’ll explore a hidden Markov model example - but combinator usage generalizes to models with much richer structure."
  },
  {
    "objectID": "concepts/combinators/intro_to_combinators.html#introducing-unfold",
    "href": "concepts/combinators/intro_to_combinators.html#introducing-unfold",
    "title": "Introduction to generative function combinators",
    "section": "Introducing Unfold",
    "text": "Introducing Unfold\nLet’s discuss Unfold.11 A quick reminder: when in doubt, you can use the console from console = genjax.pretty() to inspect the classes which we discuss in the notebooks.\nHow do we make an instance of Unfold? Given an existing generative function which is a kernel - a kernel accepts and returns the same type signature - we can create a valid Unfold instance.22 This is not strictly true. Unfold also allows you to pass in a set of static arguments which are provided to the kernel after the state argument, unchanged, at each time step. We show this at the bottom of the notebook.\nHere’s an example kernel:\n\n@genjax.gen\ndef kernel(prev_latent):\n    new_latent = genjax.Normal(prev_latent, 1.0) @ \"z\"\n    new_obs = genjax.Normal(new_latent, 1.0) @ \"x\"\n    return new_latent\n\n\nkey, tr = jax.jit(kernel.simulate)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function kernel&gt;\n├── args\n│   └── tuple\n│       └──  f32[]\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :x\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├──  f32[]\n│       │       │       └──  f32[]\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :z\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └──  f32[]\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nTo create an Unfold instance, we provide two things:\n\nThe kernel generative function.\nA static maximum unroll chain argument. Dynamically, Unfold may not unroll all the way up to this maximum - but for JAX/XLA compilation, we need to provide this maximum value as an invariant upper bound for any invocation of Unfold.\n\n\nchain = genjax.Unfold(kernel, max_length=10)\nchain\n\n\n\n\nUnfoldCombinator\n├── max_length\n│   └── (const) 10\n└── kernel\n    └── BuiltinGenerativeFunction\n        └── source\n            └── &lt;function kernel&gt;\n\n\n\nTo invoke an interface method, the arguments which Unfold expects is a Tuple, where the first element is the maximum index in the resulting chain, and the second element is the initial state.\n\n\n\n\n\n\nUsage of index argument vs. a length argument\n\n\n\nNote how we’ve bolded index above - think of the index value as denoting an upper bound on active indices for the resulting chain. An active index is one in which the value was evolved using the kernel from the previous value. Passing in index = 5 means: all values after return[5] are not evolved, they’re just filled with the return[5] value.\nIndexing follows Python convention - so e.g. passing in 0 as the index means that a single application of the kernel was applied to the state, before evolution was halted and evolved statically.\n\n\n\nkey, tr = jax.jit(chain.simulate)(key, (5, 0.3))\ntr\n\n\n\n\nVectorTrace\n├── gen_fn\n│   └── UnfoldCombinator\n│       ├── max_length\n│       │   └── (const) 10\n│       └── kernel\n│           └── BuiltinGenerativeFunction\n│               └── source\n│                   └── &lt;function kernel&gt;\n├── indices\n│   └──  i32[10]\n├── inner\n│   └── BuiltinTrace\n│       ├── gen_fn\n│       │   └── BuiltinGenerativeFunction\n│       │       └── source\n│       │           └── &lt;function kernel&gt;\n│       ├── args\n│       │   └── tuple\n│       │       └──  f32[10]\n│       ├── retval\n│       │   └──  f32[10]\n│       ├── choices\n│       │   └── Trie\n│       │       ├── :x\n│       │       │   └── DistributionTrace\n│       │       │       ├── gen_fn\n│       │       │       │   └── _Normal\n│       │       │       ├── args\n│       │       │       │   └── tuple\n│       │       │       │       ├──  f32[10]\n│       │       │       │       └──  f32[10]\n│       │       │       ├── value\n│       │       │       │   └──  f32[10]\n│       │       │       └── score\n│       │       │           └──  f32[10]\n│       │       └── :z\n│       │           └── DistributionTrace\n│       │               ├── gen_fn\n│       │               │   └── _Normal\n│       │               ├── args\n│       │               │   └── tuple\n│       │               │       ├──  f32[10]\n│       │               │       └──  f32[10]\n│       │               ├── value\n│       │               │   └──  f32[10]\n│       │               └── score\n│       │                   └──  f32[10]\n│       ├── cache\n│       │   └── Trie\n│       └── score\n│           └──  f32[10]\n├── args\n│   └── tuple\n│       ├──  i32[]\n│       └──  f32[]\n├── retval\n│   └──  f32[10]\n└── score\n    └──  f32[]\n\n\n\n\ntr.indices\n\nArray([ 0,  1,  2,  3,  4,  5, -1, -1, -1, -1], dtype=int32, weak_type=True)\n\n\n\n\ntr.get_retval()\n\nArray([-1.0472125 ,  0.282778  , -0.5653119 ,  0.92312765, -0.31269455,\n        0.46679875,  0.46679875,  0.46679875,  0.46679875,  0.46679875],      dtype=float32, weak_type=True)\n\n\n\nNote how tr.indices keep track of where the chain stopped evolving, according to the index argument to Unfold. In tr.get_retval(), we see that the final dynamic value (afterwards, evolution stops) occurs at index = 5."
  },
  {
    "objectID": "concepts/combinators/intro_to_combinators.html#combinator-choice-maps",
    "href": "concepts/combinators/intro_to_combinators.html#combinator-choice-maps",
    "title": "Introduction to generative function combinators",
    "section": "Combinator choice maps",
    "text": "Combinator choice maps\nTypically, each combinator has a unique choice map. The choice map simultaneously represents the structure of the generative choices which the transformed combinator generative function makes, as well as optimization opportunities which a user can take advantage of.\nLet’s study the choice map for UnfoldTrace.\n\nchm = tr.get_choices()\nchm\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_22328/1817037715.py:1 in &lt;module&gt;     │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_22328/1817037715.py'                 │\n│                                                                                                  │\n│ /Users/mccoybecker/research/genjax/src/genjax/_src/generative_functions/combinators/vector/vecto │\n│ r_datatypes.py:73 in get_choices                                                                 │\n│                                                                                                  │\n│    70 │   │   return self.args                                                                   │\n│    71 │                                                                                          │\n│    72 │   def get_choices(self):                                                                 │\n│ ❱  73 │   │   return VectorChoiceMap.new(self.indices, self.inner)                               │\n│    74 │                                                                                          │\n│    75 │   def get_gen_fn(self):                                                                  │\n│    76 │   │   return self.gen_fn                                                                 │\n│ &lt;@beartype(genjax._src.generative_functions.combinators.vector.vector_datatypes.VectorChoiceMap. │\n│ new) at 0x13f955da0&gt;:27 in new                                                                   │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nBeartypeCallHintParamViolation: @beartyped \ngenjax._src.generative_functions.combinators.vector.vector_datatypes.VectorChoiceMap.new() parameter \nindices=\"Array([ 0,  1,  2,  3,  4,  5, -1, -1, -1, -1], dtype=int32, weak_type=True)\" violates type hint \ntyping.Union[list, float, jaxtyping.Float[Array, '...']], as &lt;protocol \"jaxlib.xla_extension.Array\"&gt; \"Array([ 0,  \n1,  2,  3,  4,  5, -1, -1, -1, -1], dtype=int32, weak_type=True)\" not float, &lt;class \"jaxtyping.Float[Array, \n'...']\"&gt;, or list.\n\n\n\nAgain, let’s look at the indices.\n\nchm.indices\n\nNo surprises - the choice map also keeps track of which indices are active, and which indices are inactive.\nInactive indices do not participate in inference metadata computations - so e.g. if we ask for the score of the trace:\n\ntr.get_score()\n\nThe score is the same as the sum of sub-trace scores from [0:5].\n\nnp.sum(tr.get_subtree(\"z\").get_score()[0:6] + tr.get_subtree(\"x\").get_score()[0:6])\n\nThe reason why we have an index argument is that we can dynamically choose how much of the chain contributes to the generative computation. This index argument can come from other generative function - it need not be a JAX trace-time static value.\nWith this in mind, it’s best to think of Unfold as representing a space of processes which unroll up to some maximum static length - but the active generative process can halt before that maximum length."
  },
  {
    "objectID": "intermediate/bayes_3d/scratch.html",
    "href": "intermediate/bayes_3d/scratch.html",
    "title": "Gen ⊗ JAX",
    "section": "",
    "text": "import bayes3d as b\nimport genjax\nimport jax.numpy as jnp\nimport jax\nimport os\n\nconsole = genjax.pretty(show_locals=False)\n\n\nfrom dataclasses import dataclass\nfrom math import pi\n\nimport jax\nimport jax.numpy as jnp\n\nfrom genjax._src.generative_functions.distributions.distribution import ExactDensity\n\n\n@dataclass\nclass GaussianVMFPose(ExactDensity):\n    def sample(self, key, pose_mean, var, concentration, **kwargs):\n        return b.distributions.gaussian_vmf_sample(key, pose_mean, var, concentration)\n\n    def logpdf(self, pose, pose_mean, var, concentration, **kwargs):\n        return b.distributions.gaussian_vmf_logpdf(pose, pose_mean, var, concentration)\n\n\ngaussian_vmf_pose = GaussianVMFPose()\nkey = jax.random.PRNGKey(314159)\ncenter_pose = b.t3d.transform_from_pos(jnp.array([0.0, 0.0, 2.0]))\n\n\n@genjax.gen\ndef scene(center_pose):\n    pose = genjax.trace(\"pose\", gaussian_vmf_pose)(center_pose, 0.0001, 0.001)\n    return pose\n\n\nsimulate_jit = jax.jit(scene.simulate)\n\n\nkey = jax.random.PRNGKey(0)\ntrace = simulate_jit(key, (center_pose,))\n\n\ncenter_pose\n\nArray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 2.],\n       [0., 0., 0., 1.]], dtype=float32)"
  },
  {
    "objectID": "intermediate/bayes_3d/bayes_3d.html",
    "href": "intermediate/bayes_3d/bayes_3d.html",
    "title": "Gen ⊗ JAX",
    "section": "",
    "text": "import genjax"
  },
  {
    "objectID": "intermediate/gaussian_processes/gaussian_processes.html",
    "href": "intermediate/gaussian_processes/gaussian_processes.html",
    "title": "Gaussian process modeling with tinygp",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import trace\nimport tinygp\nimport tinygp.kernels as kernels\n\nggp = genjax.tinygp()\n\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nGaussian process models are a well-explored nonparametric model class describing distributions over spaces of functions. They also support sampling, logpdf evaluation, as well as exact conditioning. In GenJAX, we support usage of Gaussian process models using an auxiliary library tinygp which provides a lightweight implementation of the interfaces above for many common Gaussian process kernels.\nggp.GaussianProcess\n\n\n\n\n&lt;class 'genjax._src.extras.tinygp.tinygp.GaussianProcess'&gt;\nTo construct a Gaussian process model which implements the generative function interface, it suffices to provide the constructor gpp.GaussianProcess with a tinygp kernel:\nkernel_scaled = 4.5 * kernels.ExpSquared(scale=1.5)\nmodel = ggp.GaussianProcess(kernel_scaled)\nmodel\n\n\n\n\nGaussianProcess\n└── kernel\n    └── Product\n        ├── kernel1\n        │   └── Constant\n        │       └── value\n        │           └── (const) 4.5\n        └── kernel2\n            └── ExpSquared\n                ├── scale\n                │   └── (const) 1.5\n                └── distance\n                    └── L2Distance\nThis model is a generative function - meaning it supports all the generative function interfaces.\nisinstance(model, genjax.GenerativeFunction)\n\nTrue\nTo call simulate, we must provide a grid for evaluation:\ngrid = jnp.arange(0.0, 50.0, 0.5)\ngrid\n\nArray([ 0. ,  0.5,  1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5. ,\n        5.5,  6. ,  6.5,  7. ,  7.5,  8. ,  8.5,  9. ,  9.5, 10. , 10.5,\n       11. , 11.5, 12. , 12.5, 13. , 13.5, 14. , 14.5, 15. , 15.5, 16. ,\n       16.5, 17. , 17.5, 18. , 18.5, 19. , 19.5, 20. , 20.5, 21. , 21.5,\n       22. , 22.5, 23. , 23.5, 24. , 24.5, 25. , 25.5, 26. , 26.5, 27. ,\n       27.5, 28. , 28.5, 29. , 29.5, 30. , 30.5, 31. , 31.5, 32. , 32.5,\n       33. , 33.5, 34. , 34.5, 35. , 35.5, 36. , 36.5, 37. , 37.5, 38. ,\n       38.5, 39. , 39.5, 40. , 40.5, 41. , 41.5, 42. , 42.5, 43. , 43.5,\n       44. , 44.5, 45. , 45.5, 46. , 46.5, 47. , 47.5, 48. , 48.5, 49. ,\n       49.5], dtype=float32)\nWe can visualize samples from our genjax.GaussianProcess - and of course, we can jax.jit our interfaces, as usual.\ndef viz(ax, x, y, **kwargs):\n    ax.plot(x, y, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (grid,))\n    x = grid\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()"
  },
  {
    "objectID": "intermediate/gaussian_processes/gaussian_processes.html#structured-modeling-and-inference-with-genjax",
    "href": "intermediate/gaussian_processes/gaussian_processes.html#structured-modeling-and-inference-with-genjax",
    "title": "Gaussian process modeling with tinygp",
    "section": "Structured modeling and inference with GenJAX",
    "text": "Structured modeling and inference with GenJAX\nLet’s create some data, and then consider constructing a model and performing inference. Below, we’ll consider a noisy sinusoidal data generating process.\n\nkey, sub_key = jax.random.split(key)\nn = 50\nf = lambda x: 10 * jnp.sin(x)\nx = jax.random.uniform(key=sub_key, minval=-3.0, maxval=3.0, shape=(n,)).sort()\nkey, sub_key = jax.random.split(key)\nground_truth = f(x) + jax.random.normal(sub_key, shape=(n,))\n\n\nf, ax = plt.subplots()\nax.scatter(x, ground_truth)\n\n\n\n\nTo showcase the power of combining tinygp with a more expressive language, let’s consider a switching model with two types of kernels. The first switch branch will include a prior over the parameters of the a kernels.ExpSquared kernel, and the second will provide a kernels.Cosine kernel. Following this path, we’ll showcase how we can use genjax.GaussianProcess models inside of larger generative functions.\n\n@genjax.gen\ndef model(data):\n    factor = trace(\"factor\", genjax.TFPUniform)(0.1, 3.0)\n    scale = trace(\"scale\", genjax.TFPUniform)(0.1, 5.0)\n    kernel_scaled = kernels.Cosine(scale=scale)\n    gp = ggp.GaussianProcess(kernel_scaled)\n    y = trace(\"y\", gp)(data)\n    return y\n\n\ndef viz(ax, x, y, **kwargs):\n    ax.plot(x, y, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (x,))\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nWe can easily use sampling importance resampling here, let’s look at the results.\n\nobservations = genjax.choice_map({\"y\": ground_truth})\ninf = genjax.sampling_importance_resampling(1000, model)\ninf\n\n\n\n\nSamplingImportanceResampling\n├── num_particles\n│   └── (const) 1000\n├── model\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── &lt;function model&gt;\n└── proposal\n    └── (const) None\n\n\n\n\njitted = jax.jit(model.simulate)\nkey, tr = jitted(key, (x,))\ntr.get_retval()\n\nArray([-0.43208396, -0.58294547, -0.848159  , -0.7861588 , -0.545494  ,\n       -0.16411299,  0.14950526,  0.2080805 ,  0.8111007 ,  0.86555636,\n        0.8450676 ,  0.6299165 ,  0.32383266, -0.01688776, -0.30093664,\n       -0.5840381 , -0.6380779 , -0.81765443, -0.7392957 , -0.6654223 ,\n       -0.54848593, -0.21036375,  0.8061874 ,  0.82833725,  0.83593285,\n       -0.02110198, -0.499939  , -0.5576827 , -0.81426835, -0.39774978,\n        0.36000574,  0.62700665,  0.6447593 ,  0.83770037,  0.7923152 ,\n        0.26099455, -0.24413885, -0.23712076, -0.38988945, -0.56829345,\n       -0.5653826 , -0.84069645, -0.81233406, -0.6983161 , -0.59467906,\n       -0.02127809,  0.23496075,  0.2594615 ,  0.36821437,  0.7813539 ],      dtype=float32)\n\n\n\n\njitted = jax.jit(jax.vmap(model.simulate, in_axes=(0, None)))\nkey, sub_keys = genjax.slash(key, 100)\n_, tr = jitted(sub_keys, (x,))\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_23406/2246199438.py:3 in &lt;module&gt;     │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_23406/2246199438.py'                 │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/traceback_util.py │\n│ :162 in reraise_with_filtered_traceback                                                          │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/api.py:622 in     │\n│ cache_miss                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/dispatch.py:241   │\n│ in _xla_call_impl_lazy                                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/linear_util.py:303 in  │\n│ memoized_fun                                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/dispatch.py:357   │\n│ in _xla_callable_uncached                                                                        │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/dispatch.py:348   │\n│ in sharded_lowering                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/profiler.py:314   │\n│ in wrapper                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/interpreters/pxla.py:2 │\n│ 792 in lower_sharding_computation                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/profiler.py:314   │\n│ in wrapper                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/interpreters/partial_e │\n│ val.py:2065 in trace_to_jaxpr_final                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/interpreters/partial_e │\n│ val.py:1998 in trace_to_subjaxpr_dynamic                                                         │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/linear_util.py:167 in  │\n│ call_wrapped                                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/traceback_util.py │\n│ :162 in reraise_with_filtered_traceback                                                          │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/api.py:1685 in    │\n│ vmap_f                                                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/tree_util.py:75   │\n│ in tree_unflatten                                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/tinygp/helpers.py:65 in    │\n│ clz_from_iterable                                                                                │\n│                                                                                                  │\n│   62 │   │   meta_args = tuple(zip(meta_fields, meta))                                           │\n│   63 │   │   data_args = tuple(zip(data_fields, data))                                           │\n│   64 │   │   kwargs = dict(meta_args + data_args)                                                │\n│ ❱ 65 │   │   return data_clz(**kwargs)                                                           │\n│   66 │                                                                                           │\n│   67 │   jax.tree_util.register_pytree_node(                                                     │\n│   68 │   │   data_clz, iterate_clz, clz_from_iterable                                            │\n│ &lt;string&gt;:5 in __init__                                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/tinygp/kernels/stationary. │\n│ py:63 in __post_init__                                                                           │\n│                                                                                                  │\n│    60 │                                                                                          │\n│    61 │   def __post_init__(self) -&gt; None:                                                       │\n│    62 │   │   if jnp.ndim(self.scale):                                                           │\n│ ❱  63 │   │   │   raise ValueError(                                                              │\n│    64 │   │   │   │   \"Only scalar scales are permitted for stationary kernels; use\"             │\n│    65 │   │   │   │   \"transforms.Linear or transforms.Cholesky for more flexiblity\"             │\n│    66 │   │   │   )                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Only scalar scales are permitted for stationary kernels; usetransforms.Linear or transforms.Cholesky \nfor more flexiblity\n\n\n\n\nscale = tr[\"scale\"]\nfactor = tr[\"factor\"]\nkernel_scaled = factor * kernels.ExpSquared(scale=scale)\ngp = ggp.GaussianProcess(kernel_scaled)\nv = gp.sample(key, x)\nf, ax = plt.subplots()\nax.scatter(x, v)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistic programming with GenJAX",
    "section": "",
    "text": "TODO (MyBinder link - when repo is public)\n\n\n\nYou can open a Binder executable environment by following this tag:"
  },
  {
    "objectID": "index.html#what-is-genjax",
    "href": "index.html#what-is-genjax",
    "title": "Probabilistic programming with GenJAX",
    "section": "What is GenJAX?",
    "text": "What is GenJAX?\n\n(One sentence) It’s a GPU accelerated probabilistic programming library designed to support model composition and inference customization.\n(Longer) It’s an implementation of Gen1 on top of JAX - exposing the ability to programmatically construct and manipulate generative functions, as well as JIT compile + auto-batch inference computations using generative functions onto GPU devices.\n\n1 Gen is a multi-paradigm (generative, differentiable, incremental) language for probabilistic programming focused on generative functions: computational objects which represent probability measures over structured sample spaces."
  },
  {
    "objectID": "index.html#gens-approach-to-probabilistic-programming",
    "href": "index.html#gens-approach-to-probabilistic-programming",
    "title": "Probabilistic programming with GenJAX",
    "section": "Gen’s approach to probabilistic programming",
    "text": "Gen’s approach to probabilistic programming\nGen’s primary conceptual contribution is a formal interface (the generative function interface) of methods and associated data types which support the implementation of several families of inference algorithms. The interface forms an abstraction layer between generative function languages (those objects which implement the interface) and inference algorithms.\nThe interface is compositional - allowing usage of generative functions inside of other generative functions. It is also extensible: users can and are encouraged to write their own generative function languages, with custom optimizations or internal inference facilities.\nOf course, GenJAX provide a number of essential implementations, including a language of distributions (from both JAX and TensorFlow Probability) a program-like modeling language based on pure, numerical Python programs, and a set of higher-order combinators which express structured patterns of generative computation.\nGenJAX also provides a standard library of inference algorithms which works with generative functions - including importance sampling, MCMC, ADVI, SMC, and more."
  }
]