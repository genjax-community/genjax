{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d3e1fbc1-03e8-4c32-9849-bb9d83c79895",
   "metadata": {},
   "source": [
    "---\n",
    "title: Differentiable programming with GenJAX\n",
    "date: \"December 7, 2022\"\n",
    "abstract: This notebook describes the gradient interfaces exposed by generative function implementors in GenJAX. It also describes `TrainCombinator`, a generative function combinator which exposes the ability to train learnable parameters using gradients with respect to the log joint density of the model.\n",
    "callout-appearance: simple\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694af04e-db99-4705-a990-e18b8373dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import genjax\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Pretty printing.\n",
    "console = genjax.pretty(width=80)\n",
    "\n",
    "# Reproducibility.\n",
    "key = jax.random.PRNGKey(314159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4287478-6467-4988-a760-50efec899125",
   "metadata": {},
   "source": [
    "The generative function interface exposes functionality which allows usage of generative functions for differentiable programming. These interfaces are designed to work seamlessly with `jax.grad` - allowing (even higher-order) gradient computations which are useful for inference algorithms which require gradients and gradient estimators. In this notebook, we'll describe some of these interfaces - as well as their (current, but not forever) limitations. We'll walk through an implementation of MAP estimation, as well as the Metropolis-adjusted Langevin algorithm (MALA) using these interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f20fd-1e12-41db-897e-f25bb011efbc",
   "metadata": {},
   "source": [
    "## Gradient interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e3799-0ff3-488a-9d70-7f37b472feb1",
   "metadata": {},
   "source": [
    "Because JAX features best-in-class support for higher-order AD, GenJAX exposes interfaces that compose natively with JAX's interfaces for gradients. The primary interface method which provides `jax.grad`-compatible functions from generative functions is an interface called `unzip`. \n",
    "\n",
    "`unzip` allows a user to provide a key, and a fixed choice map - and it returns a new key and two closures:\n",
    "\n",
    "* The first closure is a \"score\" closure which accepts a choice map as the first argument, and arguments which match the non-`PRNGKey` signature types of the generative function. The score closure returns the exact joint score of the generative function. It computes the exact joint score using an interface called `assess`.^[Caveat: `assess` is not required to return the _exact_ joint score, only an estimate. However, if `jax.grad` is used on estimates - the resulting thing is not a correct gradient estimator. See the important callout below!]\n",
    "* The second closure is a \"retval\" closure which accepts a choice map as the first argument, and arguments which match the non-`PRNGKey` signature types of the generative function. The retval closure executes the generative function constrained using the union of the fixed choice map, and the user provided choice map, and returns the return value of the execution. Here, the return value is also provided by invoking the `assess` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1249b43-ad3d-4aad-8f30-d9f511f1862f",
   "metadata": {},
   "source": [
    "So really, `unzip` is syntactic sugar over another interface called `assess`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a1188-b204-47b0-b1d9-49760a676d74",
   "metadata": {},
   "source": [
    "### `assess` for exact density evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff24b11-a9de-4520-81ef-0ba23c178c69",
   "metadata": {},
   "source": [
    "`assess` is a generative function interface method which computes log joint density estimates from generative functions. `assess` requires that a user provide a choice map _which completely fills all choices encountered during execution_. Otherwise, it errors.^[And these errors are thrown at JAX trace time, so you'll get an exception before runtime.]\n",
    "\n",
    "If a generative function also draws from untraced randomness - `assess` computes an estimate whose expectation over the distribution of untraced randomness gives the correct log joint density. \n",
    "\n",
    "::: {.callout-important}\n",
    "\n",
    "## Correctness of gradient estimators\n",
    "\n",
    "When used on generative functions which include untraced randomness, naively applying `jax.grad` to the closures returned by interfaces described in this notebook **do not compute** gradient estimators which are unbiased with respect to the true gradients.\n",
    "\n",
    "Short: don't use these with untraced randomness. We're working on alternatives.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c41668-1b68-40f2-ad8a-1ef5ccd22916",
   "metadata": {},
   "source": [
    "## MAP estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bcf43-7e73-4e83-ae29-663cec67619b",
   "metadata": {},
   "source": [
    "## Exposing learnable modules with `TrainCombinator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d0ca5-8111-4c0a-bc0b-f1b92ed3d9e0",
   "metadata": {},
   "source": [
    "## Automatic differentiation variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e357e-9dcd-41ae-a7cc-e7b1f281793c",
   "metadata": {},
   "source": [
    "In this section, we'll show how we can use the gradient interfaces to implement [Automatic differentiation variational inference](https://arxiv.org/abs/1603.00788)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
