{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d3e1fbc1-03e8-4c32-9849-bb9d83c79895",
   "metadata": {},
   "source": [
    "---\n",
    "title: Differentiable programming with GenJAX\n",
    "date: \"December 7, 2022\"\n",
    "abstract: This notebook describes the gradient interfaces exposed by generative function implementors in GenJAX. It also describes `TrainCombinator`, a generative function combinator which exposes interfaces that make it easy to train learnable parameters using gradients with respect to the log joint density of the model. We describe how `TrainCombinator` can be used to perform maximum likelihood optimization, as well as variational family optimization.\n",
    "callout-appearance: simple\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694af04e-db99-4705-a990-e18b8373dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import genjax\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Pretty printing.\n",
    "console = genjax.pretty(width=80)\n",
    "\n",
    "# Reproducibility.\n",
    "key = jax.random.PRNGKey(314159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4287478-6467-4988-a760-50efec899125",
   "metadata": {},
   "source": [
    "The generative function interface exposes functionality which allows usage of generative functions for differentiable programming. These interfaces are designed to work seamlessly with `jax.grad` - allowing (even higher-order) gradient computations which are useful for inference algorithms which require gradients and gradient estimators. In this notebook, we'll describe some of these interfaces - as well as their (current, but not forever) limitations. We'll walk through an implementation of MAP estimation, as well as the Metropolis-adjusted Langevin algorithm (MALA) using these interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f20fd-1e12-41db-897e-f25bb011efbc",
   "metadata": {},
   "source": [
    "## Gradient interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e3799-0ff3-488a-9d70-7f37b472feb1",
   "metadata": {},
   "source": [
    "Because JAX features best-in-class support for higher-order AD, GenJAX exposes interfaces that compose natively with JAX's interfaces for gradients. The primary interface method which provides `jax.grad`-compatible functions from generative functions is an interface called `unzip`. \n",
    "\n",
    "`unzip` allows a user to provide a key, and a fixed choice map - and it returns a new key and two closures:\n",
    "\n",
    "* The first closure is a \"score\" closure which accepts a choice map as the first argument, and arguments which match the non-`PRNGKey` signature types of the generative function. The score closure returns the exact joint score of the generative function. It computes the exact joint score using an interface called `assess`.^[Caveat: `assess` is not required to return the _exact_ joint score, only an estimate. However, if `jax.grad` is used on estimates - the resulting thing is not a correct gradient estimator. See the important callout below!]\n",
    "* The second closure is a \"retval\" closure which accepts a choice map as the first argument, and arguments which match the non-`PRNGKey` signature types of the generative function. The retval closure executes the generative function constrained using the union of the fixed choice map, and the user provided choice map, and returns the return value of the execution. Here, the return value is also provided by invoking the `assess` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1249b43-ad3d-4aad-8f30-d9f511f1862f",
   "metadata": {},
   "source": [
    "So really, `unzip` is syntactic sugar over another interface called `assess`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a1188-b204-47b0-b1d9-49760a676d74",
   "metadata": {},
   "source": [
    "### `assess` for exact density evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff24b11-a9de-4520-81ef-0ba23c178c69",
   "metadata": {},
   "source": [
    "`assess` is a generative function interface method which computes log joint density estimates from generative functions. `assess` requires that a user provide a choice map _which completely fills all choices encountered during execution_. Otherwise, it errors.^[And these errors are thrown at JAX trace time, so you'll get an exception before runtime.]\n",
    "\n",
    "If a generative function also draws from untraced randomness - `assess` computes an estimate whose expectation over the distribution of untraced randomness gives the correct log joint density. \n",
    "\n",
    "::: {.callout-important}\n",
    "\n",
    "## Correctness of gradient estimators\n",
    "\n",
    "When used on generative functions which include untraced randomness, naively applying `jax.grad` to the closures returned by interfaces described in this notebook **do not compute** gradient estimators which are unbiased with respect to the true gradients.\n",
    "\n",
    "Short: don't use these with untraced randomness. We're working on alternatives.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f1989-0a8d-4177-bf24-40ba39752acd",
   "metadata": {},
   "source": [
    "## A running example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1916f6-827e-4177-9087-2e6f86b8d53e",
   "metadata": {},
   "source": [
    "Let's consider the following model, which we'll cover in different variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34cdb9a-a7eb-4e5f-b084-ed874f7fb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@genjax.gen\n",
    "def model(key):\n",
    "    key, x_mu = genjax.trace(\"x_mu\", genjax.Uniform)(key, -3.0, 3.0)\n",
    "    key, a = genjax.trace(\"a\", genjax.Uniform)(key, -3.0, 3.0)\n",
    "    key, b = genjax.trace(\"b\", genjax.Uniform)(key, -3.0, 3.0)\n",
    "    key, x = genjax.trace(\"x\", genjax.Normal)(key, x_mu, 1.0)\n",
    "    return genjax.trace(\"y\", genjax.Normal)(key, a * x + b, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97844cd9-b919-4c8f-8994-80b0e6a5b035",
   "metadata": {},
   "source": [
    "And, most importantly, visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be41b6b-1b63-4105-bd38-8c18f74b8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(ax, x, y, **kwargs):\n",
    "    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\n",
    "jitted = jax.jit(jax.vmap(model.simulate, in_axes=(0, None)))\n",
    "for ax in axes.flatten():\n",
    "    key, *sub_keys = jax.random.split(key, 30 + 1)\n",
    "    sub_keys = jnp.array(sub_keys)\n",
    "    _, tr = jitted(sub_keys, ())\n",
    "    x = tr[\"x\"]\n",
    "    y = tr[\"y\"]\n",
    "    viz(ax, x, y, marker=\".\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa2990-0af5-41fd-9897-d610f0c91f14",
   "metadata": {},
   "source": [
    "A nice diffuse prior over points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c41668-1b68-40f2-ad8a-1ef5ccd22916",
   "metadata": {},
   "source": [
    "## MAP estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60c1de-0aef-4648-8692-c4c3bc6f815a",
   "metadata": {},
   "source": [
    "A good first step into the gradient waters is gradient-based maximum a posteriori probability (MAP) estimation. Here's how we write this in GenJAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b3904-0825-4e87-bc6f-eb1f1b8b5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class MapUpdate(genjax.Pytree):\n",
    "    selection: genjax.Selection\n",
    "    tau: genjax.Float\n",
    "\n",
    "    def flatten(self):\n",
    "        return (self.tau,), (self.selection,)\n",
    "\n",
    "    def apply(self, key, trace):\n",
    "        args = trace.get_args()\n",
    "        gen_fn = trace.get_gen_fn()\n",
    "        key, forward_gradient_trie = gen_fn.choice_grad(key, trace, selection)\n",
    "        forward_values, _ = self.selection.filter(trace)\n",
    "        forward_values = forward_values.strip()\n",
    "        forward_values = jtu.tree_map(\n",
    "            lambda v1, v2: v1 + self.tau * v2,\n",
    "            forward_values,\n",
    "            forward_gradient_trie,\n",
    "        )\n",
    "        key, (_, _, new_trace, _) = gen_fn.update(\n",
    "            key, trace, forward_values, args\n",
    "        )\n",
    "        return key, (new_trace, True)\n",
    "\n",
    "    def __call__(self, key, trace):\n",
    "        return self.apply(key, trace)\n",
    "\n",
    "\n",
    "map_update = MapUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b86e43-7b9f-4d8f-806b-61b29b487f85",
   "metadata": {},
   "source": [
    "Simple, concise - works with any generative function whose addresses in `MapUpdate.selection` are modelled as continuous random variables.\n",
    "\n",
    "From the `Pytree` interface, any instance of `MapUpdate` has a static `selection`, and a `tau` (which determines the gradient step size) which can be dynamic.\n",
    "\n",
    "In inference code, we'd just construct `MapUpdate` before calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19baf973-6901-4f9b-9944-b91b72348b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "update = map_update(genjax.select([\"x_mu\", \"a\", \"b\"]), 0.3)\n",
    "update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bcf43-7e73-4e83-ae29-663cec67619b",
   "metadata": {},
   "source": [
    "## Exposing learnable modules with `TrainCombinator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3ec19-204e-4f32-afc6-94e911c0a8a9",
   "metadata": {},
   "source": [
    "For learning and variational inference, learnable parameters of model families are an important feature. In GenJAX, we expose a lightweight generative function wrapper around generative functions which accept `learnable_params` as a last argument - this wrapper is called `TrainCombinator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d2cfa-c622-41c1-8253-dfb93ae71c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"x_mu\": 0.0, \"a\": 0.3, \"b\": 0.4}\n",
    "\n",
    "\n",
    "@genjax.gen(genjax.TrainCombinator, params=params)\n",
    "def model(key, params):\n",
    "    x_mu = params[\"x_mu\"]\n",
    "    a = params[\"a\"]\n",
    "    b = params[\"b\"]\n",
    "    key, x = genjax.trace(\"x\", genjax.Normal)(key, x_mu, 1.0)\n",
    "    return genjax.trace(\"y\", genjax.Normal)(key, a * x + b, 1.0)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24486bef-761a-4137-a1b3-09b76d35cd0b",
   "metadata": {},
   "source": [
    "`TrainCombinator` is a module-like abstraction which closes over the parameter store passed in and initialized by the constructor. When we call `TrainCombinator`, we don't need to provide the `params` argument - it's handled automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455cab19-8ef7-4997-93b8-e037f3e38974",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, tr = model.simulate(key, ())\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c905c3-a92b-4ace-bb57-4b83fa0505f5",
   "metadata": {},
   "source": [
    "`TrainCombinator` exposes a convenient interface to a specialized scoring function which accepts `params` evaluation points, and returns the model logpdf.^[Note: because this interface method returns a **function**, we cannot JIT it. However, if we use it to produce a closure and then use that closure inside of code which we JIT, it's fine. As long as an object of **function** type doesn't try to escape across the JAX API boundary.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8be66-57b1-4ebe-8455-04f084224e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, logpdf = model.score_params(key, tr, params)\n",
    "logpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81cb05-4672-4e43-b193-080dcec97f08",
   "metadata": {},
   "source": [
    "This interface supports batching out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb524c9-40f7-4e44-9132-18c394cf55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, *sub_keys = jax.random.split(key, 100 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "sub_keys, tr = jax.vmap(model.simulate)(sub_keys, ())\n",
    "_, logpdf = jax.vmap(model.score_params, in_axes=(0, 0, None))(\n",
    "    sub_keys, tr, params\n",
    ")\n",
    "logpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14ff74-1f2b-4e3c-8148-7115bb88a6a9",
   "metadata": {},
   "source": [
    "We make extensive use of batch evaluation in variational inference. For now, let's consider maximum likelihood learning and see the other `TrainCombinator` interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d0ca5-8111-4c0a-bc0b-f1b92ed3d9e0",
   "metadata": {},
   "source": [
    "## Automatic differentiation variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850114a9-3d30-461e-9c54-04f763b7b1c5",
   "metadata": {},
   "source": [
    "In this section, we'll show how we can use the gradient interfaces to implement [Automatic differentiation variational inference](https://arxiv.org/abs/1603.00788)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
