{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a generative function with a single variable but 2000 observations or I just want to use/apply it repeatedly, what do I do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 1 1 2 1 2 2 1 1 2 2 0 2 1 0 1 2 1]\n",
      "[2 0 2 1 1 2 1 2 2 1 1 2 2 0 2 1 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import genjax\n",
    "import jax\n",
    "from genjax import bernoulli\n",
    "from genjax import gen\n",
    "\n",
    "# First start by creating a simple generative function\n",
    "@gen\n",
    "def double_flip(p,q):\n",
    "    v1 = bernoulli(p) @ \"v1\" \n",
    "    v2 = bernoulli(q) @ \"v2\" \n",
    "    return v1+v2\n",
    "\n",
    "# Now we can create a vectorized version that takes a batch of p values\n",
    "# and calls the function for each value in the batch.\n",
    "# The `in_axes` tell the `vmap_combinator` which arguments are \n",
    "# mapped over, and which are not. \n",
    "# The value `0` means we will map over this argument and `None` means we will not.\n",
    "batched_double_flip = genjax.vmap_combinator(double_flip, in_axes=(0,None))\n",
    "\n",
    "# Now we can use the batched version to generate a batch of samples\n",
    "key = jax.random.PRNGKey(0)\n",
    "size_of_batch = 20\n",
    "# To do so, we have to create batched keys and p values\n",
    "p = jax.random.uniform(key, (size_of_batch,))\n",
    "q = 0.5\n",
    "# We will run the generative function once for (p1,q), once for (p2,q), ...\n",
    "traces = batched_double_flip.simulate(key, (p,q))\n",
    "print(traces.get_retval())\n",
    "\n",
    "# We can also use call it on (p1,q1), (p2,q2), ...\n",
    "p = jax.random.uniform(key, (size_of_batch,))\n",
    "q = jax.random.uniform(key, (size_of_batch,))\n",
    "batched_double_flip_v2 = genjax.vmap_combinator(double_flip, in_axes=(0,0))\n",
    "traces = batched_double_flip_v2.simulate(key, (p,q))\n",
    "print(traces.get_retval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The batched version of the generative function is not working correctly\n"
     ]
    }
   ],
   "source": [
    "# We cannot batch different variables with different shapes\n",
    "try:\n",
    "    p = jax.random.uniform(key, (size_of_batch,))\n",
    "    q = jax.random.uniform(key, (size_of_batch+1,))\n",
    "    traces = batched_double_flip_v2.simulate(key, (p,q))\n",
    "    print(traces.get_retval())\n",
    "except:\n",
    "    print(\"Error: The batched version of the generative function is not working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "#TODO: adapt example below from Arijit for iterated vmap.\n",
    "image = jnp.zeros([300,500], dtype=jnp.float32)\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "@gen\n",
    "def sample_pixel(pixel):\n",
    "    new_pixel = genjax.normal(pixel,1.0) @ \"new_pixel\"\n",
    "    return new_pixel\n",
    "\n",
    "# on one \"pixel\" value\n",
    "tr = sample_pixel.simulate(key,(0.0,))\n",
    "#TODO: print(tr['new_pixel'])\n",
    "# prints Array(1.3694694, dtype=float32)\n",
    "\n",
    "# Now what if we want to apply a generative function over a 2D space? We can do a nested MAP combinator\n",
    "sample_image = genjax.vmap_combinator(in_axes=(0,))(genjax.vmap_combinator(in_axes=(0,))(sample_pixel))\n",
    "\n",
    "# sample an image\n",
    "tr = sample_image.simulate(key,(image,))\n",
    "#TODO: print(tr.inner.inner['new_pixel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can call the generative function with a repeat combinator\n",
    "\n",
    "#TODO: don't jit inside a for loop. is there an equivalent mistake in genjax?\n",
    "#TODO: is my thing compiling or is it blocked at traced time? check make_jaxpr.\n",
    "#TODO: I'm running OOM, what do I do?\n",
    "#TODO: why is nested vmap so slow?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genjax-trials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
