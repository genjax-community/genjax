[
  {
    "objectID": "advanced/pseudomarginal_smc/pseudomarginal_smc.html",
    "href": "advanced/pseudomarginal_smc/pseudomarginal_smc.html",
    "title": "Coarse-to-fine structure inference with pseudomarginal SMC",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\n\nIn this notebook, we’ll be combining several advanced ingredients of Gen and GenJAX to construct approximate densities induced by customized sequential Monte Carlo (SMC) targeting successively richer approximations to latent structure.\nThe notebook assumes several pre-requisites:\n\nUnderstanding generative functions, their interfaces, and the math that the interface functions compute.\nUnderstanding generative function combinators, and how genjax.Recurse can allow us to express bounded tree-like computations.\nUnderstanding the approximate density interface exposed by GenProx, and how we can equip generative functions with inference strategies to define approximate densities."
  },
  {
    "objectID": "advanced/smc_language/smc_language.html",
    "href": "advanced/smc_language/smc_language.html",
    "title": "The sequential Monte Carlo mini-language",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)"
  },
  {
    "objectID": "advanced/impl_builtin_language/impl_builtin_language.html",
    "href": "advanced/impl_builtin_language/impl_builtin_language.html",
    "title": "Implementing the builtin modeling language",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import GenerativeFunction, ChoiceMap, Selection, trace\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nOne key property of the generative function interface is that it enables a separation between model and inference code - providing an abstraction layer that facilitates the development of modular model pieces, and then inference pieces that abstract over the implementation of the interface.\nNow, implementing the interface on objects, and composing them in various ways (by e.g. specializing the implementation of the interface functions to support any intended composition) is a valid way to construct new generative functions. In fact, this is the pattern which generative function combinators follow - they accept generative functions as input, and produce new generative functions whose implementations are specialized to represent some specific pattern of computation.\nExplicitly constructing generative functions using languages of objects, however, can often feel unwieldy. Part of the way that GenJAX (and Gen.jl) alleviates this restriction is by exposing languages which construct generative functions from programs. This drastically increases the expressivity available to the programmer.\nIn GenJAX, here’s an example of the BuiltinGenerativeFunction language:\nWhen we apply one of the interface functions to this object, we get the associated data types that we expect.\nHow exactly do we do this? In this notebook, you’re going to find out. You’ll also get a chance to explore some of the capabilities which JAX exposes to library designers. Ideally, you’ll also get a sense of some of the limitations of JAX (and GenJAX) - which are restricted to support programs which are amenable to GPU/TPU acceleration."
  },
  {
    "objectID": "advanced/impl_builtin_language/impl_builtin_language.html#the-magic-of-jax",
    "href": "advanced/impl_builtin_language/impl_builtin_language.html#the-magic-of-jax",
    "title": "Implementing the builtin modeling language",
    "section": "The magic of JAX",
    "text": "The magic of JAX\nLet’s examine the generative function object:\n\nmodel\n\n\n\n\nBuiltinGenerativeFunction\n└── source\n    └── <function model>\n\n\n\nAll the decorator genjax.gen does is wrap the function into this object. It holds a reference to the function we defined above.\nBut clearly, we need to somehow get inside that function - because we’re recording data onto the BuiltinTrace which come from intermediate results of the execution of the function.\nThat’s where JAX comes in - JAX provides a way to trace pure, numerical Python programs - enabling us to construct program transformations which return new functions that compute different semantics from the original function.11 Program tracing is an approach which has its roots in automatic differentiation. If you’re interesting in this technique, I cannot recommend Autodidax: JAX core from scratch enough. It will introduce you to enough interesting PL ideas to keep you occupied for months, if not years.\nLet’s utilize one of JAX’s interpreters to construct an intermediate representation of the function which our generative function object holds reference to:\n\njaxpr = jax.make_jaxpr(model.source)(1.0)\njaxpr\n\n{ lambda ; a:f32[]. let\n    b:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap b\n    c:f32[] = trace[addr=y gen_fn=_Normal() tree_in=PyTreeDef((*, *))] a 1.0\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[] = add c d\n    f:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap f\n    g:f32[] = trace[addr=z gen_fn=_Normal() tree_in=PyTreeDef((*, *))] e 1.0\n  in (g,) }\n\n\n\nSo jax.make_jaxpr takes a function f :: A -> B and returns a function f :: A -> Jaxpr, where Jaxpr is the program representation above.\nWhen we run this function using Python’s interpreter, JAX lifts the input to something called a Tracer, JAX keeps an internal stack of interpreters which redirect infix operations on Tracer instances and modify their behavior. Additionally, JAX exposes new primitives (like all the NumPy primitives) which wrap a function called bind. bind takes in Tracer arguments, looks through them (and the interpreter stack), selects the interpreter which should handle the call - and then the interpreter is allowed to process_primitive - invoking the semantics which the interpreter defines for that primitive.\njax.make_jaxpr uses the above process to walk the program, and construct the above intermediate representation.\nNow, the point of having this representation is that we can transform it further! We can lower it to other representations (including things like XLA - the linear algebra accelerator that JAX utilizes to go high performance). We could also write another interpreter which walks this representation, invokes other primitives with bind, etc - deferring further transformation to the next interpreter in line.\nThis (admittedly rough description) above is the secret behind JAX’s compositional transformations."
  },
  {
    "objectID": "advanced/impl_builtin_language/impl_builtin_language.html#new-semantics-via-program-transformations",
    "href": "advanced/impl_builtin_language/impl_builtin_language.html#new-semantics-via-program-transformations",
    "title": "Implementing the builtin modeling language",
    "section": "New semantics via program transformations",
    "text": "New semantics via program transformations\nLet’s examine the representation once more.\n\njaxpr\n\n{ lambda ; a:f32[]. let\n    b:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap b\n    c:f32[] = trace[addr=y gen_fn=_Normal() tree_in=PyTreeDef((*, *))] a 1.0\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[] = add c d\n    f:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap f\n    g:f32[] = trace[addr=z gen_fn=_Normal() tree_in=PyTreeDef((*, *))] e 1.0\n  in (g,) }\n\n\n\nYou’ll notice that there is an intrinsic called trace here - which looks suspiciously similar to genjax.trace above.\ntrace is a custom primitive that GenJAX defines - by defining a new primitive, we can place a stub in the intermediate representation, which we can further transform to implement the semantics we wish to express.\n\nA high level view\nNow, we need to transform it! Here’s where some serious design decisions enter into the picture.\nOne thing you might notice about the Jaxpr above is that the the arity of the function is fixed, and so is the arity of the return value. But when we call simulate on our model - we get out something which is not a h :: f32[] (it’s actually a jax.Pytree with a lot more data - so we’d expect a lot more return values in the Jaxpr2.2 JAX flattens/unflattens Pytree instances on each side of the IR boundary - the IR is strongly typed, but only natively supports a few base types, and a few composite array types.\nWhat gives?\nHere’s where JAX’s support for compositional application of interpreters comes into play.\nInstead of attempting to modify the IR above to change the arity of everything (a process which the authors expect would be quite painful, and buggy) - we can write another interpreter which walks the IR and evaluates it, but that interpreter can keep track of the state that we want to put into the BuiltinTrace at the end of the interface invocation.\nThen, we can stage out that interpreter to support JIT compilation, etc. I’ll describe the process below in pseudo-types:\nWe start with f :: A -> B, and we stage it to get a new function f' :: Type[A] -> Jaxpr, then we write an interpreter I with signature I :: (Jaxpr, A) -> (B, State). The application of I itself can also be staged.\nSo this is really nice - we don’t have to munge the IR manually, we just get to write an interpreter to do the transformation for us. That’s the power that JAX provides for us!\n\n\nInterpreter design decisions\nWith the high-level view in mind, we’ll examine two of the interface implementations. The first is simulate - likely the easiest implementation to understand3. The second is update.3 For this notebook, we’re going to ignore the inference math that we wish to support!\nNow, in GenJAX, the interpreter is written to be re-usable for each of the interface functions. Because we’ve chosen to re-use the interpreter (and parametrize the transformation semantics by configuring the interpreter in other ways – besides the implementation), you’re going to see some complexity right out the gate.\nThe reason why this complexity is there is because we wish to expose incremental computing optimizations in update. To support this customization, the interpreter can best be described as a propagation interpreter - similar to Julia’s abstract interpretation machinery (if you’re familiar). A propagation interpreter treats the Jaxpr as an undirected graph - and performs interpretation by iterating until a fixpoint condition is satisfied.\nThe high level pattern from the previous section is still true! But if you’ve written interpreters for something like Structure and Interpretation of Computer Programs before - this interpreter might be a slight shock to the system.\nHere’s a boiled down form of the simulate_transform:\n\ndef simulate_transform(f, **kwargs):\n    def _inner(key, args):\n        # Step 1: stage out the function to a `Jaxpr`.\n        closed_jaxpr, (flat_args, in_tree, out_tree) = stage(f)(\n            key, *args, **kwargs\n        )\n        jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n\n        # Step 2: create a `Simulate` instance, which we parametrize\n        # the propagation interpreter with.\n        #\n        # `Bare` is an instance of something called a `Cell` - the\n        # objects which the propagation interpreter reasons about.\n        handler = Simulate()\n        final_env, ret_state = propagate(\n            Bare,\n            bare_propagation_rules,\n            jaxpr,\n            [Bare.new(v) for v in consts],\n            list(map(Bare.new, flat_args)),\n            [Bare.unknown(var.aval) for var in jaxpr.outvars],\n            handler=handler,\n        )\n\n        # Step 3: when the interpreter finishes, we read the values\n        # out of its environment.\n        flat_out = safe_map(final_env.read, jaxpr.outvars)\n        flat_out = map(lambda v: v.get_val(), flat_out)\n        key_and_returns = jtu.tree_unflatten(out_tree, flat_out)\n        key, *retvals = key_and_returns\n        retvals = tuple(retvals)\n\n        # Here's the handler state - remember the signature from\n        # above `I :: (Jaxpr, A) -> (B, State)`, these fields\n        # below are the `State`.\n        score = handler.score\n        chm = handler.choice_state\n        cache = handler.cache_state\n\n        # This returns all the things which we want to put\n        # into `BuiltinTrace`.\n        return key, (f, args, retvals, chm, score), cache\n\n    return _inner\n\nAnd, just to show you that this is the key behind how we implement simulate, I’ve copied the BuiltinGenerativeFunction class method for simulate below:\n\ndef simulate(self, key, args, **kwargs):\n    assert isinstance(args, Tuple)\n    key, (f, args, r, chm, score), cache = simulate_transform(\n        self.source, **kwargs\n    )(key, args)\n    return key, BuiltinTrace(self, args, r, chm, cache, score)\n\nWe’ll discuss propagate in a moment - but a few high-level things.\nNote that the simulate method can be staged out / used with JAX’s interfaces:\n\njitted = jax.jit(model.simulate)\nkey, tr = jitted(key, (1.0,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function model>\n├── args\n│   └── tuple\n│       └──  f32[]\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :y\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├──  f32[]\n│       │       │       └──  f32[]\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :z\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └──  f32[]\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nThat’s because simulate_transform and the interpreter implementation itself for propagate are all JAX traceable.\nThe only difference between the BuiltinTrace which we first generated at the top of the notebook and this one is that jax.jit will lift the 1.0 argument to a Tracer type, versus the non-jitted interpreter which just uses the Python float value.\nAnd again, we can also stage out our simulate implementation and get a Jaxpr back:\n\njax.make_jaxpr(model.simulate)(key, (1.0,))\n\n{ lambda ; a:u32[2] b:f32[]. let\n    c:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap c\n    d:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap d\n    e:key<fry>[] = random_wrap[impl=fry] a\n    f:key<fry>[2] = random_split[count=2] e\n    g:u32[2,2] = random_unwrap f\n    h:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] g\n    i:u32[2] = squeeze[dimensions=(0,)] h\n    j:u32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=(1, 1)] g\n    k:u32[2] = squeeze[dimensions=(0,)] j\n    l:key<fry>[] = random_wrap[impl=fry] k\n    m:u32[] = random_bits[bit_width=32 shape=()] l\n    n:u32[] = shift_right_logical m 9\n    o:u32[] = or n 1065353216\n    p:f32[] = bitcast_convert_type[new_dtype=float32] o\n    q:f32[] = sub p 1.0\n    r:f32[] = sub 1.0 -0.9999999403953552\n    s:f32[] = mul q r\n    t:f32[] = add s -0.9999999403953552\n    u:f32[] = reshape[dimensions=None new_sizes=()] t\n    v:f32[] = max -0.9999999403953552 u\n    w:f32[] = erf_inv v\n    x:f32[] = mul 1.4142135381698608 w\n    y:f32[] = mul 1.0 x\n    z:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    ba:f32[] = add z y\n    bb:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    bc:f32[] = sub ba bb\n    bd:f32[] = div bc 1.0\n    be:f32[] = abs bd\n    bf:f32[] = integer_pow[y=2] be\n    bg:f32[] = log 6.283185307179586\n    bh:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bg\n    bi:f32[] = add bf bh\n    bj:f32[] = mul -1.0 bi\n    bk:f32[] = log 1.0\n    bl:f32[] = sub 2.0 bk\n    bm:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bl\n    bn:f32[] = div bj bm\n    bo:f32[] = reduce_sum[axes=()] bn\n    bp:f32[] = add 0.0 bo\n    bq:f32[] = add ba b\n    br:key<fry>[] = random_wrap[impl=fry] i\n    bs:key<fry>[2] = random_split[count=2] br\n    bt:u32[2,2] = random_unwrap bs\n    bu:u32[1,2] = slice[\n      limit_indices=(1, 2)\n      start_indices=(0, 0)\n      strides=(1, 1)\n    ] bt\n    bv:u32[2] = squeeze[dimensions=(0,)] bu\n    bw:u32[1,2] = slice[\n      limit_indices=(2, 2)\n      start_indices=(1, 0)\n      strides=(1, 1)\n    ] bt\n    bx:u32[2] = squeeze[dimensions=(0,)] bw\n    by:key<fry>[] = random_wrap[impl=fry] bx\n    bz:u32[] = random_bits[bit_width=32 shape=()] by\n    ca:u32[] = shift_right_logical bz 9\n    cb:u32[] = or ca 1065353216\n    cc:f32[] = bitcast_convert_type[new_dtype=float32] cb\n    cd:f32[] = sub cc 1.0\n    ce:f32[] = sub 1.0 -0.9999999403953552\n    cf:f32[] = mul cd ce\n    cg:f32[] = add cf -0.9999999403953552\n    ch:f32[] = reshape[dimensions=None new_sizes=()] cg\n    ci:f32[] = max -0.9999999403953552 ch\n    cj:f32[] = erf_inv ci\n    ck:f32[] = mul 1.4142135381698608 cj\n    cl:f32[] = mul 1.0 ck\n    cm:f32[] = add bq cl\n    cn:f32[] = sub cm bq\n    co:f32[] = div cn 1.0\n    cp:f32[] = abs co\n    cq:f32[] = integer_pow[y=2] cp\n    cr:f32[] = log 6.283185307179586\n    cs:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cr\n    ct:f32[] = add cq cs\n    cu:f32[] = mul -1.0 ct\n    cv:f32[] = log 1.0\n    cw:f32[] = sub 2.0 cv\n    cx:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cw\n    cy:f32[] = div cu cx\n    cz:f32[] = reduce_sum[axes=()] cy\n    da:f32[] = add bp cz\n  in (bv, b, cm, b, 1.0, ba, bo, bq, 1.0, cm, cz, da) }\n\n\n\nGiving us our pure, array math code. You can’t help but admit that that’s pretty elegant!"
  },
  {
    "objectID": "advanced/impl_builtin_language/impl_builtin_language.html#how-does-propagate-work",
    "href": "advanced/impl_builtin_language/impl_builtin_language.html#how-does-propagate-work",
    "title": "Implementing the builtin modeling language",
    "section": "How does propagate work?",
    "text": "How does propagate work?\nNow, in this section - we’re going to talk about the nitty gritty of propagate itself. What exactly is this interpreter doing? Let’s examine the context surrounding the call to propagate:\ndef simulate_transform(f, **kwargs):\n    def _inner(key, args):\n        closed_jaxpr, (flat_args, in_tree, out_tree) = stage(f)(\n            key, *args, **kwargs\n        )\n        jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n        handler = Simulate()\n        final_env, ret_state = propagate(\n            # A lattice type\n            Bare,\n            \n            # Lattice propagation rules\n            bare_propagation_rules,\n            \n            # The Jaxpr which we wish to interpret\n            jaxpr,\n            \n            # Trace-time constants\n            [Bare.new(v) for v in consts],\n            \n            # Input cells\n            list(map(Bare.new, flat_args)),\n            \n            # Output cells\n            [Bare.unknown(var.aval) for var in jaxpr.outvars],\n            \n            # How we handle `trace`.\n            handler=handler,\n        )\n        ...\n\n    return _inner\nFirst, we stage our model function into a Jaxpr - when we perform the staging process, everything (e.g. custom datatypes which are Pytree implementors) gets flattened out to array leaves.\nAfter we stage, we collect all the data which we want to use to initialize our interpreter’s environment with - but we encounter our first bit of complexity.\nWhat is Bare? And what is a Cell? Let’s start with the latter question: a Cell is an abstract type which represents a lattice value.\nTo understand what a lattice value is - it’s worth gaining a high-level picture of what propagate attempts to do. propagate is an interpreter based on mixed concrete/abstract interpretation - it treats the Jaxpr as a graph - where the operations are nodes in the graph, and the SSA values (e.g. the named registers like ci, cj, etc) are edges.\nThe interpreter will iterate over the graph - attempting to update information about the edges by applying propagation rules (hence the name, propagate) which we define (bare_propagation_rules, above).\nA propagation rule accepts a list of input cells (the SSA edges which flow into the operation) and a list of output cells. It returns a new modified list of input cells, and a new modified list of output cells, as well as a state value (in this notebook, we won’t discuss the state value - it’s unneeded for the interfaces we will describe).\nThe way the interpreter works is that it keeps a queue of nodes and an environment which maps SSA values to lattice values. We pop a node off the queue, grab the existing lattice values for input SSA values and output SSA values, attempt to update them using a propagation rule, and then store the update in the environment. In addition, after we attempt to update the cells - we determine if the update has changed the information level of any of the cells. If the information level has changed for any cell (as measured using the partial order on lattice values), we add any nodes which the SSA value associated with that cell flows into back onto the queue.\nThis process describes an iterative algorithm which attempts to compute an information fixpoint - defined by a state transition function (which operates on the state of all cells in the Jaxpr - the environment) which we get to customize using propagation rules.\nI’m not going to inline any of the implementation of this interpreter into this notebook. I’ll refer the reader to the implementation of the interpreter.44 Note that the ideas behind this interpreter are quite widespread - but the original implementation (which the GenJAX authors modified) came from Oryx, and that implementation initially came from Roy Frostig (as far as we can tell).\n\nWhat happens in simulate?\nGreat - so how do we utilize this interpreter idea to implement the simulate_transform described above?"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html",
    "title": "Introduction to Gen and GenJAX",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import GenerativeFunction, ChoiceMap, Selection, trace\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-genjax",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-genjax",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is GenJAX?",
    "text": "What is GenJAX?\nHere are a few high-level ways to think about GenJAX:\n\nA probabilistic programming1 system based on the concepts of Gen.\nA Bayesian modelling and inference compiler with support for device acceleration (courtesy of JAX).\nA base layer for experiments in model and inference DSL design.\n\n1 New to probabilistic programming? Don’t fret, read on!There’s already a well-supported implementation of Gen in Julia. Why is a JAX port interesting?\nThere are a number of compelling technical and social reasons to explore Gen’s probabilistic programming paradigm on top of JAX, here are a few:\n\nJAX’s excellent accelerator support - our implementation natively supports several common accelerator idioms - like automatic struct-of-array representations, and the ability to automatically batch model/inference programs onto accelerators.\nJAX’s excellent support for compositional AD removes implementation and maintenance complexity for Gen’s gradient interfaces - previously a difficult challenge in other implementations. In addition, JAX’s support for convenient, higher-order AD opens up new opportunities to explore during inference design with gradient interfaces.\nJAX exposes compositional code transformations to library authors, and, as library authors, we can utilize code transformations to implement state-of-the-art optimizations for models and inference expressed in our system.\nA lot of domain experts and modelers are working in Python! Some of them even use JAX (hopefully more each year). Presenting an interface to Gen which is familiar, and takes advantage of JAX’s native ecosystem is a compelling social reason.\n\nLet’s truncate the list there for now.\nFor the JAX literati, one final (hopefully tantalizing) takeaway: by construction, all GenJAX modeling + inference code is JAX traceable - and thus, vmapable, jitable, etc."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-probabilistic-programming",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-probabilistic-programming",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is probabilistic programming?",
    "text": "What is probabilistic programming?\nPerhaps you may be coming to this notebook without any prior knowledge in probabilistic programming…\nThat’s okay! Ideally, the ideas in this notebook should be self-contained2.2 You may miss why generative functions (see below) are designed the way they are on a first read - but you’ll still get the punchline if you follow the notebook to the end.\n\nA Bayesian viewpoint\nHere’s one practical take on what probabilistic programming is all about: programming language design for expressing and solving Bayesian inference problems3.3 In the Probabilistic Computing lab at MIT, we also consider differentiable programming to be contained within the set of concerns of probabilistic programming. We won’t cover differentiable programming interfaces in this notebook.\nProbabilistic programming is a broad field, and there are corners which may not be covered by this practical take. We’ll just assume that people are interested in Bayes, and how to represent Bayes on computers in nice ways. For our purposes in this notebook, we’ll stick as much as we can to the basics.\n\n\nWhat are we actually computing with?\nThe objects which we program with expose a mixture of generative and differentiable interfaces - the interfaces are designed to support common (as well as quite advanced) classes of Bayesian inference algorithms. Gen provides automation for the tricky math which these algorithms require.\nWe separate the design of inference (whose implementation uses the interfaces), from the implementation of the interfaces on computational objects. This allows us to build languages of objects which satisfy the interfaces - and allows their compositional usage and interoperability.\nIn Gen, the objects which implement the interfaces are called generative functions."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-a-generative-function",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-a-generative-function",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is a generative function?",
    "text": "What is a generative function?\nGenerative functions are the key concept of Gen’s probabilistic programming paradigm. Generative functions are computational objects defined by a set of associated data types and methods. These types and methods describe compositional interfaces that are useful for Bayesian inference computations.\nGen’s formal description of generative functions consist of two objects:\n\n\\(P(\\tau, r; x)\\) - a normalized measure over tree-like data (choice maps) and untraced randomness4 \\(r\\), parametrized by arguments \\(x\\).\n\\(f(\\tau; x)\\) - a deterministic function from the above measure’s sample space to a space of data types.\n\n4 More on this later. It’s safe to say “I have no idea what that is” for now, and expect us to explain later or in another notebook.We can informally think of the sampling semantics of these objects as consisting of two steps:\n\nFirst, sample a choice map from \\(P\\).\nThen, compute the return value using \\(f\\).\n\nIn many of the generative function interfaces, we won’t just be interested in the final sampled return value. We’ll also be interested in what happened along the way: we’ll record the intermediate and final results of these steps in Trace objects - data structures which contain the recordings of values, along with probabilistic metadata like the score of random choices selected along the way.\nBelow, we provide an example of an (admittedly, not too interesting) GenJAX generative function5.5 There’s not just one generative function class - users can and are encouraged to design new types of generative functions which capture repeated modeling patterns. An excellent example of this modularity in Gen’s design is generative function combinators.\nThis generative function is part of a function-like language (the BuiltinGenerativeFunction language) - pay close attention to the hierarchical compositionality of generative functions in this language under an abstraction (genjax.trace) similar to a function call. We’ll discuss the addresses (\"sub\" and \"m0\") a bit later.\n\n@genjax.gen\ndef g(x):\n    m0 = genjax.trace(\"m0\", genjax.Bernoulli)(x)\n    return m0\n\n\n@genjax.gen\ndef h(x):\n    m0 = genjax.trace(\"sub\", g)(x)\n    return m0\n\n\nh\n\n\n\n\nBuiltinGenerativeFunction\n└── source\n    └── <function h>\n\n\n\nThis generative function holds a Python Callable object. For this generative function language, the interface methods (see the list under Generative function interface below) which are useful for modeling and inference are given semantics via JAX’s tracing and program transformation infrastructure.\nLet’s examine some of these operations now.\n\nconsole.inspect(genjax.BuiltinGenerativeFunction, methods=True)\n\n╭─ <class 'genjax._src.generative_functions.builtin.builtin_gen_fn.BuiltinGene─╮\n│ class BuiltinGenerativeFunction(source: Callable) -> None:                   │\n│                                                                              │\n│ BuiltinGenerativeFunction(source: Callable)                                  │\n│                                                                              │\n│         assess = def assess(self, key: jaxtyping.UInt[Array, '...'], chm:    │\n│                  genjax._src.core.datatypes.ChoiceMap, args: tuple,          │\n│                  **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],            │\n│                  tuple[typing.Any, typing.Union[float,                       │\n│                  jaxtyping.Float[Array, '...']]]]:                           │\n│    choice_grad = def choice_grad(self, key, trace, selection):               │\n│        flatten = def flatten(self):                                          │\n│ get_trace_type = def get_trace_type(self, key: jaxtyping.UInt[Array, '...'], │\n│                  args: tuple, **kwargs) ->                                   │\n│                  genjax._src.core.tracetypes.TraceType:                      │\n│     importance = def importance(self, key: jaxtyping.UInt[Array, '...'],     │\n│                  chm: genjax._src.core.datatypes.ChoiceMap, args: tuple,     │\n│                  **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],            │\n│                  tuple[typing.Union[float, jaxtyping.Float[Array, '...']],   │\n│                  genjax._src.generative_functions.builtin.builtin_datatypes… │\n│            new = def new(*args, **kwargs):                                   │\n│       simulate = def simulate(self, key: jaxtyping.UInt[Array, '...'], args: │\n│                  tuple, **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],     │\n│                  genjax._src.generative_functions.builtin.builtin_datatypes… │\n│      unflatten = def unflatten(data, xs):                                    │\n│          unzip = def unzip(self, key: jaxtyping.UInt[Array, '...'], fixed:   │\n│                  genjax._src.core.datatypes.ChoiceMap) ->                    │\n│                  Tuple[jaxUInt[Array, '...'],                                │\n│                  Callable[[genjax._src.core.datatypes.ChoiceMap, Tuple],     │\n│                  Union[float, jaxFloat[Array, '...']]],                      │\n│                  Callable[[genjax._src.core.datatypes.ChoiceMap, Tuple],     │\n│                  Any]]:                                                      │\n│         update = def update(self, key: jaxtyping.UInt[Array, '...'], prev:   │\n│                  genjax._src.core.datatypes.Trace, constraints:              │\n│                  genjax._src.core.datatypes.ChoiceMap, argdiffs: tuple,      │\n│                  **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],            │\n│                  tuple[typing.Any, typing.Union[float,                       │\n│                  jaxtyping.Float[Array, '...']],                             │\n│                  genjax._src.core.datatypes.Trace,                           │\n│                  genjax._src.core.datatypes.ChoiceMap]]:                     │\n╰──────────────────────────────────────────────────────────────────────────────╯\n\n\n\nThis is our first glimpse of the generative function interface (GFI), the secret sauce which Gen is based around.\n\n\n\n\n\n\nJAX interfaces\n\n\n\nThere’s a few methods here which are not part of the GFI, but are worth mentioning because they deal with data interfaces to JAX:\n\nflatten - which allows us to treat generative functions as Pytree implementors.\nunflatten - same as above.\n\nThese are used to register the implementor type as a Pytree, which is roughly a tree-like Python structure which JAX can zip/unzip at runtime API boundaries.\n\n\nLet’s study the simulate method first: we’ll explore its semantics, and see the types of data it produces.\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function h>\n├── args\n│   └── tuple\n│       └── (const) 0.3\n├── retval\n│   └──  bool[]\n├── choices\n│   └── Trie\n│       └── :sub\n│           └── BuiltinTrace\n│               ├── gen_fn\n│               │   └── BuiltinGenerativeFunction\n│               │       └── source\n│               │           └── <function g>\n│               ├── args\n│               │   └── tuple\n│               │       └── (const) 0.3\n│               ├── retval\n│               │   └──  bool[]\n│               ├── choices\n│               │   └── Trie\n│               │       └── :m0\n│               │           └── DistributionTrace\n│               │               ├── gen_fn\n│               │               │   └── _Bernoulli\n│               │               ├── args\n│               │               │   └── tuple\n│               │               │       └── (const) 0.3\n│               │               ├── value\n│               │               │   └──  bool[]\n│               │               └── score\n│               │                   └──  f32[]\n│               ├── cache\n│               │   └── Trie\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nIf you’re familiar with other “trace-based” probabilistic systems - this should look familiar.\nThis object instance is a piece of data which has captured information about the execution of the function. Specifically, the subtraces of other generative function calls which occur in genjax.trace statements.\nIt also captures the score - the log probability of the normalized measure which the model program represents, evaluated at the random choices which the generative call execution produced.\nIf you were paying attention above, the score is \\(\\log P(\\tau, r; x)\\).\n\nHow is simulate implemented for this language?\nFor this generative function language, we implement simulate using a code transformation! Here’s the transformed code.\n\njaxpr = jax.make_jaxpr(genjax.simulate(h))(key, (0.3,))\njaxpr\n\n{ lambda ; a:u32[2] b:f32[]. let\n    c:key<fry>[] = random_seed[impl=fry] 0\n    d:key<fry>[] = random_wrap[impl=fry] a\n    e:key<fry>[2] = random_split[count=2] d\n    f:u32[2,2] = random_unwrap e\n    g:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] f\n    h:u32[2] = squeeze[dimensions=(0,)] g\n    i:u32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=(1, 1)] f\n    j:u32[2] = squeeze[dimensions=(0,)] i\n    k:key<fry>[] = random_wrap[impl=fry] j\n    l:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    m:u32[] = random_bits[bit_width=32 shape=()] k\n    n:u32[] = shift_right_logical m 9\n    o:u32[] = or n 1065353216\n    p:f32[] = bitcast_convert_type[new_dtype=float32] o\n    q:f32[] = sub p 1.0\n    r:f32[] = sub 1.0 0.0\n    s:f32[] = mul q r\n    t:f32[] = add s 0.0\n    u:f32[] = reshape[dimensions=None new_sizes=()] t\n    v:f32[] = max 0.0 u\n    w:bool[] = lt v l\n    x:f32[] = convert_element_type[new_dtype=float32 weak_type=True] w\n    y:f32[] = sub x 0.0\n    z:bool[] = ne y 0.0\n    ba:f32[] = xla_call[\n      call_jaxpr={ lambda ; bb:bool[] bc:f32[] bd:f32[]. let\n          be:bool[] = convert_element_type[new_dtype=bool weak_type=False] bb\n          bf:f32[] = select_n be bd bc\n        in (bf,) }\n      name=_where\n    ] z y 1.0\n    bg:f32[] = xla_call[\n      call_jaxpr={ lambda ; bh:bool[] bi:f32[] bj:f32[]. let\n          bk:bool[] = convert_element_type[new_dtype=bool weak_type=False] bh\n          bl:f32[] = select_n bk bj bi\n        in (bl,) }\n      name=_where\n    ] z b 1.0\n    bm:f32[] = log bg\n    bn:f32[] = mul ba bm\n    bo:f32[] = xla_call[\n      call_jaxpr={ lambda ; bp:bool[] bq:f32[] br:f32[]. let\n          bs:bool[] = convert_element_type[new_dtype=bool weak_type=False] bp\n          bt:f32[] = select_n bs br bq\n        in (bt,) }\n      name=_where\n    ] z bn 0.0\n    bu:f32[] = sub 1.0 y\n    bv:f32[] = neg b\n    bw:bool[] = ne bu 0.0\n    bx:f32[] = xla_call[\n      call_jaxpr={ lambda ; by:bool[] bz:f32[] ca:f32[]. let\n          cb:bool[] = convert_element_type[new_dtype=bool weak_type=False] by\n          cc:f32[] = select_n cb ca bz\n        in (cc,) }\n      name=_where\n    ] bw bu 1.0\n    cd:f32[] = xla_call[\n      call_jaxpr={ lambda ; ce:bool[] cf:f32[] cg:f32[]. let\n          ch:bool[] = convert_element_type[new_dtype=bool weak_type=False] ce\n          ci:f32[] = select_n ch cg cf\n        in (ci,) }\n      name=_where\n    ] bw bv 1.0\n    cj:f32[] = log1p cd\n    ck:f32[] = mul bx cj\n    cl:f32[] = xla_call[\n      call_jaxpr={ lambda ; cm:bool[] cn:f32[] co:f32[]. let\n          cp:bool[] = convert_element_type[new_dtype=bool weak_type=False] cm\n          cq:f32[] = select_n cp co cn\n        in (cq,) }\n      name=_where\n    ] bw ck 0.0\n    cr:f32[] = add bo cl\n    cs:bool[] = lt y 0.0\n    ct:bool[] = gt y 1.0\n    cu:bool[] = convert_element_type[new_dtype=bool weak_type=False] cs\n    cv:bool[] = convert_element_type[new_dtype=bool weak_type=False] ct\n    cw:bool[] = or cu cv\n    cx:f32[] = xla_call[\n      call_jaxpr={ lambda ; cy:bool[] cz:f32[] da:f32[]. let\n          db:f32[] = select_n cy da cz\n        in (db,) }\n      name=_where\n    ] cw -inf cr\n    dc:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cx\n    dd:f32[] = reduce_sum[axes=()] dc\n    de:f32[] = add 0.0 dd\n    _:u32[2] = random_unwrap c\n    df:f32[] = add 0.0 de\n    dg:key<fry>[] = random_seed[impl=fry] 0\n    _:u32[2] = random_unwrap dg\n  in (h, b, w, b, w, b, w, dd, de, df) }\n\n\n\nThat’s a lot of code! This code is pure, numerical, and ready for acceleration. By utilizing JAX and a staging transformation, we’ve stripped out all Python overhead.\nThis is how we’ve implemented simulate for this particular generative function language.66 In general, Gen doesn’t require that we follow the same “code transformation” implementation for other generative function languages. GenJAX, however, is a bit special - because we restrict the user to remain within the JAX traceable subset of Python - any generative function interface implementation must also be JAX traceable. This is a JAX requirement, not a Gen one."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#generative-function-interface",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#generative-function-interface",
    "title": "Introduction to Gen and GenJAX",
    "section": "Generative function interface",
    "text": "Generative function interface\nThere are a few more generative function interface methods worth discussing.\nIn this notebook, instead of carefully walking through the math which these interface methods compute, we’ll defer that discussion to another notebook. Below, we give an informal discussion of what each of the interface methods computes, and roughly describe what algorithm families are supported by their usage.\n\nThe generative function interface in GenJAX\nGenJAX’s generative functions define an interface which support compositional usage of generative functions within other generative functions. The interface functions here closely mirror the interfaces defined in Gen.jl.\nIn the following, we use the following abbreviations:\n\nIS - importance sampling\nSMC - sequential Monte Carlo\nMCMC - Markov chain Monte Carlo\nVI - variational inference\n\n\n\n\n\n\n\n\n\nInterface\nType\nInference algorithm support\n\n\n\n\nsimulate\nGenerative\nIS, SMC\n\n\nimportance\nGenerative\nIS, SMC, VI\n\n\nupdate\nGenerative and incremental\nMCMC, SMC\n\n\nassess\nGenerative and differentiable\nMCMC, IS, SMC\n\n\nunzip\nDifferentiable\nDifferentiable and involutive MCMC and SMC, VI\n\n\n\nThis interface supports several methods - I’ve roughly described them and split them into the two categories Generative and Differentiable below:\n\nGenerative\n\nsimulate - sample from normalized trace measure, and return the score.\nimportance - given constraints for some addresses, sample from unnormalized trace measure and return an importance weight.\nupdate - given an existing trace, and a set of constraints and argument change values, update the trace to be consistent with the set of constraints under execution with the new arguments, and return an incremental importance weight.\nassess - given a complete choice map and arguments, return the normalized log probability.\n\n\n\nDifferentiable\n\nassess - same as above.\nunzip - given a set of fixed constraints, return two callables. The first callable score accepts constraints which fill in the complement of the fixed constraints and arguments, and returns the normalized log probability of all the constraints. The second callable retval accepts constraints and arguments, and returns the return value for the generative function call consistent with the constraints and given arguments.\n\nunzip produces two functions which can be compositionally used with jax.grad to evaluate gradients used by both differentiable and involutive MCMC and SMC."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#more-about-generative-functions",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#more-about-generative-functions",
    "title": "Introduction to Gen and GenJAX",
    "section": "More about generative functions",
    "text": "More about generative functions\nHere are a few more bits of information which should help you gain context with these objects.\n\nDistributions are generative functions\nIn GenJAX, distributions are generative functions.\n\nkey, tr = genjax.simulate(genjax.Normal)(key, (0.0, 1.0))\ntr\n\n\n\n\nDistributionTrace\n├── gen_fn\n│   └── _Normal\n├── args\n│   └── tuple\n│       ├── (const) 0.0\n│       └── (const) 1.0\n├── value\n│   └──  f32[]\n└── score\n    └──  f32[]\n\n\n\nThis should bring a sigh of relief! Ah, distributions are generative functions - the concepts can’t be too exotic.\nDistributions implement the interface in a conceptually simple way. They don’t have internal compositional choice structure (like the function-like BuiltinGenerativeFunction language above).\nDistributions themselves expose two interfaces:\n\nlogpdf - exact density evaluation.\nsample - exact sampling.\n\nWe can use these two interfaces to implement all the generative function interfaces for distributions.\n\n\nAssociated data types\n\nChoice maps are the tree-like recordings of random choices in a trace.\nSelection is an object which allows querying a trace/choice map - selecting certain choices.\n\n\n@genjax.gen\ndef h(x):\n    m1 = genjax.trace(\"m0\", genjax.Bernoulli)(x)\n    m2 = genjax.trace(\"m1\", genjax.Bernoulli)(x)\n    return m1 + m2\n\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function h>\n├── args\n│   └── tuple\n│       └── (const) 0.3\n├── retval\n│   └──  bool[]\n├── choices\n│   └── Trie\n│       ├── :m1\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Bernoulli\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       └── (const) 0.3\n│       │       ├── value\n│       │       │   └──  bool[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :m0\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Bernoulli\n│               ├── args\n│               │   └── tuple\n│               │       └── (const) 0.3\n│               ├── value\n│               │   └──  bool[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\n\nselection = genjax.select([\"m1\"])\nselected, _ = selection.filter(tr.get_choices())\nselected\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        └── :m1\n            └── DistributionTrace\n                ├── gen_fn\n                │   └── _Bernoulli\n                ├── args\n                │   └── tuple\n                │       └── (const) 0.3\n                ├── value\n                │   └──  bool[]\n                └── score\n                    └──  f32[]"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-can-i-do-with-them",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-can-i-do-with-them",
    "title": "Introduction to Gen and GenJAX",
    "section": "What can I do with them?",
    "text": "What can I do with them?\nNow, we’ve informally seen the interfaces and datatypes associated with generative functions.\nStudying the interfaces (and improvements thereof), as well as the computational objects which satisfy them can be an entire PhD’s worth of effort.\nIn the remainder of this notebook, let’s see how we can do machine learning with them.\nLet’s consider a modeling problem where we wish to perform generalized regression with outliers between two variates, taking a family of polynomials as potential curves.\nOne such model for this data generating process is shown below.\n\n# Two branches for a branching submodel.\n@genjax.gen\ndef model_y(x, coefficients):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    y = trace(\"value\", genjax.Normal)(polynomial_value, 0.3)\n    return y\n\n\n@genjax.gen\ndef outlier_model(x, coefficients):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    y = trace(\"value\", genjax.Normal)(polynomial_value, 30.0)\n    return y\n\n\n# The branching submodel.\nswitch = genjax.SwitchCombinator([model_y, outlier_model])\n\n# A mapped kernel function which calls the branching submodel.\n@genjax.gen(genjax.MapCombinator, in_axes=(0, None))\ndef kernel(x, coefficients):\n    is_outlier = trace(\"outlier\", genjax.Bernoulli)(0.1)\n    is_outlier = jnp.asarray(is_outlier, dtype=int)\n    y = trace(\"y\", switch)(is_outlier, x, coefficients)\n    return y\n\n\n@genjax.gen\ndef model(xs):\n    coefficients = trace(\"alpha\", genjax.MvNormal)(\n        np.zeros(3), 2.0 * np.identity(3)\n    )\n    ys = trace(\"ys\", kernel)(xs, coefficients)\n    return ys\n\nThere’s a number of implementation patterns which you might pick up on by studying this model.\n\nGenerative functions explicitly pass a PRNG key in and out. This conforms to JAX’s PRNG usage expectations.\nTo implement control flow, we use higher-order functions called combinators. These accept generative functions as input, and return generative functions as output.\nAny JAX compatible code is allowed in the body of a generative function.\n\nCourtesy of the interface, we get to design our model generative function in pieces.\nNow, let’s examine the sampled observation address (\"ys\", \"y\") from a sample trace from our model.\n\ndata = jnp.arange(0, 10, 0.5)\nkey, tr = jax.jit(model.simulate)(key, (data,))\ntr[\"ys\", \"y\"]\n\n\n\n\nSwitchTrace\n├── gen_fn\n│   └── SwitchCombinator\n│       └── branches\n│           └── list\n│               ├── BuiltinGenerativeFunction\n│               │   └── source\n│               │       └── <function model_y>\n│               └── BuiltinGenerativeFunction\n│                   └── source\n│                       └── <function outlier_model>\n├── chm\n│   └── IndexedChoiceMap\n│       ├── index\n│       │   └──  i32[20]\n│       └── submaps\n│           └── list\n│               ├── BuiltinTrace\n│               │   ├── gen_fn\n│               │   │   └── BuiltinGenerativeFunction\n│               │   │       └── source\n│               │   │           └── <function model_y>\n│               │   ├── args\n│               │   │   └── tuple\n│               │   │       ├──  f32[20]\n│               │   │       └──  f32[20,3]\n│               │   ├── retval\n│               │   │   └──  f32[20]\n│               │   ├── choices\n│               │   │   └── Trie\n│               │   │       └── :value\n│               │   │           └── DistributionTrace\n│               │   │               ├── gen_fn\n│               │   │               │   └── _Normal\n│               │   │               ├── args\n│               │   │               │   └── tuple\n│               │   │               │       ├──  f32[20]\n│               │   │               │       └──  f32[20]\n│               │   │               ├── value\n│               │   │               │   └──  f32[20]\n│               │   │               └── score\n│               │   │                   └──  f32[20]\n│               │   ├── cache\n│               │   │   └── Trie\n│               │   └── score\n│               │       └──  f32[20]\n│               └── BuiltinTrace\n│                   ├── gen_fn\n│                   │   └── BuiltinGenerativeFunction\n│                   │       └── source\n│                   │           └── <function outlier_model>\n│                   ├── args\n│                   │   └── tuple\n│                   │       ├──  f32[20]\n│                   │       └──  f32[20,3]\n│                   ├── retval\n│                   │   └──  f32[20]\n│                   ├── choices\n│                   │   └── Trie\n│                   │       └── :value\n│                   │           └── DistributionTrace\n│                   │               ├── gen_fn\n│                   │               │   └── _Normal\n│                   │               ├── args\n│                   │               │   └── tuple\n│                   │               │       ├──  f32[20]\n│                   │               │       └──  f32[20]\n│                   │               ├── value\n│                   │               │   └──  f32[20]\n│                   │               └── score\n│                   │                   └──  f32[20]\n│                   ├── cache\n│                   │   └── Trie\n│                   └── score\n│                       └──  f32[20]\n├── args\n│   └── tuple\n│       ├──  f32[20]\n│       └──  f32[20,3]\n├── retval\n│   └──  f32[20]\n└── score\n    └──  f32[20]\n\n\n\nHere, I’m just showing the subtrace from the switching model invocation - but it is already quite large and unwieldy!\nBesides, it also doesn’t tell us much about the values we truly care about here - the sampled values.\nFrom this model, we can get these in two ways.\nThe first way: we can just look at the trace return value.\n\ntr.get_retval()\n\nArray([   1.7448314 ,    0.41097283,   -1.9558886 ,   -5.498426  ,\n         -9.625496  ,  -15.6570425 ,  -22.74233   ,  -30.96311   ,\n        -30.182352  ,  -64.04883   ,  -62.844086  ,  -75.89779   ,\n        -90.39219   , -105.64662   , -121.877945  , -140.97981   ,\n       -159.82722   , -180.46971   , -207.86064   , -224.98926   ],      dtype=float32)\n\n\n\nThe second way: we can get them out of the choice map of the trace directly.\n\nchm = tr.get_choices()\nchm[\"ys\", \"y\", \"value\"]\n\nArray([   1.7448314 ,    0.41097283,   -1.9558886 ,   -5.498426  ,\n         -9.625496  ,  -15.6570425 ,  -22.74233   ,  -30.96311   ,\n        -30.182352  ,  -64.04883   ,  -62.844086  ,  -75.89779   ,\n        -90.39219   , -105.64662   , -121.877945  , -140.97981   ,\n       -159.82722   , -180.46971   , -207.86064   , -224.98926   ],      dtype=float32)\n\n\n\nNow, let’s construct a small visualization function to show us the samples.\n\ndef viz(ax, x, y, **kwargs):\n    (data,) = tr.get_args()\n    chm = tr.get_choices()\n    ys = np.array(chm[\"ys\", \"y\", \"value\"])\n    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (data,))\n    x = data\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nThese are the (\"ys\", \"y\", \"value\") samples for 9 traces from our model, against the points from the data we passed.\nWe just walked through one of the main elements of probabilistic programming: setting up a program, which represents a joint distribution over random variates, some of which we’ll identify with data we expect to see in the world.\nWe can adjust the noise settings of our model to produce wider priors over possible sets of points - and we may want to do this if our data is noisy!\nFor now, let’s keep the settings as is, and explore inference in GenJAX."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#your-first-inference-program",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#your-first-inference-program",
    "title": "Introduction to Gen and GenJAX",
    "section": "Your first inference program",
    "text": "Your first inference program\nNow, let’s say we have some data.\n\nx = np.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\ny = 2.0 * x + 1.5 + x**2\ny[2] = 50.0\n\n\nfig_data, ax_data = plt.subplots(figsize=(6, 6), dpi=140)\nviz(ax_data, x, y, color=\"blue\")\n\n\n\n\nIn Bayesian inference, if we wish to consider the conditional distribution \\(P(\\tau, r; x | \\text{data})\\) induced from a model \\(P(\\tau, \\text{data}, r; x)\\) - Bayes’ rule gives us a way to compute it.\n\\[\nP(\\tau, r; x | \\text{data}) = \\frac{P(\\tau, \\text{data}, r; x)}{\\int P(\\tau, \\text{data}, r; x) \\ d\\tau}\n\\]\nThe problem is that we often cannot compute the denominator (the evidence integral) easily. Instead, we turn to approximate Bayesian inference.\nDepending on how we wish to use the LHS conditional (which is called the posterior in Bayesian inference) - we have different options available to us.\nIf we wish to approximately sample from the posterior, to get an empirical sense of its shape and properties, we will often utilize techniques which provide exact samplers for another distribution which gets asymptotically close to the target posterior if we increase certain hyperparameters.\nOne such algorithm is importance sampling, and that’s what we’ll write today.\nHere’s importance sampling (with a single sample) without a custom proposal in GenJAX.\n\nobservations = genjax.choice_map(\n    {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n)\nmodel_args = (x,)\nkey, (w, tr) = model.importance(key, observations, model_args)\n\nWe’re introduced to another interface method!\nimportance accepts a PRNG key, a choice map representing observations (sometimes called constraints), and model arguments. It returns a new evolved PRNG key, and a tuple contained a log importance weight and a trace.\nThe trace is consistent with the arguments and constraints passed into the invocation.\n\nobservations[\"ys\", \"y\", \"value\"]\n\narray([ 2.19,  3.39, 50.  ,  6.26, 11.39, 12.75, 16.5 , 25.5 , 36.5 ])\n\n\n\n\ntr[\"ys\", \"y\", \"value\"]\n\nArray([ 2.19,  3.39, 50.  ,  6.26, 11.39, 12.75, 16.5 , 25.5 , 36.5 ],      dtype=float32)\n\n\n\nLet’s examine the weight now, and compare it to the score.\n\n(w, tr.get_score())\n\n(Array(-10115.872, dtype=float32), Array(-10121.649, dtype=float32))\n\n\n\nNotice that these two quantities are different.\nRemember: the score is the normalized log density of the choice map measure evaluated at the complete set of trace constraints. We’ll refer to complete traces by \\(\\tau\\).\nThe log importance weight w is slightly different.\n\nImportance sampling, informally\nLet’s discuss how importance sampling works first7.7 In this notebook, I’ll defer discussing formal proofs concerning the asymptotic consistence of posterior estimators derived from importance sampling.\nThis will provide us with an understanding as to why w is different from the score.\nMore importantly, we’ll understand how we can use importance to solve the inference task of approximately sampling from the posterior over coefficients (and ultimately, over curves) from our generative function.\n\n\n\n\n\n\nImportance sampling is typically presented by focusing on posterior expectations \\(E_{x \\sim P(x | y)}[f(x)]\\).\nIn our case, we want to sample \\(x \\sim P(x | y)\\). To do this, we’ll actually be considering a different procedure called sampling importance resampling or SIR for short.\nImportantly, importance sampling is a subroutine in SIR.\nWe’ll discuss why importance sampling works here, and provide references to why SIR works to solve our problem.\n\n\n\nLet’s start by considering two distributions which we can sample from, and evaluate densities.\nBelow, I’m plotting the densities of two distributions - a 1D Gaussian mixture and a 1D Gaussian8.8 GenJAX allows usage of TensorFlow Distributions as generative functions. Here, we’re just using the logpdf interface from distributions which expose exact logpdf evaluation - but genjax exports a wrapper which implements the complete generative function interface.\n\nmix = genjax.TFPMixture(\n    genjax.TFPCategorical, [genjax.TFPNormal, genjax.TFPNormal]\n)\nmix_args = ([0.5, 0.5], [(-3.0, 0.8), (1.0, 0.3)])\nd = genjax.TFPNormal\nd_args = (0.0, 1.0)\n\n\nfig, ax = plt.subplots(figsize=(8, 8), dpi=280)\nevaluation_points = np.arange(-5, 5, 0.01)\n\n\ndef plot_logpdf(ax, logpdf_fn, evaluation_points, **kwargs):\n    logpdfs = jax.vmap(logpdf_fn)(evaluation_points)\n    ax.scatter(evaluation_points, jnp.exp(logpdfs), marker=\".\", **kwargs)\n\n\nd_logpdf = lambda v: d.logpdf(v, *d_args)\nmix_logpdf = lambda v: mix.logpdf(v, *mix_args)\n\nplot_logpdf(\n    ax, d_logpdf, evaluation_points, color=\"red\", label=\"1D Gaussian PDF\"\n)\nplot_logpdf(\n    ax,\n    mix_logpdf,\n    evaluation_points,\n    color=\"blue\",\n    label=\"1D Gaussian mixture PDF\",\n)\nax.legend()\n\n\n\n\nTo gain context on importance sampling, imagine that the distribution which produces the blue curve is difficult to sample from - but it exposes a logpdf interface which we can use to evaluate the density at any point on the support of the distribution.\nNow, suppose you hand me the distribution which made the red curve - and it is easy to sample from, and it also exposes a logpdf interface.\nOne thing we could do is sample from the red curve and then “correct” for the fact that we’re sampling from the wrong distribution.\nThis is the key intuition behind importance sampling.\nNow, I’m going to write a procedure and ask you to just go with it … for a moment.\n\ndef importance_sample(hard, easy):\n    def _inner(key, hard_args, easy_args):\n        key, sub_key = jax.random.split(key)\n        sample = easy.sample(sub_key, *easy_args)\n        easy_logpdf = easy.logpdf(sample, *easy_args)\n        hard_logpdf = hard.logpdf(sample, *hard_args)\n        importance_weight = hard_logpdf - easy_logpdf\n        return key, (importance_weight, sample)\n\n    return _inner\n\n\nhard = genjax.TFPMixture(\n    genjax.TFPCategorical, [genjax.TFPNormal, genjax.TFPNormal]\n)\neasy = genjax.TFPNormal\njitted = jax.jit(importance_sample(hard, easy))\nkey, (importance_weight, sample) = jitted(key, mix_args, d_args)\n\n\n(importance_weight, sample)\n\n(Array(-4.0058784, dtype=float32), Array(-0.78240925, dtype=float32))\n\n\n\nNow, we can easily run this procedure many times in parallel.\n\njitted = jax.jit(\n    jax.vmap(importance_sample(hard, easy), in_axes=(0, None, None))\n)\n\n\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n_, (importance_weight, sample) = jitted(sub_keys, mix_args, d_args)\n\n\nimportance_weight\n\nArray([-6.2341752e+00,  6.2633801e-01, -3.3759670e+00, -5.9383879e+00,\n       -9.7561479e-01, -5.3102112e+00, -5.1128283e+00, -2.1514070e+00,\n        4.5841384e-01, -4.0724092e+00, -1.8354319e+00, -5.5122299e+00,\n       -6.2061067e+00, -2.1719730e-01, -3.5693016e+00, -6.1405115e+00,\n       -5.3810544e+00, -6.2284150e+00,  7.8095734e-01, -3.8515360e+00,\n       -2.8184617e-01, -1.8381605e+00, -7.5211000e+00,  1.0577252e+00,\n        1.0453278e+00,  1.0218762e+00,  9.8658347e-01,  9.5912194e-01,\n       -1.9758319e+00,  1.0580635e+00, -4.1381865e+00, -2.3692460e+00,\n        1.0351460e+00, -2.5306203e+00,  6.6269362e-01,  6.0733199e-02,\n       -1.1496689e+00, -6.2260799e+00, -6.1138239e+00,  4.6291578e-01,\n       -4.1340222e+00,  8.6949825e-01, -2.1810038e+00, -2.9703891e+00,\n       -7.1519613e-03, -1.2844566e+00, -2.3207369e+00, -2.3978152e+00,\n       -1.0548264e+00, -5.6061153e+00, -5.9159632e+00, -3.1040215e-01,\n       -1.5413404e-01, -1.2319851e-01, -1.4652884e+00, -5.0214028e-01,\n        6.9174856e-01, -5.8656144e+00, -5.3170371e+00,  6.0201246e-01,\n       -1.6653061e-02, -3.0496669e+00, -2.3092539e+00, -2.4331427e+00,\n        1.0071430e+00, -5.6184292e+00, -1.1847696e+00, -3.5587354e+00,\n       -5.8828015e+00, -3.7577522e+00, -3.6165695e+00, -5.5585594e+00,\n       -1.7351907e+00, -6.7679203e-01,  1.9070494e-01, -3.9766405e+00,\n        5.7461774e-01, -1.0918140e-02, -2.6561151e+00,  4.9570715e-01,\n        6.3730204e-01,  9.7846591e-01, -3.9247353e+00,  1.0440333e+00,\n       -6.2158747e+00, -2.4731362e+00, -4.5478611e+00,  1.0888841e+00,\n       -7.3868978e-01, -4.2448354e+00,  9.5782554e-01,  3.7766671e-01,\n       -6.2152562e+00, -5.2021756e+00, -5.1431012e+00, -1.7538964e+00,\n       -4.6911263e+00, -7.5708902e-01, -4.1147037e+00,  9.9778765e-01],      dtype=float32)\n\n\n\nWe’re just sampling from easy, then scoring the samples with importance_weight according to the log ratio easy_logpdf(sample) - hard_logpdf(sample).\nHere’s the trick - from our collection of samples and weights, let’s normalize the weights into a distribution and sample a single sample to return using it.\n\ndef sampling_importance_resampling(hard, easy, n_samples):\n    def _inner(key, hard_args, easy_args):\n        fn = importance_sample(hard, easy)\n        key, *sub_keys = jax.random.split(key, n_samples + 1)\n        sub_keys = jnp.array(sub_keys)\n        vmapped = jax.vmap(fn, in_axes=(0, None, None))\n        _, (ws, samples) = vmapped(sub_keys, hard_args, easy_args)\n        logits = ws\n        key, sub_key = jax.random.split(key)\n        index = genjax.TFPCategorical.sample(sub_key, logits)\n        final_sample = samples[index]\n        return key, final_sample\n\n    return _inner\n\n\nhard = genjax.TFPMixture(\n    genjax.TFPCategorical, [genjax.TFPNormal, genjax.TFPNormal]\n)\neasy = genjax.TFPNormal\njitted = jax.jit(sampling_importance_resampling(hard, easy, 100))\nkey, sample = jitted(key, mix_args, d_args)\n\n\nsample\n\nArray(-2.556716, dtype=float32)\n\n\n\nLet’s run this procedure a bunch of times and plot the points on the x-axis of our plot above.\n\ndef plot_on_x(ax, x, **kwargs):\n    ax.scatter(x, np.zeros_like(x), **kwargs)\n\n\nkey, *sub_keys = jax.random.split(key, 1000 + 1)\nsub_keys = jnp.array(sub_keys)\nfn = sampling_importance_resampling(hard, easy, 1000)\njitted = jax.jit(jax.vmap(fn, in_axes=(0, None, None)))\n_, samples = jitted(sub_keys, mix_args, d_args)\nplot_on_x(ax, samples, color=\"gold\", marker=\".\", alpha=0.05)\nfig\n\n\n\n\nNotice what happens with the SIR samples (in gold)?\nThey accumulate around the places you’d expect to see if you were sampling from the hard distribution!\nThat’s what importance sampling and sampling importance sampling give us - we provide a “hard” distribution with a logpdf interface, and another “easy” distribution with sample and logpdf interface9, and SIR returns an exact sampler for a distribution which approximates the hard distribution.9 There are more constraints. The second distribution must be absolutely continuous in measure with respect to the first. Let’s defer this discussion to a formal treatment of importance sampling.\n\n\nBack to our generative function\nNow that we’ve seen the ingredients and implementation of importance sampling and sampling importance resampling - let’s return to our original problem.\n\nfig_data\n\n\n\n\nIf you studied the previous section careful - one question might jump out at you: what is the “easy” distribution for model.importance?\n\nBuiltin proposals\nGenerative functions defined using the BuiltinGenerativeFunction language come with builtin proposals - it’s a distribution (which we’ll refer to as \\(Q\\)) induced from the prior, with sampling and score defined ancestrally.\nGive observation constraints \\(u\\), the importance weight which model.importance computes is10:10 This definition again considers “untraced randomness” \\(r\\). If you wish to ignore this in the math, just remove the \\(Q(r; x, \\tau)\\) term. Even in the presence of untraced randomness, the weights which Gen computes are asymptotically consistent in expectation over \\(Q(r; x, \\tau)\\)\n\\[\n\\begin{align}\n\\log w &= \\log P(\\tau, r; x) - \\log Q(\\tau; u, x)Q(r; x, \\tau) \\\\\n\\end{align}\n\\]\nFor the BuiltinGenerativeFunction language, we implement \\(Q\\) by invoking the generative function - when we arrive at a constrained address, we recursively called submodel.importance - accumulate the log weight, as well as the log score.\nNow, if an address has no constraints - we get 0.0 for the weight (think about why this is by looking at the above equation and asking what happens when \\(Q\\) has to generate a full \\(\\tau\\)). However, we still get a score.\n\n\nSequential importance resampling in GenJAX\nHere’s SIR using builtin proposals (just a single call to model.importance) in GenJAX11:11 To implement a variant with custom proposals, all we need to do is first proposal.simulate, merge the proposal choice map with the constraints, then model.importance followed by a final weight adjustment w = w - proposal_tr.get_score() - easy peasy.\n\ndef sampling_importance_resampling(model, n_samples):\n    def _inner(key, observations, model_args):\n        key, *sub_keys = jax.random.split(key, n_samples + 1)\n        sub_keys = jnp.array(sub_keys)\n        vmapped = jax.vmap(model.importance, in_axes=(0, None, None))\n        _, (lws, trs) = vmapped(sub_keys, observations, model_args)\n        key, sub_key = jax.random.split(key)\n        index = genjax.TFPCategorical.sample(sub_key, lws)\n        final_tr = jtu.tree_map(lambda v: v[index], trs)\n        return key, final_tr\n\n    return _inner\n\nOne difference between our first implementation (on just distributions) above and this one is that Trace instances are structured objects (but all of them are Pytree implementors) - meaning we need to index into the leaves when we wish to return a single sampled trace.\n\nmodel_args = (x,)\njitted = jax.jit(\n    jax.vmap(\n        sampling_importance_resampling(model, 100), in_axes=(0, None, None)\n    )\n)\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n_, samples = jitted(sub_keys, observations, model_args)\ncoefficients = samples[\"alpha\"]\n\nSo now we have an approximate sampler for the posterior and we can use it to look at properties of the posterior - like what sort of curves are likely given our data and our model prior.\n… and by the way, to get a representative set of samples from the posterior for this model, on an Apple M2 device - only takes about 0.05 seconds12.12 Just remember: we’re running this notebook on CPU - but the resulting specialized inference code can easily be moved to accelerators, courtesy of the fact that all our code is JAX traceable.\n\n%%timeit\n_, samples = jitted(sub_keys, observations, model_args)\n\n52.5 ms ± 9.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndef polynomial_at_x(x, coefficients):\n    basis_values = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(coefficients * basis_values)\n    return polynomial_value\n\n\njitted = jax.jit(jax.vmap(polynomial_at_x, in_axes=(None, 0)))\n\n\ndef plot_polynomial_values(ax, x, coefficients, **kwargs):\n    v = jitted(x, coefficients)\n    ax.scatter(np.repeat(x, len(v)), v, **kwargs)\n\n\ncoefficients = samples[\"alpha\"]\nevaluation_points = np.arange(0, 5, 0.01)\nfor data in evaluation_points:\n    plot_polynomial_values(\n        ax_data, data, coefficients, marker=\".\", color=\"gold\", alpha=0.005\n    )\nfig_data\n\n\n\n\nIntuitively, this makes a lot of sense. Our prior over polynomials considers a wide range of curves - but, if our approximate sampling process is trusted, we’re correctly seeing what we should expect to happen if we observed this data - polynomials with the coefficients shown above tend to be sampled more under the posterior.\nWe can also ask for an estimate of the posterior probability that any particle point was an outlier.\nFor example, below is the set of samples projected onto the (\"ys\", \"outlier\") address for the point which we manually set to be quite far from the curve.\n\nposterior_outlier = samples[\"ys\", \"outlier\"][:, 2]\nposterior_outlier\n\nArray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True], dtype=bool)\n\n\n\n\nnp.sum(posterior_outlier) / len(posterior_outlier)\n\nArray(0.99, dtype=float32)\n\n\n\nThat seems to make sense! We pulled that point quite far away from ground truth curve - so we’d expect that point 2 is considered an outlier under the true posterior."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#summary",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#summary",
    "title": "Introduction to Gen and GenJAX",
    "section": "Summary",
    "text": "Summary\nWe’ve covered a lot of ground in this notebook. Please reflect, re-read, and post issues!\n\nWe discussed the Gen probabilistic programming framework, and discussed GenJAX - an implementation of Gen on top of JAX.\nWe discussed generative functions - the main computational object of Gen.\nWe discussed how to create generative functions using generative function languages, and several of GenJAX’s builtin capabilities for constructing generative functions.\nWe discussed how to use generative functions to represent joint probability distributions, which can be used to construct models of phenomena.\nWe created a generative function to model a data-generating process based on sampling and evaluating random polynomials at input data - to represent a typical regression task.\nWe discussed how to formulate questions about induced conditional distributions under a probabilistic model as a Bayesian inference problem.\nWe discussed importance sampling and sampling importance resampling, two central techniques in approximate Bayesian inference.\nWe created a sampling importance resampling routine and applied it to produce approximate posterior samples from the posterior in the our polynomial generating model.\nWe investigated the approximate posterior samples, and visually inspected that they match the inferences that we might draw - both for the polynomials we expected to produce the data, as well as what data points might be outliers.\n\nThis is just the beginning! There’s a lot more to learn, but plenty to bite off with this initial notebook."
  },
  {
    "objectID": "intermediate/gaussian_processes/gaussian_processes.html",
    "href": "intermediate/gaussian_processes/gaussian_processes.html",
    "title": "Gaussian process modeling with tinygp",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import trace\nimport tinygp\nimport tinygp.kernels as kernels\n\nggp = genjax.tinygp()\n\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nGaussian process models are a well-explored nonparametric model class describing distributions over spaces of functions. They also support sampling, logpdf evaluation, as well as exact conditioning. In GenJAX, we support usage of Gaussian process models using an auxiliary library tinygp which provides a lightweight implementation of the interfaces above for many common Gaussian process kernels.\nTo construct a Gaussian process model which implements the generative function interface, it suffices to provide the constructor gpp.GaussianProcess with a tinygp kernel:\nThis model is a generative function - meaning it supports all the generative function interfaces.\nTo call simulate, we must provide a grid for evaluation:\nWe can visualize samples from our genjax.GaussianProcess - and of course, we can jax.jit our interfaces, as usual."
  },
  {
    "objectID": "intermediate/gaussian_processes/gaussian_processes.html#structured-modeling-and-inference-with-genjax",
    "href": "intermediate/gaussian_processes/gaussian_processes.html#structured-modeling-and-inference-with-genjax",
    "title": "Gaussian process modeling with tinygp",
    "section": "Structured modeling and inference with GenJAX",
    "text": "Structured modeling and inference with GenJAX\nLet’s create some data, and then consider constructing a model and performing inference. Below, we’ll consider a noisy sinusoidal data generating process.\n\nkey, sub_key = jax.random.split(key)\nn = 50\nf = lambda x: 10 * jnp.sin(x)\nx = jax.random.uniform(key=sub_key, minval=-3.0, maxval=3.0, shape=(n,)).sort()\nkey, sub_key = jax.random.split(key)\nground_truth = f(x) + jax.random.normal(sub_key, shape=(n,))\n\n\nf, ax = plt.subplots()\nax.scatter(x, ground_truth)\n\n\n\n\nTo showcase the power of combining tinygp with a more expressive language, let’s consider a switching model with two types of kernels. The first switch branch will include a prior over the parameters of the a kernels.ExpSquared kernel, and the second will provide a kernels.Cosine kernel. Following this path, we’ll showcase how we can use genjax.GaussianProcess models inside of larger generative functions.\n\n@genjax.gen\ndef model(data):\n    factor = trace(\"factor\", genjax.TFPUniform)(0.1, 3.0)\n    scale = trace(\"scale\", genjax.TFPUniform)(0.1, 5.0)\n    kernel_scaled = kernels.Cosine(scale=scale)\n    gp = ggp.GaussianProcess(kernel_scaled)\n    y = trace(\"y\", gp)(data)\n    return y\n\n\ndef viz(ax, x, y, **kwargs):\n    ax.plot(x, y, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (x,))\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nWe can easily use sampling importance resampling here, let’s look at the results.\n\nobservations = genjax.choice_map({\"y\": ground_truth})\ninf = genjax.sampling_importance_resampling(1000, model)\ninf\n\n\n\n\nSamplingImportanceResampling\n├── num_particles\n│   └── (const) 1000\n├── model\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function model>\n└── proposal\n    └── (const) None\n\n\n\n\njitted = jax.jit(model.simulate)\nkey, tr = jitted(key, (x,))\ntr.get_retval()\n\nArray([-0.43208396, -0.58294547, -0.848159  , -0.7861588 , -0.545494  ,\n       -0.16411299,  0.14950526,  0.2080805 ,  0.8111007 ,  0.86555636,\n        0.8450676 ,  0.6299165 ,  0.32383266, -0.01688776, -0.30093664,\n       -0.5840381 , -0.6380779 , -0.81765443, -0.7392957 , -0.6654223 ,\n       -0.54848593, -0.21036375,  0.8061874 ,  0.82833725,  0.83593285,\n       -0.02110198, -0.499939  , -0.5576827 , -0.81426835, -0.39774978,\n        0.36000574,  0.62700665,  0.6447593 ,  0.83770037,  0.7923152 ,\n        0.26099455, -0.24413885, -0.23712076, -0.38988945, -0.56829345,\n       -0.5653826 , -0.84069645, -0.81233406, -0.6983161 , -0.59467906,\n       -0.02127809,  0.23496075,  0.2594615 ,  0.36821437,  0.7813539 ],      dtype=float32)\n\n\n\n\njitted = jax.jit(jax.vmap(model.simulate, in_axes=(0, None)))\nkey, sub_keys = genjax.slash(key, 100)\n_, tr = jitted(sub_keys, (x,))\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_23406/2246199438.py:3 in <module>     │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/mk/btkplz1n40q001dsy957srbh0000gn/T/ipykernel_23406/2246199438.py'                 │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/traceback_util.py │\n│ :162 in reraise_with_filtered_traceback                                                          │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/api.py:622 in     │\n│ cache_miss                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/dispatch.py:241   │\n│ in _xla_call_impl_lazy                                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/linear_util.py:303 in  │\n│ memoized_fun                                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/dispatch.py:357   │\n│ in _xla_callable_uncached                                                                        │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/dispatch.py:348   │\n│ in sharded_lowering                                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/profiler.py:314   │\n│ in wrapper                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/interpreters/pxla.py:2 │\n│ 792 in lower_sharding_computation                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/profiler.py:314   │\n│ in wrapper                                                                                       │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/interpreters/partial_e │\n│ val.py:2065 in trace_to_jaxpr_final                                                              │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/interpreters/partial_e │\n│ val.py:1998 in trace_to_subjaxpr_dynamic                                                         │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/linear_util.py:167 in  │\n│ call_wrapped                                                                                     │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/traceback_util.py │\n│ :162 in reraise_with_filtered_traceback                                                          │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/api.py:1685 in    │\n│ vmap_f                                                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/tree_util.py:75   │\n│ in tree_unflatten                                                                                │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/tinygp/helpers.py:65 in    │\n│ clz_from_iterable                                                                                │\n│                                                                                                  │\n│   62 │   │   meta_args = tuple(zip(meta_fields, meta))                                           │\n│   63 │   │   data_args = tuple(zip(data_fields, data))                                           │\n│   64 │   │   kwargs = dict(meta_args + data_args)                                                │\n│ ❱ 65 │   │   return data_clz(**kwargs)                                                           │\n│   66 │                                                                                           │\n│   67 │   jax.tree_util.register_pytree_node(                                                     │\n│   68 │   │   data_clz, iterate_clz, clz_from_iterable                                            │\n│ <string>:5 in __init__                                                                           │\n│                                                                                                  │\n│ /Users/mccoybecker/miniconda3/envs/py311/lib/python3.11/site-packages/tinygp/kernels/stationary. │\n│ py:63 in __post_init__                                                                           │\n│                                                                                                  │\n│    60 │                                                                                          │\n│    61 │   def __post_init__(self) -> None:                                                       │\n│    62 │   │   if jnp.ndim(self.scale):                                                           │\n│ ❱  63 │   │   │   raise ValueError(                                                              │\n│    64 │   │   │   │   \"Only scalar scales are permitted for stationary kernels; use\"             │\n│    65 │   │   │   │   \"transforms.Linear or transforms.Cholesky for more flexiblity\"             │\n│    66 │   │   │   )                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Only scalar scales are permitted for stationary kernels; usetransforms.Linear or transforms.Cholesky \nfor more flexiblity\n\n\n\n\nscale = tr[\"scale\"]\nfactor = tr[\"factor\"]\nkernel_scaled = factor * kernels.ExpSquared(scale=scale)\ngp = ggp.GaussianProcess(kernel_scaled)\nv = gp.sample(key, x)\nf, ax = plt.subplots()\nax.scatter(x, v)"
  },
  {
    "objectID": "intermediate/incremental_computation/incremental.html",
    "href": "intermediate/incremental_computation/incremental.html",
    "title": "Incremental computation for generative functions",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nThe update interface method for generative functions defines an update operation on traces produced by generative functions. update allows the user to provide new constraints, as well as new arguments, and returns an updated trace which is consistent with the new constraints, as well as an incremental importance weight. update is used to implement many types of MCMC inference."
  },
  {
    "objectID": "intermediate/incremental_computation/incremental.html#what-is-update-used-for",
    "href": "intermediate/incremental_computation/incremental.html#what-is-update-used-for",
    "title": "Incremental computation for generative functions",
    "section": "What is update used for?",
    "text": "What is update used for?\nBefore we discuss how update can be optimized, it’s worth constructing a simple example to show why optimizing update is worthwhile, especially when repeatedly apply MCMC kernels during an inference process."
  },
  {
    "objectID": "intermediate/gradient_interfaces/gradient_interfaces.html",
    "href": "intermediate/gradient_interfaces/gradient_interfaces.html",
    "title": "Differentiable programming with GenJAX",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport dataclasses\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import trace, slash, TFPUniform, Normal\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nThe generative function interface exposes functionality which allows usage of generative functions for differentiable programming. These interfaces are designed to work seamlessly with jax.grad - allowing (even higher-order) gradient computations which are useful for inference algorithms which require gradients and gradient estimators. In this notebook, we’ll describe some of these interfaces - as well as their (current, but not forever) limitations. We’ll walk through an implementation of MAP estimation, as well as the Metropolis-adjusted Langevin algorithm (MALA) using these interfaces."
  },
  {
    "objectID": "intermediate/gradient_interfaces/gradient_interfaces.html#gradient-interfaces",
    "href": "intermediate/gradient_interfaces/gradient_interfaces.html#gradient-interfaces",
    "title": "Differentiable programming with GenJAX",
    "section": "Gradient interfaces",
    "text": "Gradient interfaces\nBecause JAX features best-in-class support for higher-order AD, GenJAX exposes interfaces that compose natively with JAX’s interfaces for gradients. The primary interface method which provides jax.grad-compatible functions from generative functions is an interface called unzip.\nunzip allows a user to provide a key, and a fixed choice map - and it returns a new key and two closures:\n\nThe first closure is a “score” closure which accepts a choice map as the first argument, and arguments which match the non-PRNGKey signature types of the generative function. The score closure returns the exact joint score of the generative function. It computes the exact joint score using an interface called assess.1\nThe second closure is a “retval” closure which accepts a choice map as the first argument, and arguments which match the non-PRNGKey signature types of the generative function. The retval closure executes the generative function constrained using the union of the fixed choice map, and the user provided choice map, and returns the return value of the execution. Here, the return value is also provided by invoking the assess interface.\n\n1 Caveat: assess is not required to return the exact joint score, only an estimate. However, if jax.grad is used on estimates - the resulting thing is not a correct gradient estimator. See the important callout below!So really, unzip is syntactic sugar over another interface called assess.\n\nassess for exact density evaluation\nassess is a generative function interface method which computes log joint density estimates from generative functions. assess requires that a user provide a choice map which completely fills all choices encountered during execution. Otherwise, it errors.22 And these errors are thrown at JAX trace time, so you’ll get an exception before runtime.\nIf a generative function also draws from untraced randomness - assess computes an estimate whose expectation over the distribution of untraced randomness gives the correct log joint density.\n\n\n\n\n\n\nCorrectness of gradient estimators\n\n\n\nWhen used on generative functions which include untraced randomness, naively applying jax.grad to the closures returned by interfaces described in this notebook do not compute gradient estimators which are unbiased with respect to the true gradients.\nShort: don’t use these with untraced randomness. We’re working on alternatives."
  },
  {
    "objectID": "intermediate/gradient_interfaces/gradient_interfaces.html#a-running-example",
    "href": "intermediate/gradient_interfaces/gradient_interfaces.html#a-running-example",
    "title": "Differentiable programming with GenJAX",
    "section": "A running example",
    "text": "A running example\nLet’s consider the following model, which we’ll cover in different variations.\n\n# If you don't specify broadcast `in_axes`, you\n# should specify number of IID samples via `repeats`.\n@genjax.gen(genjax.Map, repeats=100)\ndef sample_x(x_mu, var):\n    position = trace(\"pos\", Normal)(x_mu, 1.0)\n    return position\n\n\n@genjax.gen(genjax.Map, in_axes=(0, None, None))\ndef sample_y(x, a, b):\n    position = trace(\"pos\", Normal)(a * x + b, 1.0)\n    return position\n\n\n@genjax.gen\ndef model():\n    x_mu = trace(\"x_mu\", TFPUniform)(-3.0, 3.0)\n    a = trace(\"a\", TFPUniform)(-4.0, 4.0)\n    b = trace(\"b\", TFPUniform)(-3.0, 3.0)\n    x = trace(\"x\", sample_x)(x_mu, 1.0)\n    y = trace(\"y\", sample_y)(x, a, b)\n    return y\n\nAnd, most importantly, visualizations.\n\ndef viz(ax, x, y, **kwargs):\n    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\ntrs = []\nfor ax in axes.flatten():\n    key, tr = jitted(key, ())\n    x = tr[\"x\", \"pos\"]\n    y = tr[\"y\", \"pos\"]\n    trs.append(tr)\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nA nice diffuse prior over points."
  },
  {
    "objectID": "intermediate/gradient_interfaces/gradient_interfaces.html#map-estimation",
    "href": "intermediate/gradient_interfaces/gradient_interfaces.html#map-estimation",
    "title": "Differentiable programming with GenJAX",
    "section": "MAP estimation",
    "text": "MAP estimation\nWhen it comes to looking at the interfaces, a good first step is gradient-based maximum a posteriori probability (MAP) estimation. Let’s write this using the lowest level interface unzip first:\nNow, often we may have a trace in hand, and we just want the first-order gradient with respect to certain random choices (specified by a genjax.Selection). This is a relatively common occurrence - so there’s a higher-level API choice_grad which gives us exactly this thing.3 Here’s MapUpdate using choice_grad.3 It’s not compositional with jax.grad - but if we need that power, we can just drop back down to use unzip.\n\n@dataclasses.dataclass\nclass MapUpdate(genjax.Pytree):\n    selection: genjax.Selection\n    tau: genjax.FloatArray\n\n    def flatten(self):\n        return (self.tau,), (self.selection,)\n\n    def apply(self, key, trace):\n        args = trace.get_args()\n        gen_fn = trace.get_gen_fn()\n        key, forward_gradient_trie = gen_fn.choice_grad(\n            key, trace, self.selection\n        )\n        forward_values, _ = self.selection.filter(trace)\n        forward_values = forward_values.strip()\n        forward_values = jtu.tree_map(\n            lambda v1, v2: v1 + self.tau * v2,\n            forward_values,\n            forward_gradient_trie,\n        )\n        argdiffs = tuple(map(genjax.Diff.no_change, args))\n        key, (_, _, new_trace, _) = gen_fn.update(\n            key, trace, forward_values, argdiffs\n        )\n        return key, (new_trace, True)\n\n    def __call__(self, key, trace):\n        return self.apply(key, trace)\n\n\nmap_update = MapUpdate\n\nSimple, concise - works with any generative function whose choices specified by MapUpdate.selection support gradients on the joint logpdf.\nFrom the Pytree interface, any instance of MapUpdate has a static selection, and a tau (which determines the gradient step size) which can be dynamic.44 If this is your first time seeing the Pytree interface, note that it’s defined by the flatten interface - which allows us to specify runtime vs. trace time data in Pytree structures.\nBecause MapUpdate is a Pytree, in inference code, we’d just construct MapUpdate before calling it - and we can do this on either side of the JAX API boundary.55 E.g. outside of a jax.jit transform, inside - it’s all okay.\n\nupdate = map_update(genjax.select([\"x_mu\", \"a\", \"b\"]), 1e-4)\nupdate\n\n\n\n\nMapUpdate\n├── selection\n│   └── BuiltinSelection\n│       └── trie\n│           └── Trie\n│               ├── :x_mu\n│               │   └── AllSelection\n│               ├── :a\n│               │   └── AllSelection\n│               └── :b\n│                   └── AllSelection\n└── tau\n    └── (const) 0.0001\n\n\n\nLet’s take a sampled piece of data, extract the (\"x\", \"pos\") and (\"y\", \"pos\") addresses, and then use MAP optimization to estimate the mode of the posterior.\n\ntr = trs[2]\nselection = genjax.select([\"x\", \"y\"])\nchm, _ = selection.filter(tr.strip())\nchm\n\n\n\n\nBuiltinChoiceMap\n└── trie\n    └── Trie\n        ├── :x\n        │   └── VectorChoiceMap\n        │       ├── indices\n        │       │   └──  i32[100]\n        │       └── inner\n        │           └── BuiltinChoiceMap\n        │               └── trie\n        │                   └── Trie\n        │                       └── :pos\n        │                           └── ValueChoiceMap\n        │                               └── value\n        │                                   └──  f32[100]\n        └── :y\n            └── VectorChoiceMap\n                ├── indices\n                │   └──  i32[100]\n                └── inner\n                    └── BuiltinChoiceMap\n                        └── trie\n                            └── Trie\n                                └── :pos\n                                    └── ValueChoiceMap\n                                        └── value\n                                            └──  f32[100]\n\n\n\n\nx = chm[\"x\", \"pos\"]\ny = chm[\"y\", \"pos\"]\nfig_data, ax_data = plt.subplots(figsize=(3, 3), dpi=140)\nviz(ax_data, x, y, marker=\".\")\n\n\n\n\nIf we apply MapUpdate, we take a single optimization step:\n\nkey, (_, tr) = jax.jit(model.importance)(key, chm, ())\nkey, (tr, _) = update(key, tr)\n\nWe can use scan to apply MapUpdate repeatedly.\n\ndef chain(key, tr):\n    def _inner(carry, _):\n        key, tr = carry\n        key, (tr, _) = update(key, tr)\n        return (key, tr), ()\n\n    (key, tr), _ = jax.lax.scan(_inner, (key, tr), None, length=2000)\n    return key, tr\n\n\njitted = jax.jit(chain)\nkey, tr = jitted(key, tr)\n\nNow, we can plot the polynomial described by \"a\" and \"b\", with evaluation points generated around the estimated \"x_mu\".\n\ndef polynomial_at_x(x, coefficients):\n    basis_values = jnp.array([1.0, x])\n    polynomial_value = jnp.sum(coefficients * basis_values)\n    return polynomial_value\n\n\njitted = jax.jit(jax.vmap(polynomial_at_x, in_axes=(0, None)))\n\n\ndef plot_polynomial_values(ax, x, coefficients, **kwargs):\n    v = jitted(x, coefficients)\n    ax.scatter(x, v, **kwargs)\n\n\na = tr[\"a\"]\nb = tr[\"b\"]\nx_mu = tr[\"x_mu\"]\nkey, sub_key = jax.random.split(key)\nevaluation_points = x_mu + jax.random.normal(sub_key, shape=(1000,))\ncoefficients = jnp.array([b, a])\nplot_polynomial_values(\n    ax_data,\n    evaluation_points,\n    coefficients,\n    marker=\".\",\n    color=\"gold\",\n    alpha=0.05,\n)\nfig_data"
  },
  {
    "objectID": "intermediate/gradient_interfaces/gradient_interfaces.html#exposing-learnable-modules-with-traincombinator",
    "href": "intermediate/gradient_interfaces/gradient_interfaces.html#exposing-learnable-modules-with-traincombinator",
    "title": "Differentiable programming with GenJAX",
    "section": "Exposing learnable modules with TrainCombinator",
    "text": "Exposing learnable modules with TrainCombinator\nFor learning and variational inference, learnable parameters of model families are an important feature. In GenJAX, we expose a lightweight generative function wrapper around generative functions which accept learnable_params as a last argument - this wrapper is called TrainCombinator.\n\nparams = {\"x_mu\": 0.0, \"a\": 0.3, \"b\": 0.4}\n\n\n@genjax.gen(genjax.TrainCombinator, params=params)\ndef model(params):\n    x_mu = params[\"x_mu\"]\n    a = params[\"a\"]\n    b = params[\"b\"]\n    x = genjax.trace(\"x\", genjax.Normal)(x_mu, 1.0)\n    return genjax.trace(\"y\", genjax.Normal)(a * x + b, 1.0)\n\n\nmodel\n\n\n\n\nTrainCombinator\n├── inner\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function model>\n└── params\n    └── dict\n        ├── x_mu\n        │   └── (const) 0.0\n        ├── a\n        │   └── (const) 0.3\n        └── b\n            └── (const) 0.4\n\n\n\nTrainCombinator is a module-like abstraction which closes over the parameter store passed in and initialized by the constructor. When we call TrainCombinator, we don’t need to provide the params argument - it’s handled automatically.\n\nkey, tr = model.simulate(key, ())\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function model>\n├── args\n│   └── tuple\n│       └── dict\n│           ├── x_mu\n│           │   └── (const) 0.0\n│           ├── a\n│           │   └── (const) 0.3\n│           └── b\n│               └── (const) 0.4\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :x\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├── (const) 0.0\n│       │       │       └── (const) 1.0\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :y\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └── (const) 1.0\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nTrainCombinator exposes a convenient interface to a specialized scoring function which accepts params evaluation points, and returns the model logpdf.66 Note: because this interface method returns a function, we cannot JIT it. However, if we use it to produce a closure and then use that closure inside of code which we JIT, it’s fine. Producing this closure is also speedy! As long as an object of function type doesn’t try to escape across the JAX API boundary.\n\nkey, logpdf = model.score_params(key, tr, params)\nlogpdf\n\nArray(-4.3452287, dtype=float32)\n\n\n\nThis interface supports batching out of the box.\n\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\nsub_keys, tr = jax.vmap(model.simulate)(sub_keys, ())\n_, logpdf = jax.vmap(model.score_params, in_axes=(0, 0, None))(\n    sub_keys, tr, params\n)\nlogpdf\n\nArray([-5.4506683, -2.1061187, -3.6554127, -2.5496364, -2.7137356,\n       -1.9265054, -4.279438 , -2.3579683, -2.0518727, -4.388969 ,\n       -3.1821628, -2.3114629, -2.108521 , -2.920351 , -4.843571 ,\n       -1.8675828, -2.4382515, -2.9003625, -2.0216916, -1.9461784,\n       -2.226705 , -2.516504 , -2.8950243, -3.6285126, -6.1273694,\n       -2.01638  , -3.3069491, -2.64029  , -2.1603744, -5.427867 ,\n       -3.6290247, -3.2877336, -2.3961902, -1.9439471, -2.354165 ,\n       -2.4933012, -2.864241 , -2.6146803, -2.944218 , -3.137638 ,\n       -2.1196494, -1.8954076, -3.7771382, -1.8947375, -2.0753593,\n       -1.8903599, -2.4286273, -2.4546478, -2.0735593, -2.4206872,\n       -3.0394769, -2.1118817, -3.3210392, -4.462427 , -4.5457187,\n       -2.8484173, -5.670154 , -2.0883737, -1.8393419, -3.262835 ,\n       -2.2887907, -2.197598 , -2.3469677, -1.886466 , -2.6559672,\n       -2.7177358, -2.5440192, -3.2496984, -3.4575148, -2.179047 ,\n       -5.556399 , -2.0065482, -2.4721122, -2.574447 , -3.0421057,\n       -1.8470458, -2.0921993, -2.4571283, -2.5315878, -1.8850044,\n       -2.0099494, -2.3851435, -5.7253923, -2.3849173, -3.3000984,\n       -2.80287  , -2.2776237, -2.5407119, -1.9084063, -1.8877966,\n       -2.1774974, -2.0100193, -3.6192372, -6.0826535, -4.6003428,\n       -1.9519615, -1.850507 , -3.087071 , -2.8032985, -2.8084445],      dtype=float32)\n\n\n\nWe make extensive use of batch evaluation in variational inference. For now, let’s consider maximum likelihood learning and see the other TrainCombinator interfaces."
  },
  {
    "objectID": "intermediate/gradient_interfaces/gradient_interfaces.html#automatic-differentiation-variational-inference",
    "href": "intermediate/gradient_interfaces/gradient_interfaces.html#automatic-differentiation-variational-inference",
    "title": "Differentiable programming with GenJAX",
    "section": "Automatic differentiation variational inference",
    "text": "Automatic differentiation variational inference\nIn this section, we’ll show how we can use the gradient interfaces to implement Automatic differentiation variational inference."
  },
  {
    "objectID": "intermediate/intro_to_combinators/intro_to_combinators.html",
    "href": "intermediate/intro_to_combinators/intro_to_combinators.html",
    "title": "Introduction to generative function combinators",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=80)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)\nGen helps probabilistic programmers design and implement models and inference algorithms by automating the (often) complicated inference math. The generative function interface is the key abstraction layer which provides this automation. Generative function language designers can extend the interface to new generative function objects - providing domain-specific patterns and optimizations which users can automatically take advantage of.\nOne key class of generative function languages are combinators - higher-order functions which accept generative functions as input, and produce a new generative function type as an output.\nCombinators functionally transform the generative structure that we pass into them, expressing useful patterns - including chain-like computations, IID sampling patterns, or generative computations which form grammar-like structures.\nCombinators also expose optimization opportunities - by registering the patterns as generative functions, implementors (e.g. the library authors) can specialize the implementation of the generative function interface methods. Users of combinators can then take advantage of this interface specialization to express asymptotically optimal updates (useful in e.g. MCMC kernels), or optimized importance weight calculations.\nIn this notebook, we’ll be discussing Unfold - a combinator for expressing generative computations which are reminiscent of state-space (or Markov) models. To keep things simple, we’ll explore a hidden Markov model example - but combinator usage generalizes to models with much richer structure."
  },
  {
    "objectID": "intermediate/intro_to_combinators/intro_to_combinators.html#introducing-unfold",
    "href": "intermediate/intro_to_combinators/intro_to_combinators.html#introducing-unfold",
    "title": "Introduction to generative function combinators",
    "section": "Introducing Unfold",
    "text": "Introducing Unfold\nLet’s discuss Unfold.11 A quick reminder: when in doubt, you can use the console from console = genjax.pretty() to inspect the classes which we discuss in the notebooks.\n\nconsole.help(genjax.UnfoldCombinator)\n\n╭─ <class 'genjax._src.generative_functions.combinators.unfold.UnfoldCombinato─╮\n│ class UnfoldCombinator(kernel: genjax._src.core.datatypes.GenerativeFunction │\n│                                                                              │\n│ :code:`UnfoldCombinator` accepts a single kernel generative function         │\n│ as input and a static unroll length which specifies how many iterations      │\n│ to run the chain for.                                                        │\n│                                                                              │\n│ A kernel generative function is one which accepts and returns                │\n│ the same signature of arguments. Under the hood, :code:`UnfoldCombinator`    │\n│ is implemented using :code:`jax.lax.scan` - which has the same               │\n│ requirements.                                                                │\n│                                                                              │\n│ Parameters                                                                   │\n│ ----------                                                                   │\n│                                                                              │\n│ gen_fn: :code:`GenerativeFunction`                                           │\n│     A single *kernel* `GenerativeFunction` instance.                         │\n│                                                                              │\n│ length: :code:`Int`                                                          │\n│     An integer specifying the unroll length of the chain of applications.    │\n│                                                                              │\n│ Returns                                                                      │\n│ -------                                                                      │\n│ :code:`UnfoldCombinator`                                                     │\n│     A single :code:`UnfoldCombinator` generative function which              │\n│     implements the generative function interface using a scan-like           │\n│     pattern. This generative function will perform a dependent-for           │\n│     iteration (passing the return value of generative function application)  │\n│     to the next iteration for :code:`length` number of steps.                │\n│     The programmer must provide an initial value to start the chain of       │\n│     iterations off.                                                          │\n│                                                                              │\n│ Example                                                                      │\n│ -------                                                                      │\n│                                                                              │\n│ .. jupyter-execute::                                                         │\n│                                                                              │\n│     import jax                                                               │\n│     import genjax                                                            │\n│     console = genjax.pretty()                                                │\n│                                                                              │\n│                                                                              │\n│     @genjax.gen                                                              │\n│     def random_walk(prev):                                                   │\n│         x = genjax.trace(\"x\", genjax.Normal)(prev, 1.0)                      │\n│         return x                                                             │\n│                                                                              │\n│                                                                              │\n│     unfold = genjax.UnfoldCombinator(random_walk, 1000)                      │\n│     init = 0.5                                                               │\n│     key = jax.random.PRNGKey(314159)                                         │\n│     key, tr = jax.jit(genjax.simulate(unfold))(key, (999, init))             │\n│     console.print(tr)                                                        │\n│                                                                              │\n│         assess = def assess(self, key: jaxtyping.UInt[Array, '...'], chm:    │\n│                  genjax._src.core.datatypes.ChoiceMap, args: tuple,          │\n│                  **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],            │\n│                  tuple[typing.Any, typing.Union[float,                       │\n│                  jaxtyping.Float[Array, '...']]]]:                           │\n│    choice_grad = def choice_grad(self, key, trace, selection):               │\n│        flatten = def flatten(self):                                          │\n│ get_trace_type = def get_trace_type(self, key: jaxtyping.UInt[Array, '...'], │\n│                  args: tuple, **kwargs) ->                                   │\n│                  genjax._src.generative_functions.combinators.combinator_tr… │\n│     importance = def importance(self, key: jaxtyping.UInt[Array, '...'],     │\n│                  chm: genjax._src.core.datatypes.ChoiceMap, args: tuple,     │\n│                  **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],            │\n│                  tuple[typing.Union[float, jaxtyping.Float[Array, '...']],   │\n│                  genjax._src.generative_functions.combinators.unfold.Unfold… │\n│            new = def new(*args, **kwargs):                                   │\n│       simulate = def simulate(self, key: jaxtyping.UInt[Array, '...'], args: │\n│                  tuple, **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],     │\n│                  genjax._src.generative_functions.combinators.unfold.Unfold… │\n│      unflatten = def unflatten(data, xs):                                    │\n│          unzip = def unzip(self, key: jaxtyping.UInt[Array, '...'], fixed:   │\n│                  genjax._src.core.datatypes.ChoiceMap) ->                    │\n│                  Tuple[jaxUInt[Array, '...'],                                │\n│                  Callable[[genjax._src.core.datatypes.ChoiceMap, Tuple],     │\n│                  Union[float, jaxFloat[Array, '...']]],                      │\n│                  Callable[[genjax._src.core.datatypes.ChoiceMap, Tuple],     │\n│                  Any]]:                                                      │\n│         update = def update(self, key: jaxtyping.UInt[Array, '...'], prev:   │\n│                  genjax._src.core.datatypes.Trace, chm:                      │\n│                  genjax._src.core.datatypes.ChoiceMap, argdiffs: tuple,      │\n│                  **kwargs) -> tuple[jaxtyping.UInt[Array, '...'],            │\n│                  tuple[typing.Any, typing.Union[float,                       │\n│                  jaxtyping.Float[Array, '...']],                             │\n│                  genjax._src.generative_functions.combinators.unfold.Unfold… │\n│                  genjax._src.core.datatypes.ChoiceMap]]:                     │\n╰──────────────────────────────────────────────────────────────────────────────╯\n\n\n\nBy inspecting the class, we gain some confidence that it indeed implements the generative function interface. Below, we show a TikZ picture which illustrates the pattern which Unfold captures.\n\nUsing Unfold\nHow do we make an instance of Unfold? Given an existing generative function which is a kernel - a kernel accepts and returns the same type signature - we can create a valid Unfold instance.22 This is not strictly true. Unfold also allows you to pass in a set of static arguments which are provided to the kernel after the state argument, unchanged, at each time step. We show this at the bottom of the notebook.\nHere’s an example kernel:\n\n@genjax.gen\ndef kernel(prev_latent):\n    new_latent = genjax.trace(\"z\", genjax.Normal)(prev_latent, 1.0)\n    new_obs = genjax.trace(\"x\", genjax.Normal)(new_latent, 1.0)\n    return new_latent\n\n\nkey, tr = jax.jit(kernel.simulate)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function kernel>\n├── args\n│   └── tuple\n│       └──  f32[]\n├── retval\n│   └──  f32[]\n├── choices\n│   └── Trie\n│       ├── :x\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Normal\n│       │       ├── args\n│       │       │   └── tuple\n│       │       │       ├──  f32[]\n│       │       │       └──  f32[]\n│       │       ├── value\n│       │       │   └──  f32[]\n│       │       └── score\n│       │           └──  f32[]\n│       └── :z\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Normal\n│               ├── args\n│               │   └── tuple\n│               │       ├──  f32[]\n│               │       └──  f32[]\n│               ├── value\n│               │   └──  f32[]\n│               └── score\n│                   └──  f32[]\n├── cache\n│   └── Trie\n└── score\n    └──  f32[]\n\n\n\nTo create an Unfold instance, we provide two things:\n\nThe kernel generative function.\nA static maximum unroll chain argument. Dynamically, Unfold may not unroll all the way up to this maximum - but for JAX/XLA compilation, we need to provide this maximum value as an invariant upper bound for any invocation of Unfold.\n\n\nchain = genjax.Unfold(kernel, max_length=10)\nchain\n\n\n\n\nUnfoldCombinator\n├── kernel\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function kernel>\n└── max_length\n    └── (const) 10\n\n\n\nTo invoke an interface method, the arguments which Unfold expects is a Tuple, where the first element is the maximum index in the resulting chain, and the second element is the initial state.\n\n\n\n\n\n\nUsage of index argument vs. a length argument\n\n\n\nNote how we’ve bolded index above - think of the index value as denoting an upper bound on active indices for the resulting chain. An active index is one in which the value was evolved using the kernel from the previous value. Passing in index = 5 means: all values after return[5] are not evolved, they’re just filled with the return[5] value.\nIndexing follows Python convention - so e.g. passing in 0 as the index means that a single application of the kernel was applied to the state, before evolution was halted and evolved statically.\n\n\n\nkey, tr = jax.jit(chain.simulate)(key, (5, 0.3))\ntr\n\n\n\n\nUnfoldTrace\n├── gen_fn\n│   └── UnfoldCombinator\n│       ├── kernel\n│       │   └── BuiltinGenerativeFunction\n│       │       └── source\n│       │           └── <function kernel>\n│       └── max_length\n│           └── (const) 10\n├── indices\n│   └──  i32[10]\n├── inner\n│   └── BuiltinTrace\n│       ├── gen_fn\n│       │   └── BuiltinGenerativeFunction\n│       │       └── source\n│       │           └── <function kernel>\n│       ├── args\n│       │   └── tuple\n│       │       └──  f32[10]\n│       ├── retval\n│       │   └──  f32[10]\n│       ├── choices\n│       │   └── Trie\n│       │       ├── :x\n│       │       │   └── DistributionTrace\n│       │       │       ├── gen_fn\n│       │       │       │   └── _Normal\n│       │       │       ├── args\n│       │       │       │   └── tuple\n│       │       │       │       ├──  f32[10]\n│       │       │       │       └──  f32[10]\n│       │       │       ├── value\n│       │       │       │   └──  f32[10]\n│       │       │       └── score\n│       │       │           └──  f32[10]\n│       │       └── :z\n│       │           └── DistributionTrace\n│       │               ├── gen_fn\n│       │               │   └── _Normal\n│       │               ├── args\n│       │               │   └── tuple\n│       │               │       ├──  f32[10]\n│       │               │       └──  f32[10]\n│       │               ├── value\n│       │               │   └──  f32[10]\n│       │               └── score\n│       │                   └──  f32[10]\n│       ├── cache\n│       │   └── Trie\n│       └── score\n│           └──  f32[10]\n├── args\n│   └── tuple\n│       ├──  i32[]\n│       └──  f32[]\n├── retval\n│   └──  f32[10]\n└── score\n    └──  f32[]\n\n\n\n\ntr.indices\n\nArray([ 0,  1,  2,  3,  4,  5, -1, -1, -1, -1], dtype=int32, weak_type=True)\n\n\n\n\ntr.get_retval()\n\nArray([-1.0472125 ,  0.282778  , -0.5653119 ,  0.92312765, -0.31269455,\n        0.46679875,  0.46679875,  0.46679875,  0.46679875,  0.46679875],      dtype=float32, weak_type=True)\n\n\n\nNote how tr.indices keep track of where the chain stopped evolving, according to the index argument to Unfold. In tr.get_retval(), we see that the final dynamic value (afterwards, evolution stops) occurs at index = 5."
  },
  {
    "objectID": "intermediate/intro_to_combinators/intro_to_combinators.html#combinator-choice-maps",
    "href": "intermediate/intro_to_combinators/intro_to_combinators.html#combinator-choice-maps",
    "title": "Introduction to generative function combinators",
    "section": "Combinator choice maps",
    "text": "Combinator choice maps\nTypically, each combinator has a unique choice map. The choice map simultaneously represents the structure of the generative choices which the transformed combinator generative function makes, as well as optimization opportunities which a user can take advantage of.\nLet’s study the choice map for UnfoldTrace.\n\nchm = tr.get_choices()\nchm\n\n\n\n\nVectorChoiceMap\n├── indices\n│   └──  i32[10]\n└── inner\n    └── BuiltinTrace\n        ├── gen_fn\n        │   └── BuiltinGenerativeFunction\n        │       └── source\n        │           └── <function kernel>\n        ├── args\n        │   └── tuple\n        │       └──  f32[10]\n        ├── retval\n        │   └──  f32[10]\n        ├── choices\n        │   └── Trie\n        │       ├── :x\n        │       │   └── DistributionTrace\n        │       │       ├── gen_fn\n        │       │       │   └── _Normal\n        │       │       ├── args\n        │       │       │   └── tuple\n        │       │       │       ├──  f32[10]\n        │       │       │       └──  f32[10]\n        │       │       ├── value\n        │       │       │   └──  f32[10]\n        │       │       └── score\n        │       │           └──  f32[10]\n        │       └── :z\n        │           └── DistributionTrace\n        │               ├── gen_fn\n        │               │   └── _Normal\n        │               ├── args\n        │               │   └── tuple\n        │               │       ├──  f32[10]\n        │               │       └──  f32[10]\n        │               ├── value\n        │               │   └──  f32[10]\n        │               └── score\n        │                   └──  f32[10]\n        ├── cache\n        │   └── Trie\n        └── score\n            └──  f32[10]\n\n\n\nAgain, let’s look at the indices.\n\nchm.indices\n\nArray([ 0,  1,  2,  3,  4,  5, -1, -1, -1, -1], dtype=int32, weak_type=True)\n\n\n\nNo surprises - the choice map also keeps track of which indices are active, and which indices are inactive.\nInactive indices do not participate in inference metadata computations - so e.g. if we ask for the score of the trace:\n\ntr.get_score()\n\nArray(-16.690369, dtype=float32)\n\n\n\nThe score is the same as the sum of sub-trace scores from [0:5].\n\nnp.sum(\n    tr.get_subtree(\"z\").get_score()[0:6] + tr.get_subtree(\"x\").get_score()[0:6]\n)\n\nArray(-16.690369, dtype=float32)\n\n\n\nThe reason why we have an index argument is that we can dynamically choose how much of the chain contributes to the generative computation. This index argument can come from other generative function - it need not be a JAX trace-time static value.\nWith this in mind, it’s best to think of Unfold as representing a space of processes which unroll up to some maximum static length - but the active generative process can halt before that maximum length."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistic programming with GenJAX",
    "section": "",
    "text": "TODO (MyBinder link - when repo is public)\n\n\n\nYou can open a Binder executable environment by following this tag:"
  },
  {
    "objectID": "index.html#what-is-genjax",
    "href": "index.html#what-is-genjax",
    "title": "Probabilistic programming with GenJAX",
    "section": "What is GenJAX?",
    "text": "What is GenJAX?\n\n(One sentence) It’s a GPU accelerated probabilistic programming library designed to support model composition and inference customization.\n(Longer) It’s an implementation of Gen1 on top of JAX - exposing the ability to programmatically construct and manipulate generative functions, as well as JIT compile + auto-batch inference computations using generative functions onto GPU devices.\n\n1 Gen is a multi-paradigm (generative, differentiable, incremental) language for probabilistic programming focused on generative functions: computational objects which represent probability measures over structured sample spaces."
  },
  {
    "objectID": "index.html#gens-approach-to-probabilistic-programming",
    "href": "index.html#gens-approach-to-probabilistic-programming",
    "title": "Probabilistic programming with GenJAX",
    "section": "Gen’s approach to probabilistic programming",
    "text": "Gen’s approach to probabilistic programming\nThe philosophy of Gen is different from other probabilistic programming systems. Gen’s main conceptual contribution is a formal interface (the generative function interface) of methods and associated data types which supports the implementation of several families of inference algorithms. The interface forms an abstraction layer between generative function languages (those objects which implement the interface) and inference algorithms.\nThe interface is compositional - allowing usage of generative functions inside of other generative functions. It is also extensible: users can and are encouraged to write their own generative function languages, with custom optimizations or internal inference facilities.\nOf course, GenJAX provide a number of essential implementations, including a language of distributions (from both JAX and TensorFlow Probability) a program-like modeling language based on pure, numerical Python programs, and a set of higher-order combinators which express structured patterns of generative computation.\nGenJAX also provides a standard library of inference algorithms which works with generative functions - including importance sampling, MCMC, ADVI, SMC, and more."
  }
]