[
  {
    "objectID": "advanced/pseudomarginal_smc/pseudomarginal_smc.html",
    "href": "advanced/pseudomarginal_smc/pseudomarginal_smc.html",
    "title": "Coarse-to-fine structure inference with pseudomarginal SMC",
    "section": "",
    "text": "In this notebook, we’ll be combining several advanced ingredients of Gen and GenJAX to construct an approximate density induced by a variant of sequential Monte Carlo (SMC) targeting successively richer approximations to latent structure.\nThe notebook assumes several pre-requisites:\n\nUnderstanding generative functions, their interfaces, and the math that the interface functions compute.\nUnderstanding generative function combinators, and how genjax.Recurse can allow us to express bounded tree-like computations.\nUnderstanding the approximate density interface exposed by GenProx, and how we can equip generative functions with inference strategies to define approximate densities.\n\n\n\n\n© Copyright 2022 MIT Probabilistic Computing Project."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html",
    "title": "Introduction to Gen and GenJAX",
    "section": "",
    "text": "The purpose of this notebook to give the listener/reader an accelerated introduction to several concepts native to Gen and GenJAX (an implementation of Gen on top of JAX). As for pre-requisites, it assumes familiarity with trace-based probabilistic programming systems, and Monte Carlo inference - especially importance sampling and MCMC methods."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-genjax",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-genjax",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is GenJAX?",
    "text": "What is GenJAX?\nGenJAX is:\n\nA probabilistic programming system based on the concepts of Gen.\nA model and inference compiler with support for device acceleration (courtesy of JAX).\nA base layer for experiments in model and inference DSL design.\n\nBy virtue of a few key design decisions, and JAX’s excellent foundation - it natively supports several common accelerator idioms - like automatic struct-of-array representations, and the ability to automatically batch model/inference programs onto accelerators. It does this - while supporting the convenience of Gen’s interfaces - allowing modular construction of generative programs from smaller pieces.\nBy construction, all GenJAX modeling + inference code is JAX jittable - and thus, vmapable, etc."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-a-generative-function",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-a-generative-function",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is a generative function?",
    "text": "What is a generative function?\nGenerative functions are the key concept of Gen’s probabilistic programming paradigm. Generative functions are computational objects defined by a set of associated data types and methods. These types and methods describe compositional interfaces useful for inference computations.\nGen’s formal description of generative functions consist of two objects:\n\n\\(P(\\tau, r; x)\\) - a measure over dictionary-like data (choice maps) and untraced randomness \\(r\\), parametrized by arguments \\(x\\).\n\\(f(\\tau; x)\\) - a deterministic function from the above measure’s target space to a space of data types.\n\nWe often think about sampling choice maps from \\(P\\), computing the return value from the generative function call using \\(f\\) - we record both in Trace objects - data structures which contain the recordings of these values, along with probabilistic metadata like the score of random choices selected along the way.\nHere’s an example of a GenJAX generative function. This generative function is part of a function-like language - pay close attention to the hierarchical compositionality of generative functions in this language under an abstraction (genjax.trace) similar to a function call.\n\n@genjax.gen\ndef g(key, x):\n    key, m1 = genjax.trace(\"m0\", genjax.Bernoulli)(key, x)\n    return (key, m1)\n\n\n@genjax.gen\ndef h(key, x):\n    key, m1 = genjax.trace(\"m0\", g)(key, x)\n    return (key, m1)\n\n\nh\n\n\n\n\nBuiltinGenerativeFunction\n└── source\n    └── <function h>\n\n\n\nThis is a Callable object - operations (see the list under Generative function interface below) which are useful for modeling and inference are given semantics via program transformations.\nLet’s examine these operations now.\n\nconsole.inspect(genjax.BuiltinGenerativeFunction, methods=True)\n\n╭──────────── <class 'genjax.generative_functions.builtin.builtin_gen_fn.BuiltinGenerativeFunction'> ─────────────╮\n│ class BuiltinGenerativeFunction(source: Callable) -> None:                                                      │\n│                                                                                                                 │\n│ BuiltinGenerativeFunction(source: Callable)                                                                     │\n│                                                                                                                 │\n│         assess = def assess(self, key, chm, args, **kwargs):                                                    │\n│        flatten = def flatten(self):                                                                             │\n│ get_trace_type = def get_trace_type(self, key, args, **kwargs):                                                 │\n│     importance = def importance(self, key, chm, args, **kwargs):                                                │\n│       simulate = def simulate(self, key, args, **kwargs):                                                       │\n│      unflatten = def unflatten(data, xs):                                                                       │\n│          unzip = def unzip(self, key: jaxtyping.Integer[Array, '...'], fixed: genjax.core.datatypes.ChoiceMap)  │\n│                  -> Tuple[jaxtyping.Integer[Array, '...'], Callable[[genjax.core.datatypes.ChoiceMap, Tuple],   │\n│                  float], Callable[[genjax.core.datatypes.ChoiceMap, Tuple], Any]]:                              │\n│         update = def update(self, key, prev, new, args, **kwargs):                                              │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nThis is our first glimpse of the generative function interface (GFI), the secret sauce which Gen is based around.\nThere’s a few methods here which are not part of the GFI:\n\nflatten - which allows us to treat generative functions as Pytree implementors.\nunflatten - same as above.\n\nLet’s study the simulate method first: we’ll explore its semantics, and see the types of data it produces.\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function h>\n├── args\n│   └── (0.3,)\n├── retval\n│   └── bool[]\n├── choices\n│   └── BuiltinChoiceMap\n│       └── m0\n│           └── BuiltinTrace\n│               ├── gen_fn\n│               │   └── BuiltinGenerativeFunction\n│               │       └── source\n│               │           └── <function g>\n│               ├── args\n│               │   └── (0.3,)\n│               ├── retval\n│               │   └── bool[]\n│               ├── choices\n│               │   └── BuiltinChoiceMap\n│               │       └── m0\n│               │           └── DistributionTrace\n│               │               ├── gen_fn\n│               │               │   └── _Bernoulli\n│               │               ├── args\n│               │               │   └── (0.3,)\n│               │               ├── value\n│               │               │   └── ValueChoiceMap\n│               │               │       └── value\n│               │               │           └── bool[]\n│               │               └── score\n│               │                   └── f32[]\n│               ├── cache\n│               │   └── BuiltinTrie\n│               └── score\n│                   └── f32[]\n├── cache\n│   └── BuiltinTrie\n└── score\n    └── f32[]\n\n\n\nIf you’re familiar with other “trace-based” probabilistic systems - this should look familiar.\nThis object instance is a piece of data which has captured information about the execution of the function. Specifically, the subtraces of other generative function calls in genjax.trace.\nIt also captures the score - the log probability of the normalized measure which the model program represents, evaluated at the random choices which the generative call execution produced. If you were paying attention above, the score is \\(\\log P(\\tau, r; x)\\).\n\nHow is simulate implemented for this language?\nFor this generative function language, we implement simulate using a code transformation! Here’s the transformed code.\n\njaxpr = jax.make_jaxpr(genjax.simulate(h))(key, (0.3,))\njaxpr\n\n{ lambda ; a:u32[2] b:f32[]. let\n    c:key<fry>[] = random_wrap[impl=fry] a\n    d:key<fry>[2] = random_split[count=2] c\n    e:u32[2,2] = random_unwrap d\n    f:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] e\n    g:u32[2] = squeeze[dimensions=(0,)] f\n    h:u32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=(1, 1)] e\n    i:u32[2] = squeeze[dimensions=(0,)] h\n    j:key<fry>[] = random_wrap[impl=fry] i\n    k:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:u32[] = random_bits[bit_width=32 shape=()] j\n    m:u32[] = shift_right_logical l 9\n    n:u32[] = or m 1065353216\n    o:f32[] = bitcast_convert_type[new_dtype=float32] n\n    p:f32[] = sub o 1.0\n    q:f32[] = sub 1.0 0.0\n    r:f32[] = mul p q\n    s:f32[] = add r 0.0\n    t:f32[] = reshape[dimensions=None new_sizes=()] s\n    u:f32[] = max 0.0 t\n    v:bool[] = lt u k\n    w:f32[] = convert_element_type[new_dtype=float32 weak_type=True] v\n    x:f32[] = sub w 0.0\n    y:bool[] = ne x 0.0\n    z:f32[] = xla_call[\n      call_jaxpr={ lambda ; ba:bool[] bb:f32[] bc:f32[]. let\n          bd:bool[] = convert_element_type[new_dtype=bool weak_type=False] ba\n          be:f32[] = select_n bd bc bb\n        in (be,) }\n      name=_where\n    ] y x 1.0\n    bf:f32[] = xla_call[\n      call_jaxpr={ lambda ; bg:bool[] bh:f32[] bi:f32[]. let\n          bj:bool[] = convert_element_type[new_dtype=bool weak_type=False] bg\n          bk:f32[] = select_n bj bi bh\n        in (bk,) }\n      name=_where\n    ] y b 1.0\n    bl:f32[] = log bf\n    bm:f32[] = mul z bl\n    bn:f32[] = xla_call[\n      call_jaxpr={ lambda ; bo:bool[] bp:f32[] bq:f32[]. let\n          br:bool[] = convert_element_type[new_dtype=bool weak_type=False] bo\n          bs:f32[] = select_n br bq bp\n        in (bs,) }\n      name=_where\n    ] y bm 0.0\n    bt:f32[] = sub 1.0 x\n    bu:f32[] = neg b\n    bv:bool[] = ne bt 0.0\n    bw:f32[] = xla_call[\n      call_jaxpr={ lambda ; bx:bool[] by:f32[] bz:f32[]. let\n          ca:bool[] = convert_element_type[new_dtype=bool weak_type=False] bx\n          cb:f32[] = select_n ca bz by\n        in (cb,) }\n      name=_where\n    ] bv bt 1.0\n    cc:f32[] = xla_call[\n      call_jaxpr={ lambda ; cd:bool[] ce:f32[] cf:f32[]. let\n          cg:bool[] = convert_element_type[new_dtype=bool weak_type=False] cd\n          ch:f32[] = select_n cg cf ce\n        in (ch,) }\n      name=_where\n    ] bv bu 1.0\n    ci:f32[] = log1p cc\n    cj:f32[] = mul bw ci\n    ck:f32[] = xla_call[\n      call_jaxpr={ lambda ; cl:bool[] cm:f32[] cn:f32[]. let\n          co:bool[] = convert_element_type[new_dtype=bool weak_type=False] cl\n          cp:f32[] = select_n co cn cm\n        in (cp,) }\n      name=_where\n    ] bv cj 0.0\n    cq:f32[] = add bn ck\n    cr:bool[] = lt x 0.0\n    cs:bool[] = gt x 1.0\n    ct:bool[] = convert_element_type[new_dtype=bool weak_type=False] cr\n    cu:bool[] = convert_element_type[new_dtype=bool weak_type=False] cs\n    cv:bool[] = or ct cu\n    cw:f32[] = xla_call[\n      call_jaxpr={ lambda ; cx:bool[] cy:f32[] cz:f32[]. let\n          da:f32[] = select_n cx cz cy\n        in (da,) }\n      name=_where\n    ] cv -inf cq\n    db:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cw\n    dc:f32[] = reduce_sum[axes=()] db\n    dd:f32[] = add 0.0 dc\n    de:f32[] = add 0.0 dd\n  in (g, b, v, b, v, b, v, dc, dd, de) }"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#generative-function-interface",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#generative-function-interface",
    "title": "Introduction to Gen and GenJAX",
    "section": "Generative function interface",
    "text": "Generative function interface\nGenJAX’s generative functions define an interface which support compositional usage of generative functions within other generative functions. The interface functions here closely mirror the interfaces defined in Gen, deviating only when interfaces are redundant (or implemented in Gen.jl for performance optimized code paths - which may not be relevant to our implementation).\n\n\n\n\n\n\n\n\nInterface\nType\nInference algorithm support\n\n\n\n\nsimulate\nGenerative\nImportance sampling, SMC\n\n\nimportance\nGenerative\nImportance sampling, SMC\n\n\nupdate\nGenerative and incremental\nMCMC, SMC\n\n\nassess\nGenerative and differentiable\nMCMC, importance sampling, SMC\n\n\nunzip\nDifferentiable\nDifferentiable and involutive MCMC and SMC\n\n\n\nThis interface supports several methods - I’ve roughly described them and split them into the two categories Generative and Differentiable below:\n\nGenerative\n\nsimulate - sample from normalized trace measure, and return the score.\nimportance - given constraints for some addresses, sample from unnormalized trace measure and return an importance weight.\nupdate - given an existing trace, and a set of constraints and argument change values, update the trace to be consistent with the set of constraints under execution with the new arguments, and return an incremental importance weight.\nassess - given a complete choice map and arguments, return the normalized log probability.\n\n\n\nDifferentiable\n\nassess - same as above.\nunzip - given a set of fixed constraints, return two callables. The first callable score accepts constraints which fill in the complement of the fixed constraints and arguments, and returns the normalized log probability of all the constraints. The second callable retval accepts constraints and arguments, and returns the return value for the generative function call consistent with the constraints and given arguments.\n\nunzip produces two functions which can be compositionally used with jax.grad to evaluate gradients used by both differentiable and involutive MCMC and SMC."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#more-about-generative-functions",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#more-about-generative-functions",
    "title": "Introduction to Gen and GenJAX",
    "section": "More about generative functions",
    "text": "More about generative functions\n\nDistributions are generative functions\nIn GenJAX, distributions are generative functions.\n\nkey, tr = genjax.simulate(genjax.Normal)(key, (0.0, 1.0))\ntr\n\n\n\n\nDistributionTrace\n├── gen_fn\n│   └── _Normal\n├── args\n│   └── (0.0, 1.0)\n├── value\n│   └── ValueChoiceMap\n│       └── value\n│           └── f32[]\n└── score\n    └── f32[]\n\n\n\n\n\nAssociated data types\n\nChoice maps are the dictionary-like recordings of random choices in a trace.\nSelection is an object which allows querying a trace/choice map - selecting certain choices.\n\n\n@genjax.gen\ndef h(key, x):\n    key, m1 = genjax.trace(\"m0\", genjax.Bernoulli)(key, x)\n    key, m2 = genjax.trace(\"m1\", genjax.Bernoulli)(key, x)\n    return (key, m1 + m2)\n\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function h>\n├── args\n│   └── (0.3,)\n├── retval\n│   └── bool[]\n├── choices\n│   └── BuiltinChoiceMap\n│       ├── m0\n│       │   └── DistributionTrace\n│       │       ├── gen_fn\n│       │       │   └── _Bernoulli\n│       │       ├── args\n│       │       │   └── (0.3,)\n│       │       ├── value\n│       │       │   └── ValueChoiceMap\n│       │       │       └── value\n│       │       │           └── bool[]\n│       │       └── score\n│       │           └── f32[]\n│       └── m1\n│           └── DistributionTrace\n│               ├── gen_fn\n│               │   └── _Bernoulli\n│               ├── args\n│               │   └── (0.3,)\n│               ├── value\n│               │   └── ValueChoiceMap\n│               │       └── value\n│               │           └── bool[]\n│               └── score\n│                   └── f32[]\n├── cache\n│   └── BuiltinTrie\n└── score\n    └── f32[]\n\n\n\n\nselect = genjax.BuiltinSelection.new([\"m1\"])\nselected, _ = select.filter(tr.get_choices())\nselected\n\n\n\n\nBuiltinChoiceMap\n└── m1\n    └── DistributionTrace\n        ├── gen_fn\n        │   └── _Bernoulli\n        ├── args\n        │   └── (0.3,)\n        ├── value\n        │   └── ValueChoiceMap\n        │       └── value\n        │           └── bool[]\n        └── score\n            └── f32[]"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#great-now-what-can-i-do-with-them",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#great-now-what-can-i-do-with-them",
    "title": "Introduction to Gen and GenJAX",
    "section": "Great … now what can I do with them?",
    "text": "Great … now what can I do with them?\nWhile studying the interfaces and the computational objects which satisfied them in the abstract can be a pleasing hobby, we can do machine learning with them.\nLet’s consider a modeling problem where we wish to perform generalized regression with outliers between two variates, taking a family of polynomials as potential curves.\nOne such model for this data generating process is shown below.\n\n@genjax.gen\ndef model_y(key, x, degree):\n    key, coefficients = trace(\n        \"alpha\",\n        genjax.MapCombinator(genjax.Normal, in_axes=(None, None, None)),\n    )(key, 0.0, 2.0)\n    key, y = trace(\"value\", genjax.Normal)(key, 0.0, 1.0)\n\n\n@genjax.gen\ndef outlier_model(key, x, degree):\n    y = trace(\"value\", genjax.Normal)(key, 0.0, 10.0)\n    return key, y\n\n\nswitch = genjax.SwitchCombinator([model_y, outlier_model])\n\n\n@genjax.gen(genjax.MapCombinator, in_axes=(0, 0))\ndef kernel(key, x):\n    key, is_outlier = trace(\"outlier\", genjax.Bernoulli)(key, 0.1)\n    key, polynomial_degree = trace(\"degree\", genjax.Geometric)(key, 0.1)\n    key, y = trace(\"y\", switch)(key, is_outlier, x, polynomial_degree)\n    return key, y\n\n\n@genjax.gen\ndef model(key, xs):\n    pass\n\nThere’s a number of implementation patterns which you might pick up on by studying this model.\n\nGenerative functions explicitly pass a PRNG key in and out. This conforms to JAX’s PRNG usage expectations.\nTo implement control flow, we use higher-order functions called combinators. These accept generative functions as input, and return generative functions as output.\nAny JAX compatible code is allowed in the body of a generative function.\n\nCourtesy of the interface, we get to design our model generative function in pieces.\nNow, let’s examine a few sample traces from our model.\n\ndef trace_visualizer(tr):\n    pass"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#your-first-inference-program",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#your-first-inference-program",
    "title": "Introduction to Gen and GenJAX",
    "section": "Your first inference program",
    "text": "Your first inference program\n\n\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling and inference notebooks",
    "section": "",
    "text": "This is a notebook repository for pedagogical tutorials on the usage of GenJAX.\n\n\n\n\n\n\n© Copyright 2022 MIT Probabilistic Computing Project."
  }
]