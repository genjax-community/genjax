[
  {
    "objectID": "advanced/pseudomarginal_smc/pseudomarginal_smc.html",
    "href": "advanced/pseudomarginal_smc/pseudomarginal_smc.html",
    "title": "Coarse-to-fine structure inference with pseudomarginal SMC",
    "section": "",
    "text": "In this notebook, we’ll be combining several advanced ingredients of Gen and GenJAX to construct an approximate density induced by a variant of sequential Monte Carlo (SMC) targeting successively richer approximations to latent structure.\nThe notebook assumes several pre-requisites:\n\nUnderstanding generative functions, their interfaces, and the math that the interface functions compute.\nUnderstanding generative function combinators, and how genjax.Recurse can allow us to express bounded tree-like computations.\nUnderstanding the approximate density interface exposed by GenProx, and how we can equip generative functions with inference strategies to define approximate densities."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html",
    "title": "Introduction to Gen and GenJAX",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport genjax\nfrom genjax import GenerativeFunction, ChoiceMap, Selection, trace\n\nsns.set_theme(style=\"white\")\n\n# Pretty printing.\nconsole = genjax.pretty(width=70)\n\n# Reproducibility.\nkey = jax.random.PRNGKey(314159)"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-genjax",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-genjax",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is GenJAX?",
    "text": "What is GenJAX?\nHere are a few high-level ways to think about GenJAX:\n\nA probabilistic programming system based on the concepts of Gen.\nA Bayesian model and inference compiler with support for device acceleration (courtesy of JAX).\nA base layer for experiments in model and inference DSL design.\n\nThere’s already a well-supported implementation of Gen in Julia. Why is a JAX port interesting?\nThere are a number of compelling technical and social reasons to explore Gen’s probabilistic programming paradigm on top of JAX, here are a few:\n\nJAX’s excellent accelerator support - our implementation natively supports several common accelerator idioms - like automatic struct-of-array representations, and the ability to automatically batch model/inference programs onto accelerators.\nJAX’s excellent support for compositional AD removes implementation and maintenance complexity for Gen’s gradient interfaces - previously a difficult challenge in other implementations. In addition, JAX’s support for convenient, higher-order AD opens up new opportunities to explore during inference design with gradient interfaces.\nJAX exposes compositional code transformations to library authors, and, as library authors, we can utilize code transformations to implement state-of-the-art optimizations for models and inference expressed in our system.\nA lot of domain experts and modelers are working in Python! Some of them even use JAX (hopefully more each year). Presenting an interface to Gen which is familiar, and takes advantage of JAX’s native ecosystem is a compelling social reason.\n\nLet’s truncate the list there for now.\nFor the JAX literati, one final (hopefully tantalizing) takeaway: by construction, all GenJAX modeling + inference code is JAX traceable - and thus, vmapable, jitable, etc."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-probabilistic-programming",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-probabilistic-programming",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is probabilistic programming?",
    "text": "What is probabilistic programming?\nPerhaps you may be coming to this notebook without any prior knowledge in probabilistic programming…\nThat’s okay! Ideally, the ideas in this notebook should be self-contained1.1 You may miss why generative functions (see below) are designed the way they are on a first read - but you’ll still get the punchline if you follow the notebook to the end.\n\nA Bayesian viewpoint\nHere’s one practical take on what probabilistic programming is all about: programming language design for expressing and solving Bayesian inference problems2.2 In the Probabilistic Computing lab at MIT, we also consider differentiable programming to be contained within the set of concerns of probabilistic programming. We won’t cover differentiable programming interfaces in this notebook.\nThere are other, more general, ways to say similar things (e.g. computational representations of operations on measures). Probabilistic programming is a broad field, and there are corners which may not be covered by our practical take. We’ll just assume that people are interested in Bayes, and how to represent Bayes on computers in nice ways. For our purposes in this notebook, we’ll stick as much as we can to the basics.\n\n\nWhat are we actually computing with?\nThe objects which we program with expose a mixture of generative and differentiable interfaces - the interfaces are designed to support common (as well as quite advanced) classes of Bayesian inference algorithms. Gen provides automation for the tricky math which these algorithms sometimes require.\nWe separate the design of inference (whose implementation uses the interfaces), from the implementation of the interfaces on computational objects. This allows us to build languages of objects which satisfy the interfaces - and allows their interoperability and usage.\nIn Gen, we call objects which implement the interfaces generative functions."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-a-generative-function",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#what-is-a-generative-function",
    "title": "Introduction to Gen and GenJAX",
    "section": "What is a generative function?",
    "text": "What is a generative function?\nGenerative functions are the key concept of Gen’s probabilistic programming paradigm. Generative functions are computational objects defined by a set of associated data types and methods. These types and methods describe compositional interfaces that are useful for Bayesian inference computations.\nGen’s formal description of generative functions consist of two objects:\n\n\\(P(\\tau, r; x)\\) - a normalized measure over tree-like data (choice maps) and untraced randomness3 \\(r\\), parametrized by arguments \\(x\\).\n\\(f(\\tau; x)\\) - a deterministic function from the above measure’s sample space to a space of data types.\n\n3 More on this later. It’s safe to say “I have no idea what that is” for now, and expect us to explain later or in another notebook.We can informally think of the sampling semantics of these objects as consisting of two steps:\n\nFirst, sample a choice map from \\(P\\).\nThen, compute the return value using \\(f\\).\n\nIn many of the generative function interfaces, we won’t just be interested in the final sampled return value. We’ll also be interested in what happened along the way: we’ll record the intermediate and final results of these steps in Trace objects - data structures which contain the recordings of values, along with probabilistic metadata like the score of random choices selected along the way.\nHere’s an example of a GenJAX generative function4.4 There’s not just one generative function class - users can and are encouraged to design new types of generative functions which capture repeated modeling patterns. An excellent example of this modularity in Gen’s design is generative function combinators.\nThis generative function is part of a function-like language - pay close attention to the hierarchical compositionality of generative functions in this language under an abstraction (genjax.trace) similar to a function call.\nWe’ll discuss the addresses (\"sub\" and \"m0\") a bit later.\n\n@genjax.gen\ndef g(key, x):\n    key, m0 = genjax.trace(\"m0\", genjax.Bernoulli)(key, x)\n    return (key, m0)\n\n\n@genjax.gen\ndef h(key, x):\n    key, m0 = genjax.trace(\"sub\", g)(key, x)\n    return (key, m0)\n\n\nh\n\n\n\n\nBuiltinGenerativeFunction\n└── source\n    └── <function h>\n\n\n\nThis generative function holds a Python Callable object. For this generative function language, the interface methods (see the list under Generative function interface below) which are useful for modeling and inference are given semantics via JAX’s tracing and program transformation infrastructure.\nLet’s examine some of these operations now.\n\nconsole.inspect(genjax.BuiltinGenerativeFunction, methods=True)\n\n╭─ <class 'genjax.generative_functions.builtin.builtin_gen_fn.Builti─╮\n│ class BuiltinGenerativeFunction(source: Callable) -> None:         │\n│                                                                    │\n│ BuiltinGenerativeFunction(source: Callable)                        │\n│                                                                    │\n│         assess = def assess(self, key, chm, args, **kwargs):       │\n│        flatten = def flatten(self):                                │\n│ get_trace_type = def get_trace_type(self, key, args, **kwargs):    │\n│     importance = def importance(self, key, chm, args, **kwargs):   │\n│       simulate = def simulate(self, key, args, **kwargs):          │\n│      unflatten = def unflatten(data, xs):                          │\n│          unzip = def unzip(self, key: jaxtyping.Integer[Array,     │\n│                  '...'], fixed: genjax.core.datatypes.ChoiceMap)   │\n│                  -> Tuple[jaxtyping.Integer[Array, '...'],         │\n│                  Callable[[genjax.core.datatypes.ChoiceMap,        │\n│                  Tuple], float],                                   │\n│                  Callable[[genjax.core.datatypes.ChoiceMap,        │\n│                  Tuple], Any]]:                                    │\n│         update = def update(self, key, prev, new, args, **kwargs): │\n╰────────────────────────────────────────────────────────────────────╯\n\n\n\nThis is our first glimpse of the generative function interface (GFI), the secret sauce which Gen is based around.\n\n\n\n\n\n\nJAX interfaces\n\n\n\nThere’s a few methods here which are not part of the GFI, but are worth mentioning because they deal with data interfaces to JAX:\n\nflatten - which allows us to treat generative functions as Pytree implementors.\nunflatten - same as above.\n\nThese are used to register the implementor type as a Pytree, which is roughly a tree-like Python structure which JAX can zip/unzip at runtime API boundaries.\n\n\nLet’s study the simulate method first: we’ll explore its semantics, and see the types of data it produces.\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function h>\n├── args\n│   └── (0.3,)\n├── retval\n│   └── bool[]\n├── choices\n│   └── BuiltinChoiceMap\n│       └── inner\n│           └── HashableDict\n│               └── sub\n│                   └── BuiltinTrace\n│                       ├── gen_fn\n│                       │   └── BuiltinGenerativeFunction\n│                       │       └── source\n│                       │           └── <function g>\n│                       ├── args\n│                       │   └── (0.3,)\n│                       ├── retval\n│                       │   └── bool[]\n│                       ├── choices\n│                       │   └── BuiltinChoiceMap\n│                       │       └── inner\n│                       │           └── HashableDict\n│                       │               └── m0\n│                       │                   └── DistributionTrace\n│                       │                       ├── gen_fn\n│                       │                       │   └── _Bernoulli\n│                       │                       ├── args\n│                       │                       │   └── (0.3,)\n│                       │                       ├── value\n│                       │                       │   └── ValueChoiceMap\n│                       │                       │       └── value\n│                       │                       │           └── bool[]\n│                       │                       └── score\n│                       │                           └── f32[]\n│                       ├── cache\n│                       │   └── BuiltinTrie\n│                       └── score\n│                           └── f32[]\n├── cache\n│   └── BuiltinTrie\n└── score\n    └── f32[]\n\n\n\nIf you’re familiar with other “trace-based” probabilistic systems - this should look familiar.\nThis object instance is a piece of data which has captured information about the execution of the function. Specifically, the subtraces of other generative function calls which occur in genjax.trace statements.\nIt also captures the score - the log probability of the normalized measure which the model program represents, evaluated at the random choices which the generative call execution produced.\nIf you were paying attention above, the score is \\(\\log P(\\tau, r; x)\\).\n\nHow is simulate implemented for this language?\nFor this generative function language, we implement simulate using a code transformation! Here’s the transformed code.\n\njaxpr = jax.make_jaxpr(genjax.simulate(h))(key, (0.3,))\njaxpr\n\n{ lambda ; a:u32[2] b:f32[]. let\n    c:key<fry>[] = random_wrap[impl=fry] a\n    d:key<fry>[2] = random_split[count=2] c\n    e:u32[2,2] = random_unwrap d\n    f:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] e\n    g:u32[2] = squeeze[dimensions=(0,)] f\n    h:u32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=(1, 1)] e\n    i:u32[2] = squeeze[dimensions=(0,)] h\n    j:key<fry>[] = random_wrap[impl=fry] i\n    k:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:u32[] = random_bits[bit_width=32 shape=()] j\n    m:u32[] = shift_right_logical l 9\n    n:u32[] = or m 1065353216\n    o:f32[] = bitcast_convert_type[new_dtype=float32] n\n    p:f32[] = sub o 1.0\n    q:f32[] = sub 1.0 0.0\n    r:f32[] = mul p q\n    s:f32[] = add r 0.0\n    t:f32[] = reshape[dimensions=None new_sizes=()] s\n    u:f32[] = max 0.0 t\n    v:bool[] = lt u k\n    w:f32[] = convert_element_type[new_dtype=float32 weak_type=True] v\n    x:f32[] = sub w 0.0\n    y:bool[] = ne x 0.0\n    z:f32[] = xla_call[\n      call_jaxpr={ lambda ; ba:bool[] bb:f32[] bc:f32[]. let\n          bd:bool[] = convert_element_type[new_dtype=bool weak_type=False] ba\n          be:f32[] = select_n bd bc bb\n        in (be,) }\n      name=_where\n    ] y x 1.0\n    bf:f32[] = xla_call[\n      call_jaxpr={ lambda ; bg:bool[] bh:f32[] bi:f32[]. let\n          bj:bool[] = convert_element_type[new_dtype=bool weak_type=False] bg\n          bk:f32[] = select_n bj bi bh\n        in (bk,) }\n      name=_where\n    ] y b 1.0\n    bl:f32[] = log bf\n    bm:f32[] = mul z bl\n    bn:f32[] = xla_call[\n      call_jaxpr={ lambda ; bo:bool[] bp:f32[] bq:f32[]. let\n          br:bool[] = convert_element_type[new_dtype=bool weak_type=False] bo\n          bs:f32[] = select_n br bq bp\n        in (bs,) }\n      name=_where\n    ] y bm 0.0\n    bt:f32[] = sub 1.0 x\n    bu:f32[] = neg b\n    bv:bool[] = ne bt 0.0\n    bw:f32[] = xla_call[\n      call_jaxpr={ lambda ; bx:bool[] by:f32[] bz:f32[]. let\n          ca:bool[] = convert_element_type[new_dtype=bool weak_type=False] bx\n          cb:f32[] = select_n ca bz by\n        in (cb,) }\n      name=_where\n    ] bv bt 1.0\n    cc:f32[] = xla_call[\n      call_jaxpr={ lambda ; cd:bool[] ce:f32[] cf:f32[]. let\n          cg:bool[] = convert_element_type[new_dtype=bool weak_type=False] cd\n          ch:f32[] = select_n cg cf ce\n        in (ch,) }\n      name=_where\n    ] bv bu 1.0\n    ci:f32[] = log1p cc\n    cj:f32[] = mul bw ci\n    ck:f32[] = xla_call[\n      call_jaxpr={ lambda ; cl:bool[] cm:f32[] cn:f32[]. let\n          co:bool[] = convert_element_type[new_dtype=bool weak_type=False] cl\n          cp:f32[] = select_n co cn cm\n        in (cp,) }\n      name=_where\n    ] bv cj 0.0\n    cq:f32[] = add bn ck\n    cr:bool[] = lt x 0.0\n    cs:bool[] = gt x 1.0\n    ct:bool[] = convert_element_type[new_dtype=bool weak_type=False] cr\n    cu:bool[] = convert_element_type[new_dtype=bool weak_type=False] cs\n    cv:bool[] = or ct cu\n    cw:f32[] = xla_call[\n      call_jaxpr={ lambda ; cx:bool[] cy:f32[] cz:f32[]. let\n          da:f32[] = select_n cx cz cy\n        in (da,) }\n      name=_where\n    ] cv -inf cq\n    db:f32[] = convert_element_type[new_dtype=float32 weak_type=False] cw\n    dc:f32[] = reduce_sum[axes=()] db\n    dd:f32[] = add 0.0 dc\n    de:f32[] = add 0.0 dd\n  in (g, b, v, b, v, b, v, dc, dd, de) }\n\n\n\nThat’s a lot of code! This code is pure, numerical, and ready for acceleration. By utilizing JAX and a staging transformation, we’ve stripped out all Python overhead.\nThis is how we’ve implemented simulate for this particular generative function language.55 In general, Gen doesn’t require that we follow the same “code transformation” implementation for other generative function languages. GenJAX, however, is a bit special - because we restrict the user to remain within the JAX traceable subset of Python - any generative function interface implementation must also be JAX traceable. This is a JAX requirement, not a Gen one."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#generative-function-interface",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#generative-function-interface",
    "title": "Introduction to Gen and GenJAX",
    "section": "Generative function interface",
    "text": "Generative function interface\nThere are a few more generative function interface methods worth discussing.\nIn this notebook, instead of carefully walking through the math which these interface methods compute, we’ll defer that discussion to another notebook. Below, we give an informal discussion of what each of the interface methods computes, and roughly describe what algorithm families are supported by their usage.\n\nThe generative function interface in GenJAX\nGenJAX’s generative functions define an interface which support compositional usage of generative functions within other generative functions. The interface functions here closely mirror the interfaces defined in Gen.jl.\nIn the following, we use the following abbreviations:\n\nIS - importance sampling\nSMC - sequential Monte Carlo\nMCMC - Markov chain Monte Carlo\nVI - variational inference\n\n\n\n\n\n\n\n\n\nInterface\nType\nInference algorithm support\n\n\n\n\nsimulate\nGenerative\nIS, SMC\n\n\nimportance\nGenerative\nIS, SMC, VI\n\n\nupdate\nGenerative and incremental\nMCMC, SMC\n\n\nassess\nGenerative and differentiable\nMCMC, IS, SMC\n\n\nunzip\nDifferentiable\nDifferentiable and involutive MCMC and SMC, VI\n\n\n\nThis interface supports several methods - I’ve roughly described them and split them into the two categories Generative and Differentiable below:\n\nGenerative\n\nsimulate - sample from normalized trace measure, and return the score.\nimportance - given constraints for some addresses, sample from unnormalized trace measure and return an importance weight.\nupdate - given an existing trace, and a set of constraints and argument change values, update the trace to be consistent with the set of constraints under execution with the new arguments, and return an incremental importance weight.\nassess - given a complete choice map and arguments, return the normalized log probability.\n\n\n\nDifferentiable\n\nassess - same as above.\nunzip - given a set of fixed constraints, return two callables. The first callable score accepts constraints which fill in the complement of the fixed constraints and arguments, and returns the normalized log probability of all the constraints. The second callable retval accepts constraints and arguments, and returns the return value for the generative function call consistent with the constraints and given arguments.\n\nunzip produces two functions which can be compositionally used with jax.grad to evaluate gradients used by both differentiable and involutive MCMC and SMC."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#more-about-generative-functions",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#more-about-generative-functions",
    "title": "Introduction to Gen and GenJAX",
    "section": "More about generative functions",
    "text": "More about generative functions\nHere are a few more bits of information which should help you gain context with these objects.\n\nDistributions are generative functions\nIn GenJAX, distributions are generative functions.\n\nkey, tr = genjax.simulate(genjax.Normal)(key, (0.0, 1.0))\ntr\n\n\n\n\nDistributionTrace\n├── gen_fn\n│   └── _Normal\n├── args\n│   └── (0.0, 1.0)\n├── value\n│   └── ValueChoiceMap\n│       └── value\n│           └── f32[]\n└── score\n    └── f32[]\n\n\n\nThis should bring a sigh of relief! Ah, distributions are generative functions - the concepts can’t be too exotic.\nDistributions implement the interface in a conceptually simple way. They don’t have internal compositional choice structure (like the function-like BuiltinGenerativeFunction language above).\nDistributions themselves expose two interfaces:\n\nlogpdf - exact density evaluation.\nsample - exact sampling.\n\nWe can use these two interfaces to implement all the generative function interfaces for distributions.\n\n\nAssociated data types\n\nChoice maps are the tree-like recordings of random choices in a trace.\nSelection is an object which allows querying a trace/choice map - selecting certain choices.\n\n\n@genjax.gen\ndef h(key, x):\n    key, m1 = genjax.trace(\"m0\", genjax.Bernoulli)(key, x)\n    key, m2 = genjax.trace(\"m1\", genjax.Bernoulli)(key, x)\n    return (key, m1 + m2)\n\n\nkey, tr = genjax.simulate(h)(key, (0.3,))\ntr\n\n\n\n\nBuiltinTrace\n├── gen_fn\n│   └── BuiltinGenerativeFunction\n│       └── source\n│           └── <function h>\n├── args\n│   └── (0.3,)\n├── retval\n│   └── bool[]\n├── choices\n│   └── BuiltinChoiceMap\n│       └── inner\n│           └── HashableDict\n│               ├── m0\n│               │   └── DistributionTrace\n│               │       ├── gen_fn\n│               │       │   └── _Bernoulli\n│               │       ├── args\n│               │       │   └── (0.3,)\n│               │       ├── value\n│               │       │   └── ValueChoiceMap\n│               │       │       └── value\n│               │       │           └── bool[]\n│               │       └── score\n│               │           └── f32[]\n│               └── m1\n│                   └── DistributionTrace\n│                       ├── gen_fn\n│                       │   └── _Bernoulli\n│                       ├── args\n│                       │   └── (0.3,)\n│                       ├── value\n│                       │   └── ValueChoiceMap\n│                       │       └── value\n│                       │           └── bool[]\n│                       └── score\n│                           └── f32[]\n├── cache\n│   └── BuiltinTrie\n└── score\n    └── f32[]\n\n\n\n\nselection = genjax.select([\"m1\"])\nselected, _ = selection.filter(tr.get_choices())\nselected\n\n\n\n\nBuiltinChoiceMap\n└── inner\n    └── HashableDict\n        └── m1\n            └── DistributionTrace\n                ├── gen_fn\n                │   └── _Bernoulli\n                ├── args\n                │   └── (0.3,)\n                ├── value\n                │   └── ValueChoiceMap\n                │       └── value\n                │           └── bool[]\n                └── score\n                    └── f32[]"
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#great-what-can-i-do-with-them",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#great-what-can-i-do-with-them",
    "title": "Introduction to Gen and GenJAX",
    "section": "Great … what can I do with them?",
    "text": "Great … what can I do with them?\nNow, we’ve informally seen the interfaces and datatypes associated with generative functions.\nStudying the interfaces (and improvements thereof), as well as the computational objects which satisfy them can be an entire PhD’s worth of effort.\nIn the remainder of this notebook, let’s see how we can do machine learning with them.\nLet’s consider a modeling problem where we wish to perform generalized regression with outliers between two variates, taking a family of polynomials as potential curves.\nOne such model for this data generating process is shown below.\n\n# Two branches for a branching submodel.\n@genjax.gen\ndef model_y(key, x, coefficients):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    key, y = trace(\"value\", genjax.Normal)(key, polynomial_value, 0.3)\n    return key, y\n\n\n@genjax.gen\ndef outlier_model(key, x, coefficients):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    key, y = trace(\"value\", genjax.Normal)(key, polynomial_value, 30.0)\n    return key, y\n\n\n# The branching submodel.\nswitch = genjax.SwitchCombinator([model_y, outlier_model])\n\n# A mapped kernel function which calls the branching submodel.\n@genjax.gen(genjax.MapCombinator, in_axes=(0, 0, None))\ndef kernel(key, x, coefficients):\n    key, is_outlier = trace(\"outlier\", genjax.Bernoulli)(key, 0.1)\n    is_outlier = jnp.asarray(is_outlier, dtype=int)\n    key, y = trace(\"y\", switch)(key, is_outlier, x, coefficients)\n    return key, y\n\n\n@genjax.gen\ndef model(key, xs):\n    key, coefficients = trace(\"alpha\", genjax.MvNormal)(\n        key, np.zeros(3), 2.0 * np.identity(3)\n    )\n    key, *sub_keys = jax.random.split(key, len(xs) + 1)\n    sub_keys = jnp.array(sub_keys)\n    _, ys = trace(\"ys\", kernel)(sub_keys, xs, coefficients)\n    return key, ys\n\nThere’s a number of implementation patterns which you might pick up on by studying this model.\n\nGenerative functions explicitly pass a PRNG key in and out. This conforms to JAX’s PRNG usage expectations.\nTo implement control flow, we use higher-order functions called combinators. These accept generative functions as input, and return generative functions as output.\nAny JAX compatible code is allowed in the body of a generative function.\n\nCourtesy of the interface, we get to design our model generative function in pieces.\nNow, let’s examine the sampled observation address (\"ys\", \"y\") from a sample trace from our model.\n\ndata = jnp.arange(0, 10, 0.5)\nkey, tr = jax.jit(model.simulate)(key, (data,))\ntr[\"ys\", \"y\"]\n\n\n\n\nSwitchTrace\n├── gen_fn\n│   └── SwitchCombinator\n│       └── branches\n│           └── [\n│                 BuiltinGenerativeFunction(source = <function model_y>),\n│                 BuiltinGenerativeFunction(source = <function outlier_model>)\n│               ]\n├── chm\n│   └── IndexedChoiceMap\n│       ├── index\n│       │   └── i32[20]\n│       ├── BuiltinTrace\n│       │   ├── gen_fn\n│       │   │   └── BuiltinGenerativeFunction\n│       │   │       └── source\n│       │   │           └── <function model_y>\n│       │   ├── args\n│       │   │   └── (f32[20], f32[20,3])\n│       │   ├── retval\n│       │   │   └── f32[20]\n│       │   ├── choices\n│       │   │   └── BuiltinChoiceMap\n│       │   │       └── inner\n│       │   │           └── HashableDict\n│       │   │               └── value\n│       │   │                   └── DistributionTrace\n│       │   │                       ├── gen_fn\n│       │   │                       │   └── _Normal\n│       │   │                       ├── args\n│       │   │                       │   └── (f32[20], f32[20])\n│       │   │                       ├── value\n│       │   │                       │   └── ValueChoiceMap\n│       │   │                       │       └── value\n│       │   │                       │           └── f32[20]\n│       │   │                       └── score\n│       │   │                           └── f32[20]\n│       │   ├── cache\n│       │   │   └── BuiltinTrie\n│       │   └── score\n│       │       └── f32[20]\n│       └── BuiltinTrace\n│           ├── gen_fn\n│           │   └── BuiltinGenerativeFunction\n│           │       └── source\n│           │           └── <function outlier_model>\n│           ├── args\n│           │   └── (f32[20], f32[20,3])\n│           ├── retval\n│           │   └── f32[20]\n│           ├── choices\n│           │   └── BuiltinChoiceMap\n│           │       └── inner\n│           │           └── HashableDict\n│           │               └── value\n│           │                   └── DistributionTrace\n│           │                       ├── gen_fn\n│           │                       │   └── _Normal\n│           │                       ├── args\n│           │                       │   └── (f32[20], f32[20])\n│           │                       ├── value\n│           │                       │   └── ValueChoiceMap\n│           │                       │       └── value\n│           │                       │           └── f32[20]\n│           │                       └── score\n│           │                           └── f32[20]\n│           ├── cache\n│           │   └── BuiltinTrie\n│           └── score\n│               └── f32[20]\n├── args\n│   └── (f32[20], f32[20,3])\n├── retval\n│   └── f32[20]\n└── score\n    └── f32[20]\n\n\n\nHere, I’m just showing the subtrace from the switching model invocation - but it is already quite large and unwieldy!\nBesides, it also doesn’t tell us much about the values we truly care about here - the sampled values.\nFrom this model, we can get these in two ways.\nThe first way: we can just look at the trace return value.\n\ntr.get_retval()\n\nDeviceArray([   1.7448314 ,    0.41097283,   -1.9558886 ,   -5.498426  ,\n               -9.625496  ,  -15.6570425 ,  -22.74233   ,  -30.96311   ,\n              -30.182352  ,  -64.04883   ,  -62.844086  ,  -75.89779   ,\n              -90.39219   , -105.64662   , -121.877945  , -140.97981   ,\n             -159.82722   , -180.46971   , -207.86064   , -224.98926   ],            dtype=float32)\n\n\n\nThe second way: we can get them out of the choice map of the trace directly.\n\nchm = tr.get_choices()\nchm[\"ys\", \"y\", \"value\"]\n\nDeviceArray([   1.7448314 ,    0.41097283,   -1.9558886 ,   -5.498426  ,\n               -9.625496  ,  -15.6570425 ,  -22.74233   ,  -30.96311   ,\n              -30.182352  ,  -64.04883   ,  -62.844086  ,  -75.89779   ,\n              -90.39219   , -105.64662   , -121.877945  , -140.97981   ,\n             -159.82722   , -180.46971   , -207.86064   , -224.98926   ],            dtype=float32)\n\n\n\nNow, let’s construct a small visualization function to show us the samples.\n\ndef viz(ax, x, y, **kwargs):\n    (data,) = tr.get_args()\n    chm = tr.get_choices()\n    ys = np.array(chm[\"ys\", \"y\", \"value\"])\n    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n\n\nf, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True, dpi=280)\njitted = jax.jit(model.simulate)\nfor ax in axes.flatten():\n    key, tr = jitted(key, (data,))\n    x = data\n    y = tr.get_retval()\n    viz(ax, x, y, marker=\".\")\n\nplt.show()\n\n\n\n\nHere’s 9 samples from our model.\nWe just walked through one of the main elements of probabilistic programming: setting up a program, which represents a joint distribution over random variates, some of which we’ll identify with data we expect to see in the world.\nWe can adjust the noise settings of our model to produce wider priors over possible sets of points - and we may want to do this if our data is noisy!\nFor now, let’s keep the settings as is, and explore inference in GenJAX."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#your-first-inference-program",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#your-first-inference-program",
    "title": "Introduction to Gen and GenJAX",
    "section": "Your first inference program",
    "text": "Your first inference program\nNow, let’s say we have some data.\n\nx = np.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\ny = 2.0 * x + 1.5 + x**2\ny[2] = 50.0\n\n\nfig_data, ax_data = plt.subplots(figsize=(3, 3), dpi=140)\nviz(ax_data, x, y, color=\"blue\")\n\n\n\n\nIn Bayesian inference, if we wish to consider the conditional distribution \\(P(\\tau, r; x | \\text{data})\\) induced from a model \\(P(\\tau, \\text{data}, r; x)\\) - Bayes’ rule gives us a way to compute it.\n\\[\nP(\\tau, r; x | \\text{data}) = \\frac{P(\\tau, \\text{data}, r; x)}{\\int P(\\tau, \\text{data}, r; x) \\ d\\tau \\ dr}\n\\]\nThe problem is that we often cannot compute the denominator (the evidence integral) easily. Instead, we turn to approximate Bayesian inference.\nDepending on how we wish to use the LHS conditional (which is called the posterior in Bayesian inference) - we have different options available to us.\nIf we wish to approximately sample from the posterior, to get an empirical sense of its shape and properties, we will often utilize techniques which provide exact samplers for another distribution which gets asymptotically close to the target posterior if we increase certain hyperparameters.\nOne such algorithm is importance sampling, and that’s what we’ll write today.\nHere’s importance sampling (with a single sample) without a custom proposal in GenJAX.\n\nobservations = genjax.choice_map(\n    {\"ys\": genjax.vector_choice_map(genjax.choice_map({(\"y\", \"value\"): y}))}\n)\nmodel_args = (x,)\nkey, (w, tr) = model.importance(key, observations, model_args)\n\nWe’re introduced to another interface method!\nimportance accepts a PRNG key, a choice map representing observations (sometimes called constraints), and model arguments. It returns a new evolved PRNG key, and a tuple contained a log importance weight and a trace.\nThe trace is consistent with the arguments and constraints passed into the invocation.\n\nobservations[\"ys\", \"y\", \"value\"]\n\narray([ 2.19,  3.39, 50.  ,  6.26, 11.39, 12.75, 16.5 , 25.5 , 36.5 ])\n\n\n\n\ntr[\"ys\", \"y\", \"value\"]\n\nDeviceArray([ 2.19,  3.39, 50.  ,  6.26, 11.39, 12.75, 16.5 , 25.5 ,\n             36.5 ], dtype=float32)\n\n\n\nLet’s examine the weight now, and compare it to the score.\n\n(w, tr.get_score())\n\n(DeviceArray(-10115.872, dtype=float32), DeviceArray(-10121.649, dtype=float32))\n\n\n\nNotice that these two quantities are different.\nRemember: the score is the normalized log density of the choice map measure evaluated at the complete set of trace constraints. We’ll refer to complete traces by \\(\\tau\\).\nThe log importance weight w is slightly different.\n\nImportance sampling … informally\nLet’s discuss how importance sampling works first6.6 In this notebook, I’ll defer discussing formal proofs concerning the asymptotic consistence of posterior estimators derived from importance sampling.\nThis will provide us with an understanding as to why w is different from the score.\nMore importantly, we’ll understand how we can use importance to solve the inference task of approximately sampling from the posterior over coefficients (and ultimately, over curves) from our generative function.\n\n\n\n\n\n\nImportance sampling is typically presented by focusing on posterior expectations \\(E_{x \\sim P(x | y)}[f(x)]\\).\nIn our case, we want to sample \\(x \\sim P(x | y)\\). To do this, we’ll actually be considering a different procedure called sampling importance resampling or SIR for short.\nImportantly, importance sampling is a subroutine in SIR.\nWe’ll discuss why importance sampling works here, and provide references to why SIR works to solve our problem.\n\n\n\nLet’s start by considering two distributions which we can sample from, and evaluate densities.\nBelow, I’m plotting the densities of two distributions - a 1D Gaussian mixture and a 1D Gaussian7.7 GenJAX allows usage of TensorFlow Distributions as generative functions. Here, we’re just using the logpdf interface from distributions which expose exact logpdf evaluation - but genjax exports a wrapper which implements the complete generative function interface.\n\nmix = genjax.TFPMixture(\n    genjax.TFPCategorical, [genjax.TFPNormal, genjax.TFPNormal]\n)\nmix_args = ([0.5, 0.5], [(-3.0, 0.8), (1.0, 0.3)])\nd = genjax.TFPNormal\nd_args = (0.0, 1.0)\n\n\nfig, ax = plt.subplots(figsize=(8, 8), dpi=280)\nevaluation_points = np.arange(-5, 5, 0.01)\n\n\ndef plot_logpdf(ax, logpdf_fn, evaluation_points, **kwargs):\n    logpdfs = jax.vmap(logpdf_fn)(evaluation_points)\n    ax.scatter(evaluation_points, jnp.exp(logpdfs), marker=\".\", **kwargs)\n\n\nd_logpdf = lambda v: d.logpdf(v, *d_args)\nmix_logpdf = lambda v: mix.logpdf(v, *mix_args)\n\nplot_logpdf(\n    ax, d_logpdf, evaluation_points, color=\"red\", label=\"1D Gaussian PDF\"\n)\nplot_logpdf(\n    ax,\n    mix_logpdf,\n    evaluation_points,\n    color=\"blue\",\n    label=\"1D Gaussian mixture PDF\",\n)\nax.legend()\n\n\n\n\nTo gain context on importance sampling, imagine that the distribution which produces the blue curve is difficult to sample from - but it exposes a logpdf interface which we can use to evaluate the density at any point on the support of the distribution.\nNow, suppose you hand me the distribution which made the red curve - and it is easy to sample from, and it also exposes a logpdf interface.\nOne thing we could do is sample from the red curve and then “correct” for the fact that we’re sampling from the wrong distribution.\nThis is the key intuition behind importance sampling.\nNow, I’m going to write a procedure and ask you to just go with it … for a moment.\n\ndef importance_sample(hard, easy):\n    def _inner(key, hard_args, easy_args):\n        key, sub_key = jax.random.split(key)\n        sample = easy.sample(sub_key, *easy_args)\n        easy_logpdf = easy.logpdf(sample, *easy_args)\n        hard_logpdf = hard.logpdf(sample, *hard_args)\n        importance_weight = hard_logpdf - easy_logpdf\n        return key, (importance_weight, sample)\n\n    return _inner\n\n\nhard = genjax.TFPMixture(\n    genjax.TFPCategorical, [genjax.TFPNormal, genjax.TFPNormal]\n)\neasy = genjax.TFPNormal\njitted = jax.jit(importance_sample(hard, easy))\nkey, (importance_weight, sample) = jitted(key, mix_args, d_args)\n\n\n(importance_weight, sample)\n\n(DeviceArray(-4.0058784, dtype=float32), DeviceArray(-0.78240925, dtype=float32))\n\n\n\nNow, we can easily run this procedure many times in parallel.\n\njitted = jax.jit(\n    jax.vmap(importance_sample(hard, easy), in_axes=(0, None, None))\n)\n\n\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n_, (importance_weight, sample) = jitted(sub_keys, mix_args, d_args)\n\n\nimportance_weight\n\nDeviceArray([-6.2341752e+00,  6.2633801e-01, -3.3759670e+00,\n             -5.9383879e+00, -9.7561479e-01, -5.3102112e+00,\n             -5.1128283e+00, -2.1514070e+00,  4.5841384e-01,\n             -4.0724092e+00, -1.8354319e+00, -5.5122299e+00,\n             -6.2061067e+00, -2.1719730e-01, -3.5693016e+00,\n             -6.1405115e+00, -5.3810544e+00, -6.2284150e+00,\n              7.8095734e-01, -3.8515360e+00, -2.8184617e-01,\n             -1.8381605e+00, -7.5211000e+00,  1.0577252e+00,\n              1.0453278e+00,  1.0218762e+00,  9.8658347e-01,\n              9.5912194e-01, -1.9758319e+00,  1.0580635e+00,\n             -4.1381865e+00, -2.3692460e+00,  1.0351460e+00,\n             -2.5306203e+00,  6.6269362e-01,  6.0733199e-02,\n             -1.1496689e+00, -6.2260799e+00, -6.1138239e+00,\n              4.6291578e-01, -4.1340222e+00,  8.6949825e-01,\n             -2.1810038e+00, -2.9703891e+00, -7.1519613e-03,\n             -1.2844566e+00, -2.3207369e+00, -2.3978152e+00,\n             -1.0548264e+00, -5.6061153e+00, -5.9159632e+00,\n             -3.1040215e-01, -1.5413404e-01, -1.2319851e-01,\n             -1.4652884e+00, -5.0214028e-01,  6.9174856e-01,\n             -5.8656144e+00, -5.3170371e+00,  6.0201246e-01,\n             -1.6653061e-02, -3.0496669e+00, -2.3092539e+00,\n             -2.4331427e+00,  1.0071430e+00, -5.6184292e+00,\n             -1.1847696e+00, -3.5587354e+00, -5.8828015e+00,\n             -3.7577522e+00, -3.6165695e+00, -5.5585594e+00,\n             -1.7351907e+00, -6.7679203e-01,  1.9070494e-01,\n             -3.9766405e+00,  5.7461774e-01, -1.0918140e-02,\n             -2.6561151e+00,  4.9570715e-01,  6.3730204e-01,\n              9.7846591e-01, -3.9247353e+00,  1.0440333e+00,\n             -6.2158747e+00, -2.4731362e+00, -4.5478611e+00,\n              1.0888841e+00, -7.3868978e-01, -4.2448354e+00,\n              9.5782554e-01,  3.7766671e-01, -6.2152562e+00,\n             -5.2021756e+00, -5.1431012e+00, -1.7538964e+00,\n             -4.6911263e+00, -7.5708902e-01, -4.1147037e+00,\n              9.9778765e-01], dtype=float32)\n\n\n\nWe’re just sampling from easy, then scoring the samples with importance_weight according to the log ratio easy_logpdf(sample) - hard_logpdf(sample).\nHere’s the trick - from our collection of samples and weights, let’s normalize the weights into a distribution and sample a single sample to return using it.\n\ndef sampling_importance_resampling(hard, easy, n_samples):\n    def _inner(key, hard_args, easy_args):\n        fn = importance_sample(hard, easy)\n        key, *sub_keys = jax.random.split(key, n_samples + 1)\n        sub_keys = jnp.array(sub_keys)\n        vmapped = jax.vmap(fn, in_axes=(0, None, None))\n        _, (ws, samples) = vmapped(sub_keys, hard_args, easy_args)\n        logits = ws\n        key, sub_key = jax.random.split(key)\n        index = genjax.TFPCategorical.sample(sub_key, logits)\n        final_sample = samples[index]\n        return key, final_sample\n\n    return _inner\n\n\nhard = genjax.TFPMixture(\n    genjax.TFPCategorical, [genjax.TFPNormal, genjax.TFPNormal]\n)\neasy = genjax.TFPNormal\njitted = jax.jit(sampling_importance_resampling(hard, easy, 100))\nkey, sample = jitted(key, mix_args, d_args)\n\n\nsample\n\nDeviceArray(-2.556716, dtype=float32)\n\n\n\nLet’s run this procedure a bunch of times and plot the points on the x-axis of our plot above.\n\ndef plot_on_x(ax, x, **kwargs):\n    ax.scatter(x, np.zeros_like(x), **kwargs)\n\n\nkey, *sub_keys = jax.random.split(key, 1000 + 1)\nsub_keys = jnp.array(sub_keys)\nfn = sampling_importance_resampling(hard, easy, 1000)\njitted = jax.jit(jax.vmap(fn, in_axes=(0, None, None)))\n_, samples = jitted(sub_keys, mix_args, d_args)\nplot_on_x(ax, samples, color=\"gold\", marker=\".\", alpha=0.05)\nfig\n\n\n\n\nNotice what happens with the SIR samples (in gold)?\nThey accumulate around the places you’d expect to see if you were sampling from the hard distribution!\nThat’s what importance sampling and sampling importance sampling give us - we provide a “hard” distribution with a logpdf interface, and another “easy” distribution with sample and logpdf interface8, and SIR returns an exact sampler for a distribution which approximates the hard distribution.8 There are more constraints. The second distribution must be absolutely continuous in measure with respect to the first. Let’s defer this discussion to a formal treatment of importance sampling.\n\n\nBack to our generative function\nNow that we’ve seen the ingredients and implementation of importance sampling and sampling importance resampling - let’s return to our original problem.\n\nfig_data\n\n\n\n\nIf you studied the previous section careful - one question might jump out at you: what is the “easy” distribution for model.importance?\n\nBuiltin proposals\nGenerative functions defined using the BuiltinGenerativeFunction language come with builtin proposals - it’s a distribution (which we’ll refer to as \\(Q\\)) induced from the prior, with sampling and score defined ancestrally.\nGive observation constraints \\(u\\), the importance weight which model.importance computes is9:9 This definition again considers “untraced randomness” \\(r\\). If you wish to ignore this in the math, just remove the \\(Q(r; x, \\tau)\\) term. Even in the presence of untraced randomness, the weights which Gen computes are asymptotically consistent in expectation over \\(Q(r; x, \\tau)\\)\n\\[\n\\begin{align}\n\\log w &= \\log P(\\tau, r; x) - \\log Q(\\tau; u, x)Q(r; x, \\tau) \\\\\n\\end{align}\n\\]\nFor the BuiltinGenerativeFunction language, we implement \\(Q\\) by invoking the generative function - when we arrive at a constrained address, we recursively called submodel.importance - accumulate the log weight, as well as the log score.\nNow, if an address has no constraints - we get 0.0 for the weight (think about why this is by looking at the above equation and asking what happens when \\(Q\\) has to generate a full \\(\\tau\\)). However, we still get a score.\n\n\nSequential importance resampling in GenJAX\nHere’s SIR using builtin proposals (just a single call to model.importance) in GenJAX10:10 To implement a variant with custom proposals, all we need to do is first proposal.simulate, merge the proposal choice map with the constraints, then model.importance followed by a final weight adjustment w = w - proposal_tr.get_score() - easy peasy.\n\ndef sampling_importance_resampling(model, n_samples):\n    def _inner(key, observations, model_args):\n        key, *sub_keys = jax.random.split(key, n_samples + 1)\n        sub_keys = jnp.array(sub_keys)\n        vmapped = jax.vmap(model.importance, in_axes=(0, None, None))\n        _, (lws, trs) = vmapped(sub_keys, observations, model_args)\n        key, sub_key = jax.random.split(key)\n        index = genjax.TFPCategorical.sample(sub_key, lws)\n        final_tr = jtu.tree_map(lambda v: v[index], trs)\n        return key, final_tr\n\n    return _inner\n\nOne difference between our first implementation (on just distributions) above and this one is that Trace instances are structured objects (but all of them are Pytree implementors) - meaning we need to index into the leaves when we wish to return a single sampled trace.\n\nmodel_args = (x,)\njitted = jax.jit(\n    jax.vmap(\n        sampling_importance_resampling(model, 100), in_axes=(0, None, None)\n    )\n)\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n_, samples = jitted(sub_keys, observations, model_args)\ncoefficients = samples[\"alpha\"]\n\nSo now we have an approximate sampler for the posterior and we can use it to look at properties of the posterior - like what sort of curves are likely given our data and our model prior.\n… and by the way, to get a representative set of samples from the posterior for this model, on an Apple M2 device - only takes about 0.05 seconds11.11 Just remember: we’re running this notebook on CPU - but the resulting specialized inference code can easily be moved to accelerators, courtesy of the fact that all our code is JAX traceable.\n\n%%timeit\n_, samples = jitted(sub_keys, observations, model_args)\n\n57 ms ± 3.25 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndef polynomial_at_x(x, coefficients):\n    basis_values = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(coefficients * basis_values)\n    return polynomial_value\n\n\njitted = jax.jit(jax.vmap(polynomial_at_x, in_axes=(None, 0)))\n\n\ndef plot_polynomial_values(ax, x, coefficients, **kwargs):\n    v = jitted(x, coefficients)\n    ax.scatter(np.repeat(x, len(v)), v, **kwargs)\n\n\ncoefficients = samples[\"alpha\"]\nevaluation_points = np.arange(0, 5, 0.01)\nfor data in evaluation_points:\n    plot_polynomial_values(\n        ax_data, data, coefficients, marker=\".\", color=\"gold\", alpha=0.005\n    )\nfig_data\n\n\n\n\nIntuitively, this makes a lot of sense. Our prior over polynomials considers a wide range of curves - but, if our approximate sampling process is trusted, we’re correctly seeing what we should expect to happen if we observed this data - polynomials with the coefficients shown above tend to be sampled more under the posterior.\nWe can also ask for an estimate of the posterior probability that any particle point was an outlier.\nFor example, below is the set of samples projected onto the (\"ys\", \"outlier\") address for the point which we manually set to be quite far from the curve.\n\nposterior_outlier = samples[\"ys\", \"outlier\"][:, 2]\nposterior_outlier\n\nDeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True, False,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True,  True,  True,  True,  True,\n              True,  True,  True,  True], dtype=bool)\n\n\n\n\nnp.sum(posterior_outlier) / len(posterior_outlier)\n\nDeviceArray(0.99, dtype=float32)\n\n\n\nThat seems to make sense! We pulled that point quite far away from ground truth curve - so we’d expect that point 2 is considered an outlier under the true posterior."
  },
  {
    "objectID": "introduction/intro_to_genjax/intro_to_genjax.html#summary",
    "href": "introduction/intro_to_genjax/intro_to_genjax.html#summary",
    "title": "Introduction to Gen and GenJAX",
    "section": "Summary",
    "text": "Summary\nWe’ve covered a lot of ground in this notebook. Please reflect, re-read, and post issues!\n\nWe discussed the Gen probabilistic programming framework, and discussed GenJAX - an implementation of Gen on top of JAX.\nWe discussed generative functions - the main computational object of Gen.\nWe discussed how to create generative functions using generative function languages, and several of GenJAX’s builtin capabilities for constructing generative functions.\nWe discussed how to use generative functions to represent joint probability distributions, which can be used to construct models of phenomena.\nWe created a generative function to model a data-generating process based on sampling and evaluating random polynomials at input data - to represent a typical regression task.\nWe discussed how to formulate questions about induced conditional distributions under a probabilistic model as a Bayesian inference problem.\nWe discussed importance sampling and sampling importance resampling, two central techniques in approximate Bayesian inference.\nWe created a sampling importance resampling routine and applied it to produce approximate posterior samples from the posterior in the our polynomial generating model.\nWe investigated the approximate posterior samples, and visually inspected that they match the inferences that we might draw - both for the polynomials we expected to produce the data, as well as what data points might be outliers.\n\nThis is just the beginning! There’s a lot more to learn, but plenty to bite off with this initial notebook."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling and inference notebooks",
    "section": "",
    "text": "This is a notebook repository for pedagogical tutorials on the usage on Gen, with a focus on the GenJAX implementation."
  }
]