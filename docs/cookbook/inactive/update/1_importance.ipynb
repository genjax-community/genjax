{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Importance\n",
    "subtitle: Intro to the `update` logic\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import gen, normal, pretty\n",
    "\n",
    "pretty()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important building block of the library is the `update` method. Before investigating its details, let's look at the more user-friendly version called `importance`.\n",
    "\n",
    "`importance` is a method on generative functions. It takes a key, constraints in the form of a choicemap, and arguments for the generative function.  Let's first see how we use it and then explain what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def model(x):\n",
    "    y = normal(x, 1.0) @ \"y\"\n",
    "    z = normal(y, 1.0) @ \"z\"\n",
    "    return y + z\n",
    "\n",
    "\n",
    "constraints = C.n()\n",
    "args = (1.0,)\n",
    "key, subkey = jax.random.split(key)\n",
    "tr, w = model.importance(subkey, constraints, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a pair of a trace `tr` and a weight `w`. `tr` is produced by the model, and its choicemap satisfies the constraints given by `constraints`. \n",
    "\n",
    "For the choices that are not constrained, they are sampled from the prior distribution given by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expect normal(0., 1.) for y and constant 4. for z\n",
    "constraints = C[\"z\"].set(4.0)\n",
    "args = (0.0,)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, 100000)\n",
    "trs, ws = jax.vmap(lambda key: model.importance(key, constraints, args))(keys)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ys = trs.get_choices()[\"y\"]\n",
    "zs = trs.get_choices()[\"z\"]\n",
    "plt.hist(ys, bins=200, density=True, alpha=0.5, color=\"b\", label=\"ys\")\n",
    "plt.scatter(zs, np.zeros_like(zs), color=\"r\", label=\"zs\")\n",
    "plt.title(\"Gaussian Distribution of ys and Constant z\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights computed represent the ratio $\\frac{P(y, 4. ; x)}{P(y ; x)}$ where $P(y, z ; x)$ is the joint density given by the model at the argument $x$, and $P(y ; x)$ is the density of the subpart of the model that does not contain the constrained variables. As \"z\" is constrained in our example, it only leaves \"y\". \n",
    "\n",
    "We can easily check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerators, _ = jax.vmap(lambda y: model.assess(C[\"y\"].set(y) ^ C[\"z\"].set(4.0), args))(\n",
    "    ys\n",
    ")\n",
    "\n",
    "denominators = trs.get_subtrace((\"y\",)).get_score()\n",
    "\n",
    "# yeah, numerical stability of floats implies it's not even exactly equal ...\n",
    "jnp.allclose(ws, numerators - denominators, atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally the denominator is the joint on the sampled variables (the constraints are not sampled) and Gen has a way to automatically sampled from the generative function obtained by replacing the sampling operations of the constrained addresses by the values of the constraints. For instance in our example it would mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def constrained_model(x):\n",
    "    y = normal(x, 1.0) @ \"y\"\n",
    "    z = 4.0\n",
    "    return y + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the factorisation $P(y, z ; x) = P(y ; x)P(z | y ; x)$, the weight `ws` simplifies to $P(z | y ; x)$.\n",
    "In fact we can easily check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws == trs.get_subtrace((\"z\",)).get_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this time the equality is exact as this is how `importance` computes it. The algebraic simplification $\\frac{P(y ; x)}{P(y ; x)}=1$ is done automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review. `importance` completes a set of constraints given by a partial choicemap to a full choicemap under the model. It also efficiently computes a weight which simplifies to a distribution of the form $P(\\text{sampled } | \\text{ constraints} ; \\text{arguments})$.\n",
    "\n",
    "The complex recursive nature of this formula becomes a bit more apparent in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def fancier_model(x):\n",
    "    y1 = normal(x, 1.0) @ \"y1\"\n",
    "    z1 = normal(y1, 1.0) @ \"z1\"\n",
    "    y2 = normal(z1, 1.0) @ \"y2\"\n",
    "    z2 = normal(z1 + y2, 1.0) @ \"z2\"\n",
    "    return y2 + z2\n",
    "\n",
    "\n",
    "# if we constraint `z1` to be 4. and `z2` to be 2. we'd get a constrained model as follows:\n",
    "\n",
    "\n",
    "@gen\n",
    "def constrained_fancier_model(x):\n",
    "    y1 = normal(x, 1.0) @ \"y1\"\n",
    "    z1 = 4.0\n",
    "    y2 = normal(z1, 1.0) @ \"y2\"  # note how the sampled `y2` depends on a constraint\n",
    "    z2 = 2.0\n",
    "    return y1 + z1 + y2 + z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what does this have to do this importance sampling?\n",
    "\n",
    "What we effectively did was to sample a value `y` from the distribution `constrained_model`, which is called a proposal in importance sampling, often noted $q$. We then computed the weight $\\frac{p(y)}{q(y)}$ under some model $p$. \n",
    "Given that we constrained `z`, an equivalent view is that we observed `z` and we have a posterior inference problem: we want to approximately sample from the posterior $P(y | z)$ (all for a given argument `x`). \n",
    "\n",
    "Note that $P(y | z) = \\frac{P(y,z)}{P(z)}$ by Bayes rule. \n",
    "So our fraction $\\frac{P(y, z ; x)}{P(y ; x)}$ for the weight rewrites as $\\frac{P(y | z)P(z)}{q(y)}= P(z)\\frac{p(y)}{q(y)}$ (1).\n",
    "\n",
    "Also remember that the weight $\\frac{dp}{dq}$ for importance comes from the proper weight guarantee, i.e. it satisfies this equation: $$\\forall f.\\mathbb{E}_{y\\sim p}[f(y)]= \\mathbb{E}_{y\\sim q}[\\frac{dp}{dq}(y)f(y)] =  \\frac{1}{p(z)} \\mathbb{E}_{y\\sim q}[w(y)f(y)] $$\n",
    "\n",
    "where in the last step we used (1) and called `w` the weight computed by `importance`.\n",
    "\n",
    "By taking $f:= \\lambda y.1$, we derive that $p(z) = \\mathbb{E}_{y\\sim q}[w(y)]$. That is, by sampling from our proposal distribution, we can estimate the marginal $p(z)$. Theferore with the same samples we can estimate any quantity $\\mathbb{E}_{y\\sim p}[f(y)]$ using our estimate of $\\mathbb{E}_{y\\sim q}[w(y)f(y)]$ and our estimate of $p(z)$. That's the essence of self-normalizing importance sampling.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
