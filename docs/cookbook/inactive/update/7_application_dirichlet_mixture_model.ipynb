{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block-Gibbs on Dirichlet Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see some of the key ingredients in action in a simple but more realistic setting and write a Dirichlet mixture model in GenJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Points on the Real Line\n",
    "\n",
    "The goal here is to cluster datapoints on the real line. To do so, we model a fixed number of clusters, each as a 1D-Gaussian with fixed variance, and we want to infer their means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "The \"model of the world\" postulates:\n",
    "- A fixed number of 1D Gaussians\n",
    "- Each Gaussian is assigned a weight, representing the proportion of points assigned to each cluster \n",
    "- Each datapoint is assigned to a cluster\n",
    "\n",
    "### Generative Process\n",
    "\n",
    "We turn this into a generative model as follows:\n",
    "- We have a fixed prior mean and variance for where the cluster centers might be\n",
    "- We sample a mean for each cluster\n",
    "- We sample an initial weight per cluster (sum of weights is 1)\n",
    "- For each datapoint:\n",
    "  - We sample a cluster assignment proportional to the cluster weights\n",
    "  - We sample the datapoint noisily around the mean of the cluster\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We, the modelers, get to choose how this process is implemented. \n",
    "- We choose distributions for each sampling step in a way that makes **inference tractable**.\n",
    "- More precisely, we choose conjugate pairs so that we can do inference via Gibbs sampling. \n",
    "  - Gibbs sampling is an MCMC method that samples an initial trace, and then updates the traced choices we want to infer over time. \n",
    "  - To update a choice, Gibbs sampling samples from a conditional distribution, which is tractable with conjugate relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genstudio.plot as Plot\n",
    "import imageio\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import categorical, dirichlet, gen, normal, pretty\n",
    "from genjax._src.core.pytree import Const\n",
    "\n",
    "pretty()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the generative model we described above. It has several hyperparameters that are somewhat manually inferred. An extension to the model could instead do inference over these hyperparameters, and fix hyper-hyperparameters instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "PRIOR_VARIANCE = 10.0\n",
    "OBS_VARIANCE = 1.0\n",
    "N_DATAPOINTS = 5000\n",
    "N_CLUSTERS = 40\n",
    "ALPHA = float(N_DATAPOINTS / (N_CLUSTERS * 10))\n",
    "PRIOR_MEAN = 50.0\n",
    "N_ITER = 50\n",
    "\n",
    "# Debugging mode\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "# Sub generative functions of the bigger model\n",
    "@gen\n",
    "def generate_cluster(mean, var):\n",
    "    cluster_mean = normal(mean, var) @ \"mean\"\n",
    "    return cluster_mean\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster_weight(alphas):\n",
    "    probs = dirichlet(alphas) @ \"probs\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_datapoint(probs, clusters):\n",
    "    idx = categorical(jnp.log(probs)) @ \"idx\"\n",
    "    obs = normal(clusters[idx], OBS_VARIANCE) @ \"obs\"\n",
    "    return obs\n",
    "\n",
    "\n",
    "# Main model\n",
    "@gen\n",
    "def generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):\n",
    "    clusters = (\n",
    "        generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)\n",
    "        @ \"clusters\"\n",
    "    )\n",
    "\n",
    "    probs = generate_cluster_weight.inline(\n",
    "        alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())\n",
    "    )\n",
    "\n",
    "    datapoints = (\n",
    "        generate_datapoint.repeat(n=n_datapoints.unwrap())(probs, clusters)\n",
    "        @ \"datapoints\"\n",
    "    )\n",
    "\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some synthetic data to test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with N_CLUSTERS clusters evenly spaced\n",
    "points_per_cluster = int(N_DATAPOINTS / N_CLUSTERS)\n",
    "cluster_indices = jnp.arange(N_CLUSTERS)\n",
    "offsets = PRIOR_VARIANCE * (-4 + 8 * cluster_indices / N_CLUSTERS)\n",
    "\n",
    "# Create keys for each cluster\n",
    "keys = jax.random.split(jax.random.key(0), N_CLUSTERS)\n",
    "\n",
    "# Generate uniform random points for each cluster\n",
    "uniform_points = jax.vmap(lambda k: jax.random.uniform(k, shape=(points_per_cluster,)))(\n",
    "    keys\n",
    ")\n",
    "\n",
    "# Add offset and prior mean to each cluster's points\n",
    "shifted_points = uniform_points + (PRIOR_MEAN + offsets[:, None])\n",
    "\n",
    "datapoints = C[\"datapoints\", \"obs\"].set(shifted_points.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the main inference loop. As we said at the beginning, we do MCMC via Gibbs sampling. Inference therefore consist of a main loop and we evolve a trace over time. The final trace contains a sample from the approximate posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(datapoints):\n",
    "    key = jax.random.key(32421)\n",
    "    args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    initial_weights = C[\"probs\"].set(jnp.ones(N_CLUSTERS) / N_CLUSTERS)\n",
    "    constraints = datapoints | initial_weights\n",
    "    tr, _ = generate_data.importance(subkey, constraints, args)\n",
    "\n",
    "    if DEBUG:\n",
    "        all_posterior_means = [tr.get_choices()[\"clusters\", \"mean\"]]\n",
    "        all_posterior_weights = [tr.get_choices()[\"probs\"]]\n",
    "        all_cluster_assignment = [tr.get_choices()[\"datapoints\", \"idx\"]]\n",
    "\n",
    "        jax.debug.print(\"Initial means: {v}\", v=all_posterior_means[0])\n",
    "        jax.debug.print(\"Initial weights: {v}\", v=all_posterior_weights[0])\n",
    "\n",
    "        for _ in range(N_ITER):\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "            all_posterior_means.append(tr.get_choices()[\"clusters\", \"mean\"])\n",
    "\n",
    "            # # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "            all_cluster_assignment.append(tr.get_choices()[\"datapoints\", \"idx\"])\n",
    "\n",
    "            # # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "            all_posterior_weights.append(tr.get_choices()[\"probs\"])\n",
    "\n",
    "        return all_posterior_means, all_posterior_weights, all_cluster_assignment, tr\n",
    "\n",
    "    else:\n",
    "        # One Gibbs sweep consist of updating each latent variable\n",
    "        def update(carry, _):\n",
    "            key, tr = carry\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = update_cluster_means(subkey, tr)\n",
    "\n",
    "            # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = update_datapoint_assignment(subkey, tr)\n",
    "\n",
    "            # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = update_cluster_weights(subkey, tr)\n",
    "            return (key, tr), None\n",
    "\n",
    "        # Overall inference performs a fixed number of Gibbs sweeps\n",
    "        (key, tr), _ = jax.jit(jax.lax.scan)(update, (key, tr), None, length=N_ITER)\n",
    "        return tr\n",
    "\n",
    "\n",
    "def update_cluster_means(key, trace):\n",
    "    # We can update each cluster in parallel\n",
    "    # For each cluster, we find the datapoints in that cluster and compute their mean\n",
    "    datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]\n",
    "    datapoints = trace.get_choices()[\"datapoints\", \"obs\"]\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    current_means = trace.get_choices()[\"clusters\", \"mean\"]\n",
    "\n",
    "    # Count number of points per cluster\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Will contain some NaN due to clusters having no datapoint\n",
    "    cluster_means = (\n",
    "        jax.vmap(\n",
    "            lambda i: jnp.sum(jnp.where(datapoint_indexes == i, datapoints, 0)),\n",
    "            in_axes=(0),\n",
    "            out_axes=(0),\n",
    "        )(jnp.arange(n_clusters))\n",
    "        / category_counts\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Normal-iid-Normal distribution\n",
    "    # See https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf\n",
    "    # Note that there's a typo in the math for the posterior mean.\n",
    "    posterior_means = (\n",
    "        PRIOR_VARIANCE\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * cluster_means\n",
    "        + (OBS_VARIANCE / category_counts)\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * PRIOR_MEAN\n",
    "    )\n",
    "\n",
    "    posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)\n",
    "\n",
    "    # Gibbs resampling of cluster means\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_means = (\n",
    "        generate_cluster.vmap()\n",
    "        .simulate(key, (posterior_means, posterior_variances))\n",
    "        .get_choices()[\"mean\"]\n",
    "    )\n",
    "\n",
    "    # Remove the sampled Nan due to clusters having no datapoint and pick previous mean in that case, i.e. no Gibbs update for them\n",
    "    chosen_means = jnp.where(category_counts == 0, current_means, new_means)\n",
    "\n",
    "    if DEBUG:\n",
    "        jax.debug.print(\"Category counts: {v}\", v=category_counts)\n",
    "        jax.debug.print(\"Current means: {v}\", v=cluster_means)\n",
    "        jax.debug.print(\"Posterior means: {v}\", v=posterior_means)\n",
    "        jax.debug.print(fmt=\"Posterior variance: {v}\", v=posterior_variances)\n",
    "        jax.debug.print(\"Resampled means: {v}\", v=new_means)\n",
    "        jax.debug.print(\"Chosen means: {v}\", v=chosen_means)\n",
    "\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"clusters\", \"mean\"].set(chosen_means), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_datapoint_assignment(key, trace):\n",
    "    # We want to update the index for each datapoint, in parallel.\n",
    "    # It means we want to resample the i, but instead of being from the prior\n",
    "    # P(i | probs), we do it from the local posterior P(i | probs, xs).\n",
    "    # We need to do it for all addresses [\"datapoints\", \"idx\", i],\n",
    "    # and as these are independent (when conditioned on the rest)\n",
    "    # we can resample them in parallel.\n",
    "\n",
    "    # Conjugate update for a categorical is just exact posterior via enumeration\n",
    "    # P(x | y ) = P(x, y) \\ sum_x P(x, y).\n",
    "    # P(x | y1, y2) = P(x | y1)\n",
    "    # Sampling from Categorical(P(x = 1 | y ), P(x = 2 | y), ...) is the same as\n",
    "    # sampling from Categorical(P(x = 1, y), P(x = 2, y))\n",
    "    # as the weights need not be normalized\n",
    "    # In addition, if the model factorizes as P(x, y1, y2) = P(x, y1)P(y1 | y2),\n",
    "    # we can further simplify P(y1 | y2) from the categorical as it does not depend on x. More generally We only need to look at the children and parents of x (\"idx\" in our situation, which are conveniently wrapped in the generate_datapoint generative function).\n",
    "    def compute_local_density(x, i):\n",
    "        datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]\n",
    "        chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)\n",
    "        clusters = trace.get_choices()[\"clusters\", \"mean\"]\n",
    "        probs = trace.get_choices()[\"probs\"]\n",
    "        args = (probs, clusters)\n",
    "        model_logpdf, _ = generate_datapoint.assess(chm, args)\n",
    "        return model_logpdf\n",
    "\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    n_datapoints = trace.get_args()[1].unwrap()\n",
    "    local_densities = jax.vmap(\n",
    "        lambda x: jax.vmap(lambda i: compute_local_density(x, i))(\n",
    "            jnp.arange(n_clusters)\n",
    "        )\n",
    "    )(jnp.arange(n_datapoints))\n",
    "\n",
    "    # Conjugate update by sampling from posterior categorical\n",
    "    # Note: I think we could've used something like\n",
    "    # generate_datapoint.vmap().importance which would perhaps\n",
    "    # work in a more general setting but would definitely be slower here.\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_datapoint_indexes = (\n",
    "        genjax.categorical.vmap().simulate(key, (local_densities,)).get_choices()\n",
    "    )\n",
    "    # Gibbs resampling of datapoint assignment to clusters\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_cluster_weights(key, trace):\n",
    "    # Count number of points per cluster\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Dirichlet distribution\n",
    "    # See https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical_or_multinomial\n",
    "    new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts\n",
    "\n",
    "    # Gibbs resampling of cluster weights\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_retval()\n",
    "\n",
    "    if DEBUG:\n",
    "        jax.debug.print(fmt=\"Category counts: {v}\", v=category_counts)\n",
    "        jax.debug.print(fmt=\"New alpha: {v}\", v=new_alpha)\n",
    "        jax.debug.print(fmt=\"New probs: {v}\", v=new_probs)\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)\n",
    "    return new_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run inference, obtaining the final trace and some intermediate traces for visualizing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    (\n",
    "        all_posterior_means,\n",
    "        all_posterior_weights,\n",
    "        all_cluster_assignment,\n",
    "        posterior_trace,\n",
    "    ) = infer(datapoints)\n",
    "else:\n",
    "    posterior_trace = infer(datapoints)\n",
    "\n",
    "posterior_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot for a given index\n",
    "def create_plot(idx):\n",
    "    datapoint = datapoints[\"datapoints\", \"obs\"]\n",
    "    posterior_means = all_posterior_means[idx]\n",
    "    posterior_weights = all_posterior_weights[idx]\n",
    "    cluster_assignment = all_cluster_assignment[idx]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Plot datapoints colored by cluster assignment and posterior means together\n",
    "    for i in range(len(posterior_means)):\n",
    "        # Only plot points assigned to this cluster\n",
    "\n",
    "        mask = cluster_assignment == i\n",
    "        n_points = jnp.sum(mask)  # Count points in this cluster\n",
    "\n",
    "        if not jnp.any(mask):  # Skip if no points assigned to this cluster\n",
    "            continue\n",
    "\n",
    "        # Add random jitter to y-coordinates\n",
    "        key = jax.random.PRNGKey(i)  # Use cluster index as seed\n",
    "        y_jitter = jax.random.uniform(\n",
    "            key, shape=datapoint[mask].shape, minval=-0.1, maxval=0.1\n",
    "        )\n",
    "\n",
    "        ax.scatter(\n",
    "            datapoint[mask],\n",
    "            y_jitter,  # Use jittered y-coordinates\n",
    "            color=f\"C{i}\",\n",
    "            alpha=0.3,\n",
    "            s=5,\n",
    "        )\n",
    "\n",
    "        # Plot posterior means with size proportional to weights\n",
    "        weight = posterior_weights[i]  # Get current weight for this iteration\n",
    "        ax.scatter(\n",
    "            posterior_means[i],\n",
    "            0,\n",
    "            color=f\"C{i}\",\n",
    "            marker=\"*\",\n",
    "            s=300 + weight * 1200,  # Made cluster means much bigger\n",
    "            alpha=1,\n",
    "            label=f\"Cluster {i + 1} (Prob: {weight:.6f}, Points: {n_points})\",  # Added point count\n",
    "        )\n",
    "\n",
    "        # Plot standard deviation of the Gaussian means\n",
    "        ax.errorbar(\n",
    "            posterior_means[i],\n",
    "            0,\n",
    "            xerr=jnp.sqrt(OBS_VARIANCE),\n",
    "            fmt=\"o\",\n",
    "            color=f\"C{i}\",\n",
    "            capsize=5,\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    ax.set_title(f\"Iteration {idx}\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "NUM_FRAMES = 50\n",
    "\n",
    "# Create animation\n",
    "frames = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    fig = create_plot(int(N_ITER / NUM_FRAMES * i))\n",
    "    # Convert figure to image array\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(\n",
    "        fig.canvas.buffer_rgba(), dtype=np.uint8\n",
    "    )  # Updated to use buffer_rgba\n",
    "    image = image.reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (4,)\n",
    "    )  # Note: buffer_rgba returns RGBA\n",
    "    frames.append(image[:, :, :3])  # Convert RGBA to RGB by dropping alpha channel\n",
    "    plt.close(fig)\n",
    "\n",
    "# Create animation from frames\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "from matplotlib import animation\n",
    "\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig,\n",
    "    [[plt.imshow(frame)] for frame in frames],\n",
    "    interval=200,  # .2 second between frames\n",
    "    blit=True,\n",
    ")\n",
    "\n",
    "# Save animation as GIF\n",
    "imageio.mimsave(\"dirichlet_mixture_animation.gif\", frames, fps=15)\n",
    "\n",
    "# Display animation in notebook\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the animation\n",
    "data_points = datapoints[\"datapoints\", \"obs\"].tolist()\n",
    "np.random.seed(42)\n",
    "jitter = np.random.uniform(-0.05, 0.05, size=len(data_points)).tolist()\n",
    "std_dev = np.sqrt(OBS_VARIANCE) * 1.5\n",
    "all_cluster_assignments_list = [a.tolist() for a in all_cluster_assignment]\n",
    "all_posterior_means_list = [m.tolist() for m in all_posterior_means]\n",
    "all_posterior_weights_list = [w.tolist() for w in all_posterior_weights]\n",
    "\n",
    "# Create a visualizer with animation\n",
    "(\n",
    "    Plot.initialState({\"frame\": 0, \"hoveredCluster\": None})\n",
    "    |\n",
    "    # Main visualization that updates based on the current frame\n",
    "    Plot.plot({\n",
    "        \"marks\": [\n",
    "            # 1. Data points with jitter - show ALL data points with optional highlighting\n",
    "            Plot.dot(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "                    const frame = $state.frame;\n",
    "                    const hoveredCluster = $state.hoveredCluster;\n",
    "                    const dataPoints = \"\"\"\n",
    "                    + str(data_points)\n",
    "                    + \"\"\";\n",
    "                    const jitter = \"\"\"\n",
    "                    + str(jitter)\n",
    "                    + \"\"\";\n",
    "                    const assignments = \"\"\"\n",
    "                    + str(all_cluster_assignments_list)\n",
    "                    + \"\"\"[frame];\n",
    "                    \n",
    "                    // Return all points with hover-aware opacity\n",
    "                    return dataPoints.map((x, i) => {\n",
    "                        const clusterIdx = assignments[i];\n",
    "                        // If a cluster is hovered, reduce opacity of other clusters' points\n",
    "                        const isHovered = hoveredCluster !== null && clusterIdx === hoveredCluster;\n",
    "                        const opacity = hoveredCluster === null ? 0.5 : (isHovered ? 0.7 : 0.15);\n",
    "                        return [x, jitter[i], clusterIdx, opacity];\n",
    "                    });\n",
    "                }()\"\"\"\n",
    "                ),\n",
    "                {\"x\": \"0\", \"y\": \"1\", \"fill\": \"2\", \"r\": 3, \"opacity\": \"3\"},\n",
    "            ),\n",
    "            # 2. Error bars with hover highlighting\n",
    "            Plot.line(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "                    const frame = $state.frame;\n",
    "                    const hoveredCluster = $state.hoveredCluster;\n",
    "                    const means = \"\"\"\n",
    "                    + str(all_posterior_means_list)\n",
    "                    + \"\"\"[frame];\n",
    "                    const weights = \"\"\"\n",
    "                    + str(all_posterior_weights_list)\n",
    "                    + \"\"\"[frame];\n",
    "                    const stdDev = \"\"\"\n",
    "                    + str(std_dev)\n",
    "                    + \"\"\";\n",
    "                    const capSize = 0.04;  // Size of the vertical cap lines\n",
    "                    \n",
    "                    const result = [];\n",
    "                    for (let i = 0; i < means.length; i++) {\n",
    "                        // Only include error bars for clusters with weight >= 0.01\n",
    "                        if (weights[i] >= 0.01) {\n",
    "                            // Determine if this cluster is being hovered\n",
    "                            const isHovered = hoveredCluster === i;\n",
    "                            const opacity = hoveredCluster === null ? 0.7 : (isHovered ? 1.0 : 0.3);\n",
    "                            const strokeWidth = isHovered ? 4 : 3;\n",
    "                            \n",
    "                            // Left cap (vertical line)\n",
    "                            result.push([means[i] - stdDev, -capSize, i, opacity, strokeWidth]);\n",
    "                            result.push([means[i] - stdDev, capSize, i, opacity, strokeWidth]);\n",
    "                            \n",
    "                            // Right cap (vertical line)\n",
    "                            result.push([means[i] + stdDev, -capSize, i, opacity, strokeWidth]);\n",
    "                            result.push([means[i] + stdDev, capSize, i, opacity, strokeWidth]);\n",
    "                        }\n",
    "                    }\n",
    "                    return result;\n",
    "                }()\"\"\"\n",
    "                ),\n",
    "                {\n",
    "                    \"x\": \"0\",\n",
    "                    \"y\": \"1\",\n",
    "                    \"stroke\": \"2\",\n",
    "                    \"strokeWidth\": \"4\",\n",
    "                    \"opacity\": \"3\",\n",
    "                    \"z\": \"2\",\n",
    "                },\n",
    "            ),\n",
    "            # 3. Horizontal lines for error bars with hover highlighting\n",
    "            Plot.line(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "                    const frame = $state.frame;\n",
    "                    const hoveredCluster = $state.hoveredCluster;\n",
    "                    const means = \"\"\"\n",
    "                    + str(all_posterior_means_list)\n",
    "                    + \"\"\"[frame];\n",
    "                    const weights = \"\"\"\n",
    "                    + str(all_posterior_weights_list)\n",
    "                    + \"\"\"[frame];\n",
    "                    const stdDev = \"\"\"\n",
    "                    + str(std_dev)\n",
    "                    + \"\"\";\n",
    "                    \n",
    "                    const result = [];\n",
    "                    for (let i = 0; i < means.length; i++) {\n",
    "                        // Only include lines for clusters with weight >= 0.01\n",
    "                        if (weights[i] >= 0.01) {\n",
    "                            // Determine if this cluster is being hovered\n",
    "                            const isHovered = hoveredCluster === i;\n",
    "                            const opacity = hoveredCluster === null ? 0.7 : (isHovered ? 1.0 : 0.3);\n",
    "                            const strokeWidth = isHovered ? 4 : 3;\n",
    "                            \n",
    "                            result.push([means[i] - stdDev, 0, i, opacity, strokeWidth]);  // Left endpoint\n",
    "                            result.push([means[i] + stdDev, 0, i, opacity, strokeWidth]);  // Right endpoint\n",
    "                        }\n",
    "                    }\n",
    "                    return result;\n",
    "                }()\"\"\"\n",
    "                ),\n",
    "                {\n",
    "                    \"x\": \"0\",\n",
    "                    \"y\": \"1\",\n",
    "                    \"stroke\": \"2\",\n",
    "                    \"strokeWidth\": \"4\",  # Now dynamic based on hover\n",
    "                    \"opacity\": \"3\",  # Now dynamic based on hover\n",
    "                    \"z\": \"2\",\n",
    "                },\n",
    "            ),\n",
    "            # 4. Cluster means as stars with hover highlighting - MUCH larger size\n",
    "            Plot.dot(\n",
    "                Plot.js(\n",
    "                    \"\"\"function() {\n",
    "        const frame = $state.frame;\n",
    "        const hoveredCluster = $state.hoveredCluster;\n",
    "        const means = \"\"\"\n",
    "                    + str(all_posterior_means_list)\n",
    "                    + \"\"\"[frame];\n",
    "        const weights = \"\"\"\n",
    "                    + str(all_posterior_weights_list)\n",
    "                    + \"\"\"[frame];\n",
    "        \n",
    "        // Create a simple array for each cluster mean\n",
    "        return means.map((mean, i) => {\n",
    "            // Only include means for clusters with sufficient weight\n",
    "            if (weights[i] >= 0.01) {\n",
    "                const isHovered = hoveredCluster === i;\n",
    "                return [\n",
    "                    mean,           // x position\n",
    "                    0,              // y position\n",
    "                    i,              // cluster index for color\n",
    "                    isHovered ? 1.0 : 0.8  // opacity changes on hover\n",
    "                ];\n",
    "            }\n",
    "            return null;  // Skip low-weight clusters\n",
    "        }).filter(d => d !== null);  // Remove null values\n",
    "    }()\"\"\"\n",
    "                ),\n",
    "                {\n",
    "                    \"x\": \"0\",\n",
    "                    \"y\": \"1\",\n",
    "                    \"fill\": \"2\",\n",
    "                    \"r\": 10,\n",
    "                    \"symbol\": \"star\",\n",
    "                    \"stroke\": \"black\",\n",
    "                    \"strokeWidth\": 2,\n",
    "                    \"opacity\": \"3\",\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "        \"grid\": True,\n",
    "        \"marginTop\": 40,\n",
    "        \"marginRight\": 40,\n",
    "        \"marginBottom\": 40,\n",
    "        \"marginLeft\": 40,\n",
    "        \"style\": {\"height\": \"400px\"},\n",
    "        \"title\": Plot.js(\n",
    "            \"`Iteration ${$state.frame} of \" + str(len(all_posterior_means) - 1) + \"`\"\n",
    "        ),\n",
    "    })\n",
    "    |\n",
    "    # Animation controls and legend with hover effects\n",
    "    Plot.html([\n",
    "        \"div\",\n",
    "        {\"className\": \"p-4\"},\n",
    "        [\n",
    "            \"div\",\n",
    "            {\"className\": \"mb-4\"},\n",
    "            Plot.Slider(\n",
    "                \"frame\",\n",
    "                init=0,\n",
    "                range=[0, len(all_posterior_means) - 1],\n",
    "                step=1,\n",
    "                label=\"Iteration\",\n",
    "                width=\"100%\",\n",
    "                fps=8,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            \"div\",\n",
    "            {\"className\": \"mt-4\"},\n",
    "            Plot.js(\n",
    "                \"\"\"function() {\n",
    "                const frame = $state.frame;\n",
    "                const weights = \"\"\"\n",
    "                + str(all_posterior_weights_list)\n",
    "                + \"\"\"[frame];\n",
    "                \n",
    "                // Count assignments in current frame\n",
    "                const assignments = \"\"\"\n",
    "                + str(all_cluster_assignments_list)\n",
    "                + \"\"\"[frame];\n",
    "                const counts = {};\n",
    "                assignments.forEach(a => {\n",
    "                    counts[a] = (counts[a] || 0) + 1;\n",
    "                });\n",
    "                \n",
    "                // Sort clusters by weight, filter by minimum weight, and limit to top 10\n",
    "                const topClusters = Object.keys(weights)\n",
    "                    .map(i => [parseInt(i), weights[i], counts[parseInt(i)] || 0])\n",
    "                    .filter(([_, weight, __]) => weight >= 0.01)  // Only include clusters with weight >= 0.01\n",
    "                    .sort((a, b) => b[1] - a[1])\n",
    "                    .slice(0, 10);  // Only show top 10 clusters\n",
    "                \n",
    "                // Create empty placeholders if we have fewer than 10 clusters\n",
    "                const placeholders = [];\n",
    "                if (topClusters.length < 10) {\n",
    "                    for (let i = 0; i < 10 - topClusters.length; i++) {\n",
    "                        placeholders.push(\n",
    "                            [\"div\", {\"className\": \"flex items-center my-1\", \"style\": {\"height\": \"24px\"}}, \"\"]\n",
    "                        );\n",
    "                    }\n",
    "                }\n",
    "                    \n",
    "                return [\n",
    "                    \"div\",\n",
    "                    {},\n",
    "                    [\"h3\", {}, `Top Clusters by Weight (Iteration ${frame})`],\n",
    "                    [\"div\", \n",
    "                        {\n",
    "                            \"className\": \"mt-2\",\n",
    "                            \"style\": {\n",
    "                                \"minHeight\": \"240px\",  // Fixed height for 10 items\n",
    "                                \"display\": \"flex\",\n",
    "                                \"flexDirection\": \"column\"\n",
    "                            }\n",
    "                        },\n",
    "                        ...topClusters.map(([i, weight, count]) => \n",
    "                            [\"div\", \n",
    "                                {\n",
    "                                    \"className\": \"flex items-center my-1\",\n",
    "                                    \"style\": {\n",
    "                                        \"cursor\": \"pointer\",\n",
    "                                        \"padding\": \"4px\",\n",
    "                                        \"borderRadius\": \"4px\",\n",
    "                                        \"backgroundColor\": $state.hoveredCluster === i ? \"#f0f0f0\" : \"transparent\"\n",
    "                                    },\n",
    "                                    \"onMouseEnter\": () => { $state.hoveredCluster = i; },\n",
    "                                    \"onMouseLeave\": () => { $state.hoveredCluster = null; }\n",
    "                                },\n",
    "                                [\"div\", {\n",
    "                                    \"className\": \"w-6 h-6 flex-shrink-0 mr-2\", \n",
    "                                    \"style\": {\"backgroundColor\": `var(--plot-color-${i % 10})`}\n",
    "                                }],\n",
    "                                [\"span\", {}, `Cluster ${i} (Weight: ${weight.toFixed(4)}, Points: ${count})`]\n",
    "                            ]\n",
    "                        ),\n",
    "                        ...placeholders  // Add empty placeholder rows to maintain height\n",
    "                    ]\n",
    "                ];\n",
    "            }()\"\"\"\n",
    "            ),\n",
    "        ],\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interested reader, here's some exercises to try out to make this model better:\n",
    "1) Extend the model to infer the variance of the clusters by putting an inverse_gamma prior replacing the `OBS_VARIANCE` hyperparameter and doing block-Gibbs on it using the normal-inverse-gamma conjugacy\n",
    "2) Try a better initialization of the datapoint assignment: pick a point a use something like k-means and assign all the surrounding points to the same initial cluster. Iterate on all the points until they all have some initial cluster.\n",
    "3) Improve inference using SMC via data annealing: subssample 1/100 of the data and run inference on this, then run inference again on 1/10 of the data starting with the inferred choices for cluster means and weights from the previous trace, and finally repeat for the whole data.\n",
    "\n",
    "Note that the model is still expected to get stuck in local minima (the clustering at the borders isn't great), and one way to improve upon it would be to use a split-merge move, via reversible-jump MCMC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
