{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: dirichlet mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see some of the ingredients in action in a simple but more realistic setting and write a dirichlet mixture model in GenJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Points on the Real Line\n",
    "\n",
    "The goal here is to cluster datapoints on the real line. To do so, we model a fixed number of clusters, each as a 1D-Gaussian with fixed variance, and we want to infer their means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "The \"model of the world\" postulates:\n",
    "- A fixed number of 1D Gaussians\n",
    "- Each Gaussian is assigned a weight, representing the proportion of points assigned to each cluster \n",
    "- Each datapoint is assigned to a cluster\n",
    "\n",
    "### Generative Process\n",
    "\n",
    "We turn this into a generative model as follows:\n",
    "- We have a fixed prior mean and variance for where the cluster centers might be\n",
    "- We sample a mean for each cluster\n",
    "- We sample an initial weight per cluster (sum of weights is 1)\n",
    "- For each datapoint:\n",
    "  - We sample a cluster assignment proportional to the cluster weights\n",
    "  - We sample the datapoint noisily around the mean of the cluster\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We, the modelers, get to choose how this process is implemented. \n",
    "- We choose distributions for each sampling step in a way that makes **inference tractable**.\n",
    "- More precisely, we choose conjugate pairs so that we can do inference via Gibbs sampling. \n",
    "  - Gibbs sampling is an MCMC method that samples an initial trace, and then updates the traced choices we want to infer over time. \n",
    "  - To update a choice, Gibbs sampling samples from a conditional distribution, which is tractable with conjugate relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import categorical, dirichlet, gen, normal, pretty\n",
    "from genjax._src.core.pytree import Const, Pytree\n",
    "\n",
    "pretty()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the generative model we described above. It has several hyperparameters that are somewhat manually inferred. An extension to the model could instead do inference over these hyperparameters, and fix hyper-hyperparameters instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "PRIOR_VARIANCE = 10.0\n",
    "OBS_VARIANCE = 1.0\n",
    "N_DATAPOINTS = 5000\n",
    "N_CLUSTERS = 40\n",
    "ALPHA = float(N_DATAPOINTS / (N_CLUSTERS * 10))\n",
    "PRIOR_MEAN = 50.0\n",
    "N_ITER = 50\n",
    "\n",
    "# Debugging mode\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "# Simple custom data type to represent a cluster by its mean\n",
    "@Pytree.dataclass\n",
    "class Cluster(Pytree):\n",
    "    mean: float\n",
    "\n",
    "\n",
    "# Sub generative functions of the bigger model\n",
    "@gen\n",
    "def generate_cluster(mean, var):\n",
    "    cluster_mean = normal(mean, var) @ \"mean\"\n",
    "    return Cluster(cluster_mean)\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster_weight(alphas):\n",
    "    probs = dirichlet(alphas) @ \"probs\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_datapoint(probs, clusters):\n",
    "    idx = categorical(jnp.log(probs)) @ \"idx\"\n",
    "    obs = normal(clusters.mean[idx], OBS_VARIANCE) @ \"obs\"\n",
    "    return obs\n",
    "\n",
    "\n",
    "# Main model\n",
    "@gen\n",
    "def generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):\n",
    "    clusters = (\n",
    "        generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)\n",
    "        @ \"clusters\"\n",
    "    )\n",
    "\n",
    "    probs = generate_cluster_weight.inline(\n",
    "        alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())\n",
    "    )\n",
    "\n",
    "    datapoints = (\n",
    "        generate_datapoint.repeat(n=n_datapoints.unwrap())(probs, clusters)\n",
    "        @ \"datapoints\"\n",
    "    )\n",
    "\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some synthetic data to test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = C[\"datapoints\", \"obs\"].set(\n",
    "    jnp.concatenate([\n",
    "        jax.random.uniform(jax.random.key(i), shape=(int(N_DATAPOINTS / N_CLUSTERS),))\n",
    "        + PRIOR_MEAN\n",
    "        + PRIOR_VARIANCE * (-4 + 8 * i / N_CLUSTERS)\n",
    "        for i in range(N_CLUSTERS)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the main inference loop. As we said at the beginning, we do MCMC via Gibbs sampling. Inference therefore consist of a main loop and we evolve a trace over time. The final trace contains a sample from the approximate posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(datapoints):\n",
    "    key = jax.random.key(32421)\n",
    "    args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    initial_weights = C[\"probs\"].set(jnp.ones(N_CLUSTERS) / N_CLUSTERS)\n",
    "    constraints = datapoints | initial_weights\n",
    "    tr, _ = generate_data.importance(subkey, constraints, args)\n",
    "\n",
    "    if DEBUG:\n",
    "        all_posterior_means = [tr.get_choices()[\"clusters\", \"mean\"]]\n",
    "        all_posterior_weights = [tr.get_choices()[\"probs\"]]\n",
    "        all_cluster_assignment = [tr.get_choices()[\"datapoints\", \"idx\"]]\n",
    "\n",
    "        jax.debug.print(\"Initial means: {v}\", v=all_posterior_means[0])\n",
    "        jax.debug.print(\"Initial weights: {v}\", v=all_posterior_weights[0])\n",
    "\n",
    "        for _ in range(N_ITER):\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "            all_posterior_means.append(tr.get_choices()[\"clusters\", \"mean\"])\n",
    "\n",
    "            # # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "            all_cluster_assignment.append(tr.get_choices()[\"datapoints\", \"idx\"])\n",
    "\n",
    "            # # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "            all_posterior_weights.append(tr.get_choices()[\"probs\"])\n",
    "\n",
    "        return all_posterior_means, all_posterior_weights, all_cluster_assignment, tr\n",
    "\n",
    "    else:\n",
    "        # One Gibbs sweep consist of updating each latent variable\n",
    "        def update(carry, _):\n",
    "            key, tr = carry\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "\n",
    "            # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "\n",
    "            # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "            return (key, tr), None\n",
    "\n",
    "        # Overall inference performs a fixed number of Gibbs sweeps\n",
    "        (key, tr), _ = jax.lax.scan(update, (key, tr), None, length=N_ITER)\n",
    "        return tr\n",
    "\n",
    "\n",
    "def update_cluster_means(key, trace):\n",
    "    # We can update each cluster in parallel\n",
    "    # For each cluster, we find the datapoints in that cluster and compute their mean\n",
    "    datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]\n",
    "    datapoints = trace.get_choices()[\"datapoints\", \"obs\"]\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    current_means = trace.get_choices()[\"clusters\", \"mean\"]\n",
    "\n",
    "    # Count number of points per cluster\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Will contain some NaN due to clusters having no datapoint\n",
    "    cluster_means = (\n",
    "        jax.vmap(\n",
    "            lambda i: jnp.sum(jnp.where(datapoint_indexes == i, datapoints, 0)),\n",
    "            in_axes=(0),\n",
    "            out_axes=(0),\n",
    "        )(jnp.arange(n_clusters))\n",
    "        / category_counts\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Normal-iid-Normal distribution\n",
    "    # See https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf\n",
    "    # Note that there's a typo in the math for the posterior mean.\n",
    "    posterior_means = (\n",
    "        PRIOR_VARIANCE\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * cluster_means\n",
    "        + (OBS_VARIANCE / category_counts)\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * PRIOR_MEAN\n",
    "    )\n",
    "\n",
    "    posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)\n",
    "\n",
    "    # Gibbs resampling of cluster means\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_means = (\n",
    "        generate_cluster.vmap()\n",
    "        .simulate(key, (posterior_means, posterior_variances))\n",
    "        .get_choices()[\"mean\"]\n",
    "    )\n",
    "\n",
    "    # Remove the sampled Nan due to clusters having no datapoint and pick previous mean in that case, i.e. no Gibbs update for them\n",
    "    chosen_means = jnp.where(category_counts == 0, current_means, new_means)\n",
    "\n",
    "    if DEBUG:\n",
    "        jax.debug.print(\"Category counts: {v}\", v=category_counts)\n",
    "        jax.debug.print(\"Current means: {v}\", v=cluster_means)\n",
    "        jax.debug.print(\"Posterior means: {v}\", v=posterior_means)\n",
    "        jax.debug.print(fmt=\"Posterior variance: {v}\", v=posterior_variances)\n",
    "        jax.debug.print(\"Resampled means: {v}\", v=new_means)\n",
    "        jax.debug.print(\"Chosen means: {v}\", v=chosen_means)\n",
    "\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"clusters\", \"mean\"].set(chosen_means), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_datapoint_assignment(key, trace):\n",
    "    # We want to update the index for each datapoint, in parallel.\n",
    "    # It means we want to resample the i, but instead of being from the prior\n",
    "    # P(i | probs), we do it from the local posterior P(i | probs, xs).\n",
    "    # We need to do it for all addresses [\"datapoints\", \"idx\", i],\n",
    "    # and as these are independent (when conditioned on the rest)\n",
    "    # we can resample them in parallel.\n",
    "\n",
    "    # Conjugate update for a categorical is just exact posterior via enumeration\n",
    "    # P(x | y ) = P(x, y) \\ sum_x P(x, y).\n",
    "    # P(x | y1, y2) = P(x | y1)\n",
    "    # Sampling from Categorical(P(x = 1 | y ), P(x = 2 | y), ...) is the same as\n",
    "    # sampling from Categorical(P(x = 1, y), P(x = 2, y))\n",
    "    # as the weights need not be normalized\n",
    "    # In addition, if the model factorizes as P(x, y1, y2) = P(x, y1)P(y1 | y2),\n",
    "    # we can further simplify P(y1 | y2) from the categorical as it does not depend on x. More generally We only need to look at the children and parents of x (\"idx\" in our situation, which are conveniently wrapped in the generate_datapoint generative function).\n",
    "    def compute_local_density(x, i):\n",
    "        datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]\n",
    "        chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)\n",
    "        clusters = Cluster(trace.get_choices()[\"clusters\", \"mean\"])\n",
    "        probs = trace.get_choices()[\"probs\"]\n",
    "        args = (probs, clusters)\n",
    "        model_logpdf, _ = generate_datapoint.assess(chm, args)\n",
    "        return model_logpdf\n",
    "\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    n_datapoints = trace.get_args()[1].unwrap()\n",
    "    local_densities = jax.vmap(\n",
    "        lambda x: jax.vmap(lambda i: compute_local_density(x, i))(\n",
    "            jnp.arange(n_clusters)\n",
    "        )\n",
    "    )(jnp.arange(n_datapoints))\n",
    "\n",
    "    # Conjugate update by sampling from posterior categorical\n",
    "    # Note: I think we could've used something like\n",
    "    # generate_datapoint.vmap().importance which would perhaps\n",
    "    # work in a more general setting but would definitely be slower here.\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_datapoint_indexes = (\n",
    "        genjax.categorical.vmap().simulate(key, (local_densities,)).get_choices()\n",
    "    )\n",
    "    # Gibbs resampling of datapoint assignment to clusters\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_cluster_weights(key, trace):\n",
    "    # Count number of points per cluster\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Dirichlet distribution\n",
    "    # See https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical_or_multinomial\n",
    "    new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts\n",
    "\n",
    "    # Gibbs resampling of cluster weights\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_retval()\n",
    "\n",
    "    if DEBUG:\n",
    "        jax.debug.print(fmt=\"Category counts: {v}\", v=category_counts)\n",
    "        jax.debug.print(fmt=\"New alpha: {v}\", v=new_alpha)\n",
    "        jax.debug.print(fmt=\"New probs: {v}\", v=new_probs)\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)\n",
    "    return new_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run inference, obtaining the final trace and some intermediate traces for visualizing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    (\n",
    "        all_posterior_means,\n",
    "        all_posterior_weights,\n",
    "        all_cluster_assignment,\n",
    "        posterior_trace,\n",
    "    ) = infer(datapoints)\n",
    "else:\n",
    "    posterior_trace = infer(datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot for a given index\n",
    "def create_plot(idx):\n",
    "    datapoint = datapoints[\"datapoints\", \"obs\"]\n",
    "    posterior_means = all_posterior_means[idx]\n",
    "    posterior_weights = all_posterior_weights[idx]\n",
    "    cluster_assignment = all_cluster_assignment[idx]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Plot datapoints colored by cluster assignment and posterior means together\n",
    "    for i in range(len(posterior_means)):\n",
    "        # Only plot points assigned to this cluster\n",
    "\n",
    "        mask = cluster_assignment == i\n",
    "        n_points = jnp.sum(mask)  # Count points in this cluster\n",
    "\n",
    "        if not jnp.any(mask):  # Skip if no points assigned to this cluster\n",
    "            continue\n",
    "\n",
    "        # Add random jitter to y-coordinates\n",
    "        key = jax.random.PRNGKey(i)  # Use cluster index as seed\n",
    "        y_jitter = jax.random.uniform(\n",
    "            key, shape=datapoint[mask].shape, minval=-0.1, maxval=0.1\n",
    "        )\n",
    "\n",
    "        ax.scatter(\n",
    "            datapoint[mask],\n",
    "            y_jitter,  # Use jittered y-coordinates\n",
    "            color=f\"C{i}\",\n",
    "            alpha=0.3,\n",
    "            s=5,\n",
    "        )\n",
    "\n",
    "        # Plot posterior means with size proportional to weights\n",
    "        weight = posterior_weights[i]  # Get current weight for this iteration\n",
    "        ax.scatter(\n",
    "            posterior_means[i],\n",
    "            0,\n",
    "            color=f\"C{i}\",\n",
    "            marker=\"*\",\n",
    "            s=300 + weight * 1200,  # Made cluster means much bigger\n",
    "            alpha=1,\n",
    "            label=f\"Cluster {i + 1} (Prob: {weight:.6f}, Points: {n_points})\",  # Added point count\n",
    "        )\n",
    "\n",
    "        # Plot standard deviation of the Gaussian means\n",
    "        ax.errorbar(\n",
    "            posterior_means[i],\n",
    "            0,\n",
    "            xerr=jnp.sqrt(OBS_VARIANCE),\n",
    "            fmt=\"o\",\n",
    "            color=f\"C{i}\",\n",
    "            capsize=5,\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    ax.set_title(f\"Iteration {idx}\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "NUM_FRAMES = 50\n",
    "\n",
    "# Create animation\n",
    "frames = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    fig = create_plot(int(N_ITER / NUM_FRAMES * i))\n",
    "    # Convert figure to image array\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(\n",
    "        fig.canvas.buffer_rgba(), dtype=np.uint8\n",
    "    )  # Updated to use buffer_rgba\n",
    "    image = image.reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (4,)\n",
    "    )  # Note: buffer_rgba returns RGBA\n",
    "    frames.append(image[:, :, :3])  # Convert RGBA to RGB by dropping alpha channel\n",
    "    plt.close(fig)\n",
    "\n",
    "# Create animation from frames\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "from matplotlib import animation\n",
    "\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig,\n",
    "    [[plt.imshow(frame)] for frame in frames],\n",
    "    interval=200,  # .2 second between frames\n",
    "    blit=True,\n",
    ")\n",
    "\n",
    "# Save animation as GIF\n",
    "imageio.mimsave(\"dirichlet_mixture_animation.gif\", frames, fps=15)\n",
    "\n",
    "# Display animation in notebook\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interested reader, here's some exercises to try out to make this model better:\n",
    "1) Extend the model to infer the variance of the clusters by putting an inverse_gamma prior replacing the `OBS_VARIANCE` hyperparameter and doing block-Gibbs on it using the normal-inverse-gamma conjugacy\n",
    "2) Try a better initialization of the datapoint assignment: pick a point a use something like k-means and assign all the surrounding points to the same initial cluster. Iterate on all the points until they all have some initial cluster.\n",
    "3) Improve inference using SMC via data annealing: subssample 1/100 of the data and run inference on this, then run inference again on 1/10 of the data starting with the inferred choices for cluster means and weights from the previous trace, and finally repeat for the whole data.\n",
    "\n",
    "Note that the model is still expected to get stuck in local minima, and one way to improve upon it would be to use a split-merge involutive MCMC move."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
