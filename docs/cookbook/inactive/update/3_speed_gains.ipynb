{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Compute gains via incremental computation or how to not compute log pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import gen, normal, pretty\n",
    "from genjax._src.core.pytree import Const\n",
    "\n",
    "pretty()\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cookbooks, we have seen that `importance` and `update`  do algebraic simplifications in the weight ratios that they are computing. \n",
    "Let's first see the difference in the case of `importance` by testing a naive version of sampling importance resampling (SIR) to one using `importance`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a model to be used in the rest ot the cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def model(size_model: Const[int]):\n",
    "    size_model = size_model.unwrap()\n",
    "    x = normal(0.0, 1.0) @ \"x\"\n",
    "    a = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"\n",
    "    b = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"\n",
    "    c = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"\n",
    "    obs = normal(jnp.sum(a) + jnp.sum(b) + jnp.sum(c) + x, 5.0) @ \"obs\"\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare naive SIR to the one using `importance` and the default proposal, let's write define the default proposal manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def default_proposal(size_model: Const[int]):\n",
    "    size_model = size_model.unwrap()\n",
    "    _ = normal(0.0, 1.0) @ \"x\"\n",
    "    _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"\n",
    "    _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"\n",
    "    _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write SIR with a parameter controlling whether to call the slow or fast version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = C[\"obs\"].set(\n",
    "    1.0,\n",
    ")\n",
    "\n",
    "\n",
    "def sir(key, N: int, use_fast: bool, size_model):\n",
    "    if use_fast:\n",
    "        traces, weights = jax.vmap(model.importance, in_axes=(0, None, None))(\n",
    "            jax.random.split(key, N), obs, size_model\n",
    "        )\n",
    "    else:\n",
    "        traces = jax.vmap(default_proposal.simulate, in_axes=(0, None))(\n",
    "            jax.random.split(key, N), size_model\n",
    "        )\n",
    "\n",
    "        chm_proposal = traces.get_choices()\n",
    "        q_weights, _ = jax.vmap(\n",
    "            lambda idx: default_proposal.assess(\n",
    "                jax.tree_util.tree_map(lambda v: v[idx], chm_proposal), size_model\n",
    "            )\n",
    "        )(jnp.arange(N))\n",
    "\n",
    "        chm_model = chm_proposal | C[\"obs\"].set(jnp.ones(N) * obs[\"obs\"])\n",
    "        p_weights, _ = jax.vmap(\n",
    "            lambda idx: model.assess(\n",
    "                jax.tree_util.tree_map(lambda v: v[idx], chm_model), size_model\n",
    "            )\n",
    "        )(jnp.arange(N))\n",
    "\n",
    "        weights = p_weights - q_weights\n",
    "\n",
    "    idx = genjax.categorical.simulate(key, (weights,)).get_retval()\n",
    "    samples = traces.get_choices()\n",
    "    resampled = jax.tree_util.tree_map(lambda v: v[idx], samples)\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the speed of the 2 versions (beware there's some variance in the estimate, but adding more trials makes the runtime comparison take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = C[\"obs\"].set(\n",
    "    1.0,\n",
    ")\n",
    "model_sizes = [10, 100, 1000]\n",
    "N_sir = 100\n",
    "num_trials = 30\n",
    "slow_times = []\n",
    "fast_times = []\n",
    "\n",
    "for model_size in model_sizes:\n",
    "    total_time_slow = 0\n",
    "    total_time_fast = 0\n",
    "    model_size = Const(model_size)\n",
    "    obs = C[\"obs\"].set(\n",
    "        1.0,\n",
    "    )\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    # warm up run to trigger jit compilation\n",
    "    jitted = jax.jit(sir, static_argnums=(1, 2))\n",
    "    jitted(subkey, N_sir, False, (Const(model_sizes),))\n",
    "    jitted(subkey, N_sir, True, (Const(model_sizes),))\n",
    "\n",
    "    # measure time for each algorithm\n",
    "    key, subkey = jax.random.split(key)\n",
    "    total_time_slow = timeit.timeit(\n",
    "        lambda: jitted(subkey, N_sir, False, (Const(model_sizes),)), number=num_trials\n",
    "    )\n",
    "    total_time_fast = timeit.timeit(\n",
    "        lambda: jitted(subkey, N_sir, True, (Const(model_sizes),)), number=num_trials\n",
    "    )\n",
    "\n",
    "    average_time_slow = total_time_slow / num_trials\n",
    "    average_time_fast = total_time_fast / num_trials\n",
    "    slow_times.append(average_time_slow)\n",
    "    fast_times.append(average_time_fast)\n",
    "\n",
    "plt.plot(model_sizes, [time for time in slow_times], marker=\"o\", label=\"Slow Algorithm\")\n",
    "plt.plot(model_sizes, [time for time in fast_times], marker=\"o\", label=\"Fast Algorithm\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Argument (n)\")\n",
    "plt.ylabel(\"Average Time (seconds)\")\n",
    "plt.title(\"Average Execution Time of MH move for different model sizes\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing inference with iterative algorithms like MCMC, we often need to make small adjustments to the choice map.\n",
    "We have seen that `update` can be used to compute part of the MH acceptance ratio. \n",
    "So now let's try to compare two versions of an MH move, one computing naively thee ratio and one using update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a very basic kernel to rejuvenate the variable \"x\" in an MH algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def rejuv_x(x):\n",
    "    x = normal(x, 1.0) @ \"x\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write 2 versions of computing the MH acceptance ratio as well as the MH algorithm to rejuvenate the variable \"x\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio_slow(key, fwd_choice, fwd_weight, model_args, chm):\n",
    "    model_weight_old, _ = model.assess(chm, model_args)\n",
    "    new_chm = fwd_choice | chm\n",
    "    model_weight_new, _ = model.assess(new_chm, model_args)\n",
    "    old_x = C[\"x\"].set(chm[\"x\"])\n",
    "    proposal_args_backward = (fwd_choice[\"x\"],)\n",
    "    bwd_weight, _ = rejuv_x.assess(old_x, proposal_args_backward)\n",
    "    α = model_weight_new - model_weight_old - fwd_weight + bwd_weight\n",
    "    return α\n",
    "\n",
    "\n",
    "def compute_ratio_fast(key, fwd_choice, fwd_weight, model_args, trace):\n",
    "    argdiffs = genjax.Diff.no_change(model_args)\n",
    "    _, weight, _, discard = model.update(key, trace, fwd_choice, argdiffs)\n",
    "    proposal_args_backward = (fwd_choice[\"x\"],)\n",
    "    bwd_weight, _ = rejuv_x.assess(discard.choice_map, proposal_args_backward)\n",
    "    α = weight - fwd_weight + bwd_weight\n",
    "    return α\n",
    "\n",
    "\n",
    "def metropolis_hastings_move(key, trace, use_fast):\n",
    "    model_args = trace.get_args()\n",
    "    proposal_args_forward = (trace.get_choices()[\"x\"],)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    fwd_choice, fwd_weight, _ = rejuv_x.propose(subkey, proposal_args_forward)\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    if use_fast:\n",
    "        α = compute_ratio_fast(subkey, fwd_choice, fwd_weight, model_args, trace)\n",
    "    else:\n",
    "        chm = trace.get_choices()\n",
    "        α = compute_ratio_slow(subkey, fwd_choice, fwd_weight, model_args, chm)\n",
    "\n",
    "    old_choice = C[\"x\"].set(trace.get_choices()[\"x\"])\n",
    "    key, subkey = jax.random.split(key)\n",
    "    ret_trace = jax.lax.cond(\n",
    "        jnp.log(jax.random.uniform(subkey)) < α, lambda: fwd_choice, lambda: old_choice\n",
    "    )\n",
    "    return ret_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the performance of each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sizes = [1000, 10000, 100000, 1000000, 10000000, 100000000]\n",
    "slow_times = []\n",
    "fast_times = []\n",
    "\n",
    "for model_size in model_sizes:\n",
    "    total_time_slow = 0\n",
    "    total_time_fast = 0\n",
    "    num_trials = 5000 if model_size <= 1000000 else 100\n",
    "    model_size = Const(model_size)\n",
    "    obs = C[\"obs\"].set(\n",
    "        1.0,\n",
    "    )\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    # create a trace from the model of the right size\n",
    "    tr, _ = jax.jit(model.importance, static_argnums=(2))(subkey, obs, (model_size,))\n",
    "\n",
    "    # warm up run to trigger jit compilation\n",
    "    jitted = jax.jit(metropolis_hastings_move, static_argnums=(2))\n",
    "    jitted(subkey, tr, False)\n",
    "    jitted(subkey, tr, True)\n",
    "\n",
    "    # measure time for each algorithm\n",
    "    key, subkey = jax.random.split(key)\n",
    "    total_time_slow = timeit.timeit(\n",
    "        lambda: jitted(subkey, tr, False), number=num_trials\n",
    "    )\n",
    "    total_time_fast = timeit.timeit(lambda: jitted(subkey, tr, True), number=num_trials)\n",
    "    average_time_slow = total_time_slow / num_trials\n",
    "    average_time_fast = total_time_fast / num_trials\n",
    "    slow_times.append(average_time_slow)\n",
    "    fast_times.append(average_time_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# First half of the values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    model_sizes[: len(model_sizes) // 2],\n",
    "    [time * 1000 for time in slow_times[: len(slow_times) // 2]],\n",
    "    marker=\"o\",\n",
    "    label=\"No incremental computation\",\n",
    ")\n",
    "plt.plot(\n",
    "    model_sizes[: len(model_sizes) // 2],\n",
    "    [time * 1000 for time in fast_times[: len(fast_times) // 2]],\n",
    "    marker=\"o\",\n",
    "    label=\"Default incremental computation\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Argument (n)\")\n",
    "plt.ylabel(\"Average Time (milliseconds)\")\n",
    "plt.title(\"Average Execution Time of MH move for different model sizes (First Half)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Second half of the values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    model_sizes[len(model_sizes) // 2 :],\n",
    "    [time * 1000 for time in slow_times[len(slow_times) // 2 :]],\n",
    "    marker=\"o\",\n",
    "    label=\"No incremental computation\",\n",
    ")\n",
    "plt.plot(\n",
    "    model_sizes[len(model_sizes) // 2 :],\n",
    "    [time * 1000 for time in fast_times[len(fast_times) // 2 :]],\n",
    "    marker=\"o\",\n",
    "    label=\"Default incremental computation\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Argument (n)\")\n",
    "plt.ylabel(\"Average Time (milliseconds)\")\n",
    "plt.title(\"Average Execution Time of MH move for different model sizes (Second Half)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
