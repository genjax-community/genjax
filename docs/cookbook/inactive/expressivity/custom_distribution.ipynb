{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### How do I create a custom distribution in GenJAX? [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChiSym/genjax/blob/main/docs/cookbook/inactive/expressivity/custom_distribution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install --quiet \"genjax[genstudio]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import Distribution, ExactDensity, Pytree, Weight, gen, normal, pretty\n",
    "from genjax.typing import PRNGKey\n",
    "\n",
    "tfd = tfp.distributions\n",
    "key = jax.random.key(0)\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GenJAX, there are two simple ways to extend the language by adding custom distributions which can be seamlessly used by the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way is to add a distribution for which we can compute its density exactly. \n",
    "In this case the API follows what one expects: one method to sample and one method to compute logpdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class NormalInverseGamma(ExactDensity):\n",
    "    def sample(self, key: PRNGKey, μ, σ, α, β):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x = tfd.Normal(μ, σ).sample(seed=key)\n",
    "        y = tfd.InverseGamma(α, β).sample(seed=subkey)\n",
    "        return (x, y)\n",
    "\n",
    "    def logpdf(self, v, μ, σ, α, β):\n",
    "        x, y = v\n",
    "        a = tfd.Normal(μ, σ).log_prob(x)\n",
    "        b = tfd.InverseGamma(α, β).log_prob(y)\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a particular instance of the distribution\n",
    "norm_inv_gamma = NormalInverseGamma()\n",
    "\n",
    "\n",
    "@gen\n",
    "def model():\n",
    "    (x, y) = norm_inv_gamma(0.0, 1.0, 1.0, 1.0) @ \"xy\"\n",
    "    z = normal(x, y) @ \"z\"\n",
    "    return z\n",
    "\n",
    "\n",
    "# Sampling from the model\n",
    "key, subkey = jax.random.split(key)\n",
    "jax.jit(model.simulate)(key, ())\n",
    "\n",
    "# Computing density of joint\n",
    "jax.jit(model.assess)(C[\"xy\"].set((2.0, 2.0)) | C[\"z\"].set(2.0), ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is to create a distribution via the `Distribution` class. \n",
    "Here, the `logpdf` method is replace by the more general `estimate_logpdf` method. The distribution is asked to return an unbiased density estimate of its logpdf at the provided value. \n",
    "The `sample` method is replaced by `random_weighted`. It returns a sample from the distribution as well as an unbiased estimate of the reciprocal density, i.e. an estimate of $\\frac{1}{p(x)}$.\n",
    "Here we'll create a simple mixture of Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class GaussianMixture(Distribution):\n",
    "    # It can have static args\n",
    "    bias: float = Pytree.static(default=0.0)\n",
    "\n",
    "    # For distributions that can compute their densities exactly, `random_weighted` should return a sample x and the reciprocal density 1/p(x).\n",
    "    def random_weighted(self, key: PRNGKey, probs, means, vars) -> tuple[Weight, any]:\n",
    "        # making sure that the inputs are jnp arrays for jax compatibility\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "\n",
    "        # sampling from the categorical distribution and then sampling from the normal distribution\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(\n",
    "            loc=means[cat_index] + jnp.asarray(self.bias), scale=vars[cat_index]\n",
    "        )\n",
    "        key, subkey = jax.random.split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "\n",
    "        # calculating the reciprocal density\n",
    "        zipped = jnp.stack([probs, means, vars], axis=1)\n",
    "        weight_recip = -jnp.log(\n",
    "            sum(\n",
    "                jax.vmap(\n",
    "                    lambda z: tfd.Normal(\n",
    "                        loc=z[1] + jnp.asarray(self.bias), scale=z[2]\n",
    "                    ).prob(normal_sample)\n",
    "                    * tfd.Categorical(probs=probs).prob(z[0])\n",
    "                )(zipped)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return weight_recip, normal_sample\n",
    "\n",
    "    # For distributions that can compute their densities exactly, `estimate_logpdf` should return the log density at x.\n",
    "    def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -> Weight:\n",
    "        zipped = jnp.stack([probs, means, vars], axis=1)\n",
    "        return jnp.log(\n",
    "            sum(\n",
    "                jax.vmap(\n",
    "                    lambda z: tfd.Normal(\n",
    "                        loc=z[1] + jnp.asarray(self.bias), scale=z[2]\n",
    "                    ).prob(x)\n",
    "                    * tfd.Categorical(probs=probs).prob(z[0])\n",
    "                )(zipped)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_mix = GaussianMixture(0.0)\n",
    "\n",
    "\n",
    "@gen\n",
    "def model(probs):\n",
    "    mix1 = gauss_mix(probs, jnp.array([0.0, 1.0]), jnp.array([1.0, 1.0])) @ \"mix1\"\n",
    "    mix2 = gauss_mix(probs, jnp.array([0.0, 1.0]), jnp.array([1.0, 1.0])) @ \"mix2\"\n",
    "    return mix1, mix2\n",
    "\n",
    "\n",
    "probs = jnp.array([0.5, 0.5])\n",
    "\n",
    "# Sampling from the model\n",
    "key, subkey = jax.random.split(key)\n",
    "jax.jit(model.simulate)(subkey, (probs,))\n",
    "\n",
    "# Computing density of joint\n",
    "key, subkey = jax.random.split(key)\n",
    "jax.jit(model.importance)(subkey, C[\"mix1\"].set(3.0) | C[\"mix2\"].set(4.0), (probs,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
