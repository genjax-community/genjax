{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Details on random_weighted and estimate_logpdf [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChiSym/genjax/blob/main/docs/cookbook/inactive/expressivity/stochastic_probabilities_math.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install --quiet \"genjax[genstudio]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `estimate_logpdf`. \n",
    "We have that the marginal distribution over the returned value `x` (the sample from the normal distribution) is given by\n",
    "$$p(x) = \\sum_i p(x\\mid z=i) p(z=i)$$ \n",
    "where the sum is over the possible values of the categorical distribution, $p(x|z=i)$  is the density of the $i$-th normal at $x$, and $p(z=i)$ is the density of the categorical at the value $i$.\n",
    "\n",
    "This sum can be rewritten as the expectation under the categorical distribution $p(z)$: \n",
    "$$\\sum_i p(x\\mid z=i)p(z=i) = \\mathbb{E}_{z\\sim p(z)}[p(x\\mid z)]$$  \n",
    "This means we can get an unbiased estimate of the expectation by simply sampling a `z` and returning `p(x|z)`: the average value of this process is obviously its expectation (it's the definition on the expectation).\n",
    "In other words, we proved that the estimation strategy used in `estimate_logpdf` indeed returns an unbiased estimate of the exact marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, as we discussed above we cannot in general invert an unbiased estimate to get an unbiased estimate of the reciprocal, so one may be suspicious that the returned weight in `random_weighted` looks like the negation (in logspace) of the one returned in `estimate_logpdf`. \n",
    "Here the argument is different, based on the following identity: \n",
    "$$\\frac{1}{p(x)} = \\mathbb{E}_{z\\sim p(z\\mid x)}[\\frac{1}{p(x\\mid z)}]$$\n",
    "The idea is that we can get an unbiased estimate if we can sample from the posterior $p(z|x)$. Given an $x$, this is an intractable sampling problem in general. However, in `random_weighted`, we sample a $z$ together with the $x$, and this $z$ is an exact posterior sample of $z$ that we get \"for free\". \n",
    "Now to finish the explanation, the compact way to prove the identity is as follows.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\frac{1}{p(x)} &\\\\\n",
    "= \\frac{1}{p(x)} \\mathbb{E}_{z \\sim B}[p(z)] & \\text{$p(z)$ density w.r.t. base measure $B$ and of total mass 1}\\\\\n",
    "= \\frac{1}{p(x)} \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z\\mid x)}]   &\\text{seeing $p(z|x)$ as an importance sampler for $B$}\\\\\n",
    "= \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z\\mid x)p(x)}]  & \\text{$p(x)$ doesn't depend on $z$ moved within the expectation}\\\\\n",
    "= \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z,x)}]   & \\text{ definition of joint distribution}\\\\\n",
    "= \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z)p(x|z)}] & \\text{definition of conditional distribution}\\\\\n",
    "=  \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{1}{p(x|z)}]   & \\text{simplification}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
