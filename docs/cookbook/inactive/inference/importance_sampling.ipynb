{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Importance sampling\n",
    "subtitle: I want to do my first inference task, how do I do it?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do it with importance sampling, which works as follows. We choose a distribution $q$ called a proposal that you we will sample from, and we need a distribution $p$ of interest, typically representing a posterior from a model having received observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import jit, vmap\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import Target, bernoulli, beta, gen, pretty, smc\n",
    "\n",
    "key = jax.random.key(0)\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at a simple python version of the algorithm to get the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sample(model, proposal):\n",
    "    def _inner(key, model_args, proposal_args):\n",
    "        # we sample from the easy distribution, the proposal `q`\n",
    "        trace = proposal.simulate(key, *proposal_args)\n",
    "        chm = trace.get_choices()\n",
    "        # we evaluate the score of the easy distribution q(x)\n",
    "        proposal_logpdf = trace.get_score()\n",
    "        # we evaluate the score of the hard distribution p(x)\n",
    "        model_logpdf, _ = model.assess(chm, *model_args)\n",
    "        # the importance weight is p(x)/q(x), which corrects for the bias from sampling from q instead of p\n",
    "        importance_weight = model_logpdf - proposal_logpdf\n",
    "        return (trace, importance_weight)\n",
    "        # we return the trace and the importance weight\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this on a very simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genjax.normal\n",
    "proposal = genjax.normal\n",
    "\n",
    "model_args = (0.0, 1.0)\n",
    "proposal_args = (3.0, 4.0)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "sample, importance_weight = jit(importance_sample(model, proposal))(\n",
    "    subkey, (model_args,), (proposal_args,)\n",
    ")\n",
    "print(importance_weight, sample.get_choices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run it in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted = jit(\n",
    "    vmap(\n",
    "        importance_sample(model, proposal),\n",
    "        in_axes=(0, None, None),\n",
    "    )\n",
    ")\n",
    "key, *sub_keys = jax.random.split(key, 100 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "(sample, importance_weight) = jitted(sub_keys, (model_args,), (proposal_args,))\n",
    "sample.get_choices(), importance_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GenJAX, every generative function comes equipped with a default proposal which we can use for importance sampling. \n",
    "\n",
    "Let's define a generative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(1.0, u) @ \"p\"\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By giving constraints to some of the random samples, which we call observations, we obtain a posterior inference problem where the goal is to infer the distribution of the random variables which are not observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = C[\"v\"].set(1)\n",
    "args = (0.5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `.importance` defines a default proposal based on the generative function which targets the posterior distribution we just defined. \n",
    "It returns a pair containing a trace and the log incremental weight. \n",
    "This weight corrects for the bias from sampling from the proposal instead of the intractable posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "trace, weight = beta_bernoulli_process.importance(subkey, obs, args)\n",
    "\n",
    "trace, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "K = 100\n",
    "\n",
    "\n",
    "def SIR(N, K, model, chm):\n",
    "    @jit\n",
    "    def _inner(key, args):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        traces, weights = vmap(model.importance, in_axes=(0, None, None))(\n",
    "            jax.random.split(key, N), chm, args\n",
    "        )\n",
    "        idxs = vmap(jax.jit(genjax.categorical.simulate), in_axes=(0, None))(\n",
    "            jax.random.split(subkey, K), (weights,)\n",
    "        ).get_retval()\n",
    "        samples = traces.get_choices()\n",
    "        resampled_samples = vmap(lambda idx: jtu.tree_map(lambda v: v[idx], samples))(\n",
    "            idxs\n",
    "        )\n",
    "        return resampled_samples\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = C[\"v\"].set(1)\n",
    "args = (0.5,)\n",
    "key, subkey = jax.random.split(key)\n",
    "samples = jit(SIR(N, K, beta_bernoulli_process, chm))(subkey, args)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do the basically the same thing using library functions.\n",
    "\n",
    "To do this, we first define a Target for importance sampling, i.e. the posterior inference problem we're targetting. It consists of a generative function, arguments to the generative function, and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_posterior = Target(beta_bernoulli_process, (args,), chm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an inference strategy algorithm (Algorithm class) to use to approximate the target distribution. \n",
    "\n",
    "It's importance sampling with $N$ particles in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = smc.ImportanceK(target_posterior, k_particles=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a different sense of what's going on, the hierarchy of classes is as follows:\n",
    "\n",
    "`ImportanceK <: SMCAlgorithm <: Algorithm <: SampleDistribution <: Distribution <: GenerativeFunction <: Pytree`\n",
    "\n",
    "In words, importance sampling (`ImportanceK`) is a particular instance of Sequential Monte Carlo ( `SMCAlgorithm`). The latter is one instance of approximate inference strategy (`Algorithm`). \n",
    "An inference strategy in particular produces samples for a distribution (`SampleDistribution`), which is a distribution (`Distribution`) whose return value is the sample. A distribution here is the definition from GenSP (Lew et al 2023) which has two methods `random_weighted` and `estimate_logpdf`. See the appropriate cookbook for details on these.\n",
    "Finally, a distribution is a particular case of generative function (`GenerativeFunction`), which are all pytrees (`Pytree`) to be JAX-compatible and in particular jittable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get K independent samples from the approximate posterior distribution, we can for instance use `vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a bit different from the previous example, because each of the final\n",
    "# K samples is obtained by running a different set of N-particles.\n",
    "# This can of course be optimized but we keep it simple here.\n",
    "jitted = jit(vmap(alg.simulate, in_axes=(0, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, *sub_keys = jax.random.split(key, K + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "posterior_samples = jitted(sub_keys, (target_posterior,)).get_retval()\n",
    "\n",
    "# This only does the importance sampling step, not the resampling step\n",
    "# Therefore the shape is (K, N, 1)\n",
    "posterior_samples[\"p\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the mean value estimate for `\"p\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"p\"].mean(axis=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we compare the relative difference with the one obtained using the previous method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100.0 * jnp.abs(\n",
    "    samples[\"p\"].mean() - posterior_samples[\"p\"].mean(axis=(0, 1))\n",
    ") / posterior_samples[\"p\"].mean(axis=(0, 1))  # about 2% difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
