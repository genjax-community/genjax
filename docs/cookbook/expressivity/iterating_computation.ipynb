{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a generative function with a single variable but 2000 observations or I just want to use/apply it repeatedly, what do I do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 1 1 2 1 2 2 1 1 2 2 0 2 1 0 1 2 1]\n",
      "[2 0 2 1 1 2 1 2 2 1 1 2 2 0 2 1 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import genjax\n",
    "import jax\n",
    "from genjax import bernoulli\n",
    "from genjax import gen\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# First start by creating a simple generative function\n",
    "@gen\n",
    "def double_flip(p,q):\n",
    "    v1 = bernoulli(p) @ \"v1\" \n",
    "    v2 = bernoulli(q) @ \"v2\" \n",
    "    return v1+v2\n",
    "\n",
    "# Now we can create a vectorized version that takes a batch of p values\n",
    "# and calls the function for each value in the batch.\n",
    "# The `in_axes` tell the `vmap_combinator` which arguments are \n",
    "# mapped over, and which are not. \n",
    "# The value `0` means we will map over this argument and `None` means we will not.\n",
    "batched_double_flip = genjax.vmap_combinator(double_flip, in_axes=(0,None))\n",
    "\n",
    "# Now we can use the batched version to generate a batch of samples\n",
    "key = jax.random.PRNGKey(0)\n",
    "size_of_batch = 20\n",
    "# To do so, we have to create batched keys and p values\n",
    "p = jax.random.uniform(key, (size_of_batch,))\n",
    "q = 0.5\n",
    "# We will run the generative function once for (p1, q), once for (p2, q), ...\n",
    "traces = batched_double_flip.simulate(key, (p, q))\n",
    "print(traces.get_retval())\n",
    "\n",
    "# We can also use call it on (p1,q1), (p2,q2), ...\n",
    "p = jax.random.uniform(key, (size_of_batch,))\n",
    "q = jax.random.uniform(key, (size_of_batch,))\n",
    "batched_double_flip_v2 = genjax.vmap_combinator(double_flip, in_axes = (0, 0))\n",
    "traces = batched_double_flip_v2.simulate(key, (p, q))\n",
    "print(traces.get_retval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The batched version of the generative function is not working correctly\n"
     ]
    }
   ],
   "source": [
    "# Note: We cannot batch different variables with different shapes\n",
    "try:\n",
    "    p = jax.random.uniform(key, (size_of_batch,))\n",
    "    q = jax.random.uniform(key, (size_of_batch + 1,))\n",
    "    traces = batched_double_flip_v2.simulate(key, (p, q))\n",
    "    print(traces.get_retval())\n",
    "except:\n",
    "    print(\"Error: The batched version of the generative function is not working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about iterating `vmap`, e.g. if we want to apply a generative function acting on a pixel over a 2D space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_pixel: 1.3694694\n",
      "IdxChm(addr=<jax.Array int32(300,) [≥0, ≤299] zero:1 nonzero:299>, c=IdxChm(addr=<jax.Array int32(300, 500)>, c=StaticChm(addr='new_pixel', c=ValueChm(v=<jax.Array float32(300, 500)>))))\n",
      "2D space choicemap: Mask(flag=<jax.Array(True, dtype=bool)>, value=<jax.Array(0.80852467, dtype=float32)>)\n",
      "2D space choicemap: Mask(flag=<jax.Array(True, dtype=bool)>, value=<jax.Array(0.32702535, dtype=float32)>)\n",
      "sampled_image: [[ 0.13146028 -1.1258085  -0.3549479 ]\n",
      " [ 0.6431166   0.59783465  0.7178338 ]]\n",
      "\n",
      "sampled_image: [[ 0.15701398  0.8691822  -0.88012   ]\n",
      " [-1.0565741  -0.9048738  -0.4763998 ]]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "image = jnp.zeros([300,500], dtype = jnp.float32)\n",
    "\n",
    "# Generative function on one \"pixel\" value\n",
    "@gen\n",
    "def sample_pixel(pixel):\n",
    "    new_pixel = genjax.normal(pixel, 1.0) @ \"new_pixel\"\n",
    "    return new_pixel\n",
    "\n",
    "tr = sample_pixel.simulate(key, (0.0,))\n",
    "print(\"new_pixel:\", tr.get_sample()[\"new_pixel\"])\n",
    "\n",
    "# Now what if we want to apply a generative function over a 2D space? \n",
    "# We can use a nested `vmap_combinator`\n",
    "sample_image = genjax.vmap_combinator(in_axes = (0,))(genjax.vmap_combinator(in_axes=(0,))(sample_pixel))\n",
    "\n",
    "tr = sample_image.simulate(key,(image,))\n",
    "#We can access the new_pixel value for each pixel in the image\n",
    "print(tr.get_sample())\n",
    "print(\"2D space choicemap:\", tr.get_sample()[0, 0, \"new_pixel\"])\n",
    "print(\"2D space choicemap:\", tr.get_sample()[299, 499, \"new_pixel\"])\n",
    "\n",
    "# Model wrapped in a bigger model\n",
    "image = jnp.zeros([2,3], dtype=jnp.float32)\n",
    "@gen\n",
    "def model(p):\n",
    "    sampled_image = sample_image(image) @ \"sampled_image\"\n",
    "    return sampled_image[0]+p\n",
    "\n",
    "tr = model.simulate(key,(0.0,))\n",
    "# We can use ellipsis to access the new_pixel value for each pixel in the image\n",
    "print(\"sampled_image:\", tr.get_sample()[\"sampled_image\",...,...,\"new_pixel\"])\n",
    "print()\n",
    "\n",
    "# Alternatively, we can flatten the 2 dimensions into one and use a single `vmap_combinator`. \n",
    "# This can be more efficient in some cases and usually has a faster compile time.\n",
    "sample_image_flat = genjax.vmap_combinator(in_axes=(0,))(sample_pixel)\n",
    "tr = sample_image_flat.simulate(key,(image.flatten(),))\n",
    "# resize the sample to the original shape\n",
    "out_image = tr.get_sample()[...,\"new_pixel\"].reshape(image.shape)\n",
    "print(\"sampled_image:\", out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh but my iteration is actually over time, not space, i.e. I may want to reuse the same model by composing it with itself, e.g. for an HMM. \n",
    "For this, we can use the `scan` combinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of z at the beginning: Mask(flag=<jax.Array(True, dtype=bool)>, value=<jax.Array(-1.6903946, dtype=float32)>)\n",
      "Value of y at the end: Mask(flag=<jax.Array(True, dtype=bool)>, value=<jax.Array(-1.7305961, dtype=float32)>)\n",
      "[-1.6903946  0.6667787 -1.222213  -3.3950129 -2.656077  -1.0463438\n",
      " -2.2370918 -2.027141  -2.7775748 -3.7542048]\n",
      "Mask(flag=<jax.Array(True, dtype=bool)>, value=<jax.Array(-0.06991103, dtype=float32)>)\n",
      "Mask(flag=<jax.Array(True, dtype=bool)>, value=<jax.Array(4.7136474, dtype=float32)>)\n",
      "[-0.06991103  0.25033963 -0.5039929   0.12699662 -1.3981494  -0.7294941\n",
      " -0.4651397  -0.539045    1.7028391   3.6123393 ]\n"
     ]
    }
   ],
   "source": [
    "# Simple kernel for a Hidden Markov Model (HMM) example.\n",
    "@gen\n",
    "def hmm_kernel(x):\n",
    "    z = genjax.normal(x, 1.0) @ \"z\"\n",
    "    y = genjax.normal(z, 1.0) @ \"y\"\n",
    "    return y\n",
    "\n",
    "# Now we can create a function that runs the kernel multiple times\n",
    "@genjax.scan_combinator(max_length=10)\n",
    "@gen\n",
    "def hmm(x, c):\n",
    "    x1 = hmm_kernel(x) @ \"x1\"\n",
    "    return x1, None\n",
    "\n",
    "# Alternatively, we can directly create the same HMM model\n",
    "@genjax.scan_combinator(max_length=10)\n",
    "@gen\n",
    "def hmm_v2(x, c):\n",
    "    z = genjax.normal(x, 1.0) @ \"z\"\n",
    "    y = genjax.normal(z, 1.0) @ \"y\"\n",
    "    return y, None\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey = jax.random.split(key)\n",
    "initial_x = 0.0\n",
    "tr_1 = hmm.simulate(key, (initial_x, None))\n",
    "print(\"Value of z at the beginning:\", tr_1.get_sample()[0, \"x1\",\"z\"])\n",
    "print(\"Value of y at the end:\", tr_1.get_sample()[9, \"x1\",\"y\"])\n",
    "print(tr_1.get_sample()[..., \"x1\",\"z\"])\n",
    "\n",
    "tr_2 = hmm_v2.simulate(subkey, (initial_x, None))\n",
    "print(tr_2.get_sample()[0, \"z\"])\n",
    "print(tr_2.get_sample()[9,\"y\"])\n",
    "print(tr_2.get_sample()[...,\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0018916 3.0128644 3.0139802 3.026225  3.0105915 3.0164824 3.002301\n",
      " 3.0027137 3.0074248 2.9685202]\n",
      "\n",
      "[3.0059068 3.0282145 3.00671   3.0299015 3.0067174 3.0182729 3.0149763\n",
      " 3.0054522 3.0251455 2.9722319]\n",
      "\n",
      "IdxChm(addr=<jax.Array([0, 1, 2], dtype=int32)>, c=AddrMapChm(addr_map={Ellipsis: '_internal'}, c=StaticChm(addr='_internal', c=XorChm(c1=StaticChm(addr='x', c=ValueChm(v=<jax.Array float32(3, 3, 5) ≈3.0 ±1.4 [≥0.99, ≤5.0] nonzero:45>)), c2=StaticChm(addr='y', c=ValueChm(v=<jax.Array float32(3, 3, 5) ≈3.0 ±1.4 [≥0.99, ≤5.0] nonzero:45>))))))\n",
      "\n",
      "[[[1.0132105  2.0106866  3.020013   3.9952815  4.9921556 ]\n",
      "  [1.0091456  2.0109982  3.0036159  4.011836   5.004953  ]\n",
      "  [0.9880202  2.0039277  2.9946487  4.0137424  4.9885173 ]]\n",
      "\n",
      " [[0.9929348  2.0283499  2.995508   3.9934149  5.0182095 ]\n",
      "  [1.0136458  1.982539   3.0122788  3.9929068  5.0067253 ]\n",
      "  [1.0344796  2.002393   3.0159934  3.9936728  4.9772143 ]]\n",
      "\n",
      " [[0.99285704 2.001703   3.0308223  3.9815516  5.0091867 ]\n",
      "  [0.99326736 1.9940647  3.017618   3.9916716  5.0125604 ]\n",
      "  [1.0013216  1.9915222  3.0150259  4.0063915  5.010889  ]]]\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can call the generative function with a repeat combinator\n",
    "# This will run the generative function multiple times on a single argument and return the results\n",
    "@genjax.gen\n",
    "def model(y):\n",
    "    x = genjax.normal(y, 0.01) @ \"x\"\n",
    "    y = genjax.normal(x, 0.01) @ \"y\"\n",
    "    return y\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = 3.0\n",
    "tr = model.repeat(num_repeats=10).simulate(key, (arg,))\n",
    "\n",
    "print(tr.get_sample()[...,\"x\"])\n",
    "print()\n",
    "print(tr.get_retval())\n",
    "print()\n",
    "\n",
    "# It can be combined with vmap\n",
    "sub_keys = jax.random.split(key, 3)\n",
    "args = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "num_repeats = 3\n",
    "tr = jax.jit(jax.vmap(model.repeat(num_repeats=num_repeats).simulate, in_axes=(0, None)))(sub_keys, (args,))\n",
    "print(tr.get_sample())\n",
    "print()\n",
    "\n",
    "# Note that it's running a computation |keys| * |args| * |num_repeats| times,\n",
    "# i.e. 45 times in this case\n",
    "print(tr.get_retval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genjax-trials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
