{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, random\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import beta, gen, pretty\n",
    "\n",
    "key = jax.random.key(0)\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. JAX expects arrays/tuples everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def f(p):\n",
    "    v = genjax.bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "# First way of failing\n",
    "key, subkey = jax.random.split(key)\n",
    "try:\n",
    "    f.simulate(key, 0.5)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Second way of failing\n",
    "key, subkey = jax.random.split(key)\n",
    "try:\n",
    "    f.simulate(subkey, [0.5])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Third way of failing\n",
    "key, subkey = jax.random.split(key)\n",
    "try:\n",
    "    f.simulate(subkey, (0.5))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Correct way\n",
    "key, subkey = jax.random.split(key)\n",
    "f.simulate(subkey, (0.5,)).get_retval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GenJAX relies on Tensor Flow Probability and it sometimes does unintuitive things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli distribution uses logits instead of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def g(p):\n",
    "    v = genjax.bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "arg = (3.0,)  # 3 is not a valid probability but a valid logit\n",
    "keys = jax.random.split(subkey, 30)\n",
    "# simulate 30 times\n",
    "jax.vmap(g.simulate, in_axes=(0, None))(keys, arg).get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values which are stricter than $0$ are considered to be the value True.\n",
    "This means that observing that the value of `\"v\"` is $4$ will be considered possible while intuitively `\"v\"` should only have support on $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = C[\"v\"].set(3)\n",
    "g.assess(chm, (0.5,))[0]  # This should be -inf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the flip function which uses probabilities instead of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def h(p):\n",
    "    v = genjax.flip(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "arg = (0.3,)  # 0.3 is a valid probability\n",
    "keys = jax.random.split(subkey, 30)\n",
    "# simulate 30 times\n",
    "jax.vmap(h.simulate, in_axes=(0, None))(keys, arg).get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical distributions also use logits instead of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def i(p):\n",
    "    v = genjax.categorical(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "arg = ([3.0, 1.0, 2.0],)  # lists of 3 logits\n",
    "keys = jax.random.split(subkey, 30)\n",
    "# simulate 30 times\n",
    "jax.vmap(i.simulate, in_axes=(0, None))(keys, arg).get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. JAX code can be compiled for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jit` is the way to force JAX to compile the code.\n",
    "It can be used as a decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def f_v1(p):\n",
    "    return jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_v2 = jit(lambda p: jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the effect. Notice that the first and second have the same performance while the third is much slower (~50x on a mac m2 cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "def f_v3(p):\n",
    "    jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n",
    "\n",
    "\n",
    "arg = jax.numpy.eye(500)\n",
    "# Warmup to force jit compilation\n",
    "f_v1(arg)\n",
    "f_v2(arg)\n",
    "# Runtime comparison\n",
    "%timeit f_v1(arg)\n",
    "%timeit f_v2(arg)\n",
    "%timeit f_v3(arg)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Going from Python to JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 For loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_loop(x):\n",
    "    for i in range(100):\n",
    "        x = 2 * x\n",
    "    return x\n",
    "\n",
    "\n",
    "def jax_loop(x):\n",
    "    jax.lax.fori_loop(0, 100, lambda i, x: 2 * x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Conditional statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_cond(x):\n",
    "    if x.sum() > 0:\n",
    "        return x * x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def jax_cond(x):\n",
    "    jax.lax.cond(x.sum(), lambda x: x * x, lambda x: x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 While loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_while(x):\n",
    "    while x.sum() > 0:\n",
    "        x = x * x\n",
    "    return x\n",
    "\n",
    "\n",
    "def jax_while(x):\n",
    "    jax.lax.while_loop(lambda x: x.sum() > 0, lambda x: x * x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Is my thing compiling or is it blocked at traced time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jax, the first time you run a function, it is traced, which produces a Jaxpr, a representation of the computation that Jax can optimize.\n",
    "\n",
    "So in order to debug whether a function is running or not, if it passes the first check that Python let's you write it, you can check if it is running by checking if it is traced, before actually running it on data.\n",
    "\n",
    "\n",
    "This is done by calling `make_jaxpr` on the function. If it returns a Jaxpr, then the function is traced and ready to be run on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_fine(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "jax.make_jaxpr(im_fine)(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_wont_be_so_fine(x):\n",
    "    return jax.lax.while_loop(lambda x: x > 0, lambda x: x * x, x)\n",
    "\n",
    "\n",
    "jax.make_jaxpr(i_wont_be_so_fine)(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Try running the function for 8 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process():\n",
    "    ctx = multiprocessing.get_context(\"spawn\")\n",
    "    p = ctx.Process(target=i_wont_be_so_fine, args=(1.0,))\n",
    "    p.start()\n",
    "    time.sleep(5000)\n",
    "    if p.is_alive():\n",
    "        print(\"I'm still running\")\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"genjax/docs/sharp-edges-notebooks/basics/script.py\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "result.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Using random keys for generative functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GenJAX, we use explicit random keys to generate random numbers. This is done by splitting a key into multiple keys, and using them to generate random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(0.0, u) @ \"p\"\n",
    "    v = genjax.bernoulli(p) @ \"v\"  # sweet\n",
    "    return v\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, 20)\n",
    "jitted = jit(beta_bernoulli_process.simulate)\n",
    "\n",
    "jax.vmap(jitted, in_axes=(0, None))(keys, (0.5,)).get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. JAX uses 32-bit floats by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "x = random.uniform(subkey, (1000,), dtype=jnp.float64)\n",
    "print(\"surprise surprise: \", x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A common TypeError occurs when one tries using np instead of jnp, which is the JAX version of numpy, the former uses 64-bit floats by default, while the JAX version uses 32-bit floats by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This on its own gives a UserWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.array([1, 2, 3], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an array from `numpy` instead of `jax.numpy` will truncate the array to 32-bit floats and also give a UserWarning when used in JAX code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innocent_looking_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def innocent_looking_function(x):\n",
    "    return jax.lax.cond(x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x)\n",
    "\n",
    "\n",
    "input = jnp.array([1.0, 2.0, 3.0])\n",
    "innocent_looking_function(input)\n",
    "\n",
    "try:\n",
    "    # This looks fine so far but...\n",
    "    innocent_looking_array = np.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "    # This actually raises a TypeError, as one branch has type float32\n",
    "    # while the other has type float64\n",
    "    @jax.jit\n",
    "    def innocent_looking_function(x):\n",
    "        return jax.lax.cond(\n",
    "            x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x\n",
    "        )\n",
    "\n",
    "    input = jnp.array([1, 2, 3])\n",
    "    innocent_looking_function(input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Beware to OOM on the GPU which happens faster than you might think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple HMM model that can be run on the GPU.\n",
    "By simply changing $N$ from $300$ to $1000$, the code will typically run out of memory on the GPU as it will take ~300GB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "n_repeats = 100\n",
    "variance = jnp.eye(N)\n",
    "key, subkey = jax.random.split(key)\n",
    "initial_state = jax.random.normal(subkey, (N,))\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm_step(x, _):\n",
    "    new_x = genjax.mv_normal(x, variance) @ \"new_x\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "hmm = hmm_step.scan(n=100)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "jitted = jit(hmm.repeat(n=n_repeats).simulate)\n",
    "trace = jitted(subkey, (initial_state, None))\n",
    "key, subkey = jax.random.split(key)\n",
    "%timeit jitted(subkey, (initial_state, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running out of memory, you can try de-batching one of the computations, or using a smaller batch size. For instance, in this example, we can de-batch the `repeat` combinator, which will reduce the memory usage by a factor of $100$, at the cost of some performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted = jit(hmm.simulate)\n",
    "\n",
    "\n",
    "def hmm_debatched(key, initial_state):\n",
    "    keys = jax.random.split(key, n_repeats)\n",
    "    traces = {}\n",
    "    for i in range(n_repeats):\n",
    "        trace = jitted(keys[i], (initial_state, None))\n",
    "        traces[i] = trace\n",
    "    return traces\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "# About 4x slower on arm64 CPU and 40x on a Google Colab GPU\n",
    "%timeit hmm_debatched(subkey, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Fast sampling can be inaccurate and yield Nan/wrong results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, truncating a normal distribution outside 5.5 standard deviations from its mean can yield NaNs. Many default TFP/JAX implementations that run on the GPU use fast implementations on 32bits. If one really wants that, one could use slower implementations that use 64bits and an exponential tilting Monte Carlo algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genjax.truncated_normal.sample(\n",
    "    jax.random.key(2), 0.5382424, 0.05, 0.83921564 - 0.03, 0.83921564 + 0.03\n",
    ")\n",
    "\n",
    "minv = 0.83921564 - 0.03\n",
    "maxv = 0.83921564 + 0.03\n",
    "mean = 0.5382424\n",
    "std = 0.05\n",
    "\n",
    "\n",
    "def raw_jax_truncated(key, minv, maxv, mean, std):\n",
    "    low = (minv - mean) / std\n",
    "    high = (maxv - mean) / std\n",
    "    return std * jax.random.truncated_normal(key, low, high, (), jnp.float32) + mean\n",
    "\n",
    "\n",
    "raw_jax_truncated(jax.random.key(2), minv, maxv, mean, std)\n",
    "# ==> Array(0.80921566, dtype=float32)\n",
    "\n",
    "jax.jit(raw_jax_truncated)(jax.random.key(2), minv, maxv, mean, std)\n",
    "# ==> Array(nan, dtype=float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
