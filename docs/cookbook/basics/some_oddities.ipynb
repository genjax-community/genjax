{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some sharp edges of Jax that are good to know before really starting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax import bernoulli, gen\n",
    "import jax\n",
    "from jax import jit\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "import jax.numpy as jnp\n",
    "import multiprocessing\n",
    "import time\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1] JAX expects arrays/tuples everywhere\n",
    "@gen\n",
    "def f(p):\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "# First way of failing\n",
    "try:\n",
    "    f.simulate(key, 0.5)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Second way of failing\n",
    "try:\n",
    "    f.simulate(key, [0.5])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Third way of failing\n",
    "try:\n",
    "    f.simulate(key, (0.5))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Correct way\n",
    "f.simulate(key, (0.5,)).get_retval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2] We rely on Tensor Flow Probability and it sometimes does unintuitive things.\n",
    "\n",
    "# Bernoulli distribution uses logits instead of probabilities\n",
    "@gen\n",
    "def g(p):\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = (3.0,)  # 3 is not a valid probability but a valid logit\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([g.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "\n",
    "# Values which are stricter than 0 are considered to be the value True.\n",
    "# This means that observing that the value of \"v\" is 4 will be considered possible while intuitively \"v\" should only have support on 0 and 1.\n",
    "chm = C[\"v\"].set(3)\n",
    "print()\n",
    "print(g.assess(chm, (0.5,))[0])  # This should be -inf.\n",
    "print()\n",
    "\n",
    "# Alternatively, we can use the flip function which uses probabilities instead of logits.\n",
    "from genjax import flip\n",
    "\n",
    "\n",
    "@gen\n",
    "def h(p):\n",
    "    v = flip(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = (0.3,)  # 0.3 is a valid probability\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([h.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "print()\n",
    "\n",
    "# Categorical distribution also use logits instead of probabilities\n",
    "from genjax import categorical\n",
    "\n",
    "\n",
    "@gen\n",
    "def i(p):\n",
    "    v = categorical(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = ([3.0, 1.0, 2.0],)  # lists of 3 logits\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([i.simulate(key, arg).get_sample()[\"v\"] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3] JAX code can be compiled for better performance.\n",
    "\n",
    "# jit is the way to force JAX to compile the code.\n",
    "# It can be used as a decorator\n",
    "@jit\n",
    "def f_v1(p):\n",
    "    return jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n",
    "\n",
    "\n",
    "# Or as a function\n",
    "f_v2 = jit(lambda p: jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p))\n",
    "\n",
    "\n",
    "# Baseline\n",
    "def f_v3(p):\n",
    "    jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n",
    "\n",
    "\n",
    "arg = jax.numpy.eye(500)\n",
    "# Warmup to force jit compilation\n",
    "f_v1(arg)\n",
    "f_v2(arg)\n",
    "# Runtime comparison\n",
    "%timeit f_v1(arg)\n",
    "%timeit f_v2(arg)\n",
    "%timeit f_v3(arg)\n",
    "# Notice that the first and second have the same performance while the third is much slower (~50x on a mac m2 cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4] Going from Python to JAX\n",
    "# For loops\n",
    "def python_loop(x):\n",
    "    for i in range(100):\n",
    "        x = 2 * x\n",
    "    return x\n",
    "\n",
    "\n",
    "def jax_loop(x):\n",
    "    jax.lax.fori_loop(0, 100, lambda i, x: 2 * x, x)\n",
    "\n",
    "\n",
    "# Conditional statements\n",
    "def python_cond(x):\n",
    "    if x.sum() > 0:\n",
    "        return x * x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def jax_cond(x):\n",
    "    jax.lax.cond(x.sum(), lambda x: x * x, lambda x: x, x)\n",
    "\n",
    "\n",
    "# While loops\n",
    "def python_while(x):\n",
    "    while x.sum() > 0:\n",
    "        x = x * x\n",
    "    return x\n",
    "\n",
    "\n",
    "def jax_while(x):\n",
    "    jax.lax.while_loop(lambda x: x.sum() > 0, lambda x: x * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5] Is my thing compiling or is it blocked at traced time?\n",
    "\n",
    "# In Jax, the first time you run a function, it is traced, which produces a Jaxpr, a representation of the computation that Jax can optimize.\n",
    "\n",
    "# So in order to debug whether a function is running or not, if it passes the first check that Python let's you write it, you can check if it is running by checking if it is traced, before actually running it on data.\n",
    "\n",
    "\n",
    "# This is done by calling `make_jaxpr` on the function. If it returns a Jaxpr, then the function is traced and ready to be run on data.\n",
    "def im_fine(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "print(jax.make_jaxpr(im_fine)(1.0))\n",
    "print()\n",
    "\n",
    "\n",
    "def i_wont_be_so_fine(x):\n",
    "    return jax.lax.while_loop(lambda x: x > 0, lambda x: x * x, x)\n",
    "\n",
    "\n",
    "print(jax.make_jaxpr(i_wont_be_so_fine)(1.0))\n",
    "print()\n",
    "\n",
    "\n",
    "# Try running the function for 8 seconds\n",
    "def run_process():\n",
    "    ctx = multiprocessing.get_context(\"spawn\")\n",
    "    p = ctx.Process(target=i_wont_be_so_fine, args=(1.0,))\n",
    "    p.start()\n",
    "    time.sleep(8)\n",
    "    if p.is_alive():\n",
    "        print(\"I'm still running\")\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"genjax/docs/sharp-edges-notebooks/basics/script.py\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6] Using random keys for generative functions\n",
    "\n",
    "# In GenJAX, we use explicit random keys to generate random numbers. This is done by splitting a key into multiple keys, and using them to generate random numbers.\n",
    "from genjax import bernoulli, gen, beta\n",
    "\n",
    "\n",
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(0.0, u) @ \"p\"\n",
    "    v = bernoulli(p) @ \"v\"  # sweet\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key, 20)\n",
    "jitted = jit(beta_bernoulli_process.simulate)\n",
    "print(jnp.array([jitted(key, (0.5,)).get_sample()[\"v\"] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7] JAX uses 32-bit floats by default\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "x = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\n",
    "print(\"surprise surprise: \", x.dtype)\n",
    "print()\n",
    "\n",
    "# A common TypeError occurs when one tries using np instead of jnp, which is the JAX version of numpy, the former uses 64-bit floats by default, while the JAX version uses 32-bit floats by default.\n",
    "\n",
    "# This on its own gives a UserWarning\n",
    "jnp.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "# This will truncate the array to 32-bit floats and also give a UserWarning\n",
    "innocent_looking_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def innocent_looking_function(x):\n",
    "    return jax.lax.cond(x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x)\n",
    "\n",
    "\n",
    "input = jnp.array([1.0, 2.0, 3.0])\n",
    "innocent_looking_function(input)\n",
    "\n",
    "try:\n",
    "    # This actually raises a TypeError\n",
    "    innocent_looking_array = np.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "    @jax.jit\n",
    "    def innocent_looking_function(x):\n",
    "        return jax.lax.cond(\n",
    "            x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x\n",
    "        )\n",
    "\n",
    "    input = jnp.array([1, 2, 3])\n",
    "    innocent_looking_function(input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8] Beware to OOM on the GPU which happens faster than you might think\n",
    "\n",
    "# Here's a simple HMM model that can be run on the GPU.\n",
    "# By simply changing N from 300 to 1000, the code will typically run out of memory on the GPU as it will take ~300GB of memory.\n",
    "\n",
    "import genjax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "N = 300\n",
    "n_repeats = 100\n",
    "variance = jnp.eye(N)\n",
    "initial_state = jax.random.normal(jax.random.PRNGKey(0), (N,))\n",
    "\n",
    "\n",
    "@genjax.scan_combinator(max_length=100)\n",
    "@genjax.gen\n",
    "def hmm(x, c):\n",
    "    new_x = genjax.mv_normal(x, variance) @ \"new_x\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "jitted = jit(hmm.repeat(num_repeats=n_repeats).simulate)\n",
    "trace = jitted(key, (initial_state, None))\n",
    "%timeit jitted(subkey, (initial_state, None))\n",
    "\n",
    "# If you are running out of memory, you can try de-batching one of the computations, or using a smaller batch size.\n",
    "# For instance, in this example, we can de-batch the repeat combinator, which will reduce the memory usage by a factor of 100, at the cost of some performance.\n",
    "jitted = jit(hmm.simulate)\n",
    "\n",
    "\n",
    "def hmm_debatched(key, initial_state):\n",
    "    keys = jax.random.split(key, n_repeats)\n",
    "    traces = {}\n",
    "    for i in range(n_repeats):\n",
    "        trace = jitted(keys[i], (initial_state, None))\n",
    "        traces[i] = trace\n",
    "    return traces\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "# About 4x slower on arm64 CPU and 40x on a Google Colab GPU\n",
    "%timeit hmm_debatched(key, initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
