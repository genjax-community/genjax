{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some sharp edges of Jax that are good to know before really starting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method\u001b[1m\u001b[35m genjax._src.generative_functions.static.StaticGenerativeFunction.simulate()\u001b[0m parameter \u001b[1m\u001b[34margs\u001b[0m=\u001b[1m\u001b[31m0.5\u001b[0m violates type hint \u001b[1m\u001b[32m<class 'tuple'>\u001b[0m, as \u001b[1m\u001b[33mfloat \u001b[0m\u001b[1m\u001b[31m0.5\u001b[0m not instance of \u001b[1m\u001b[32mtuple\u001b[0m.\n",
      "Method\u001b[1m\u001b[35m genjax._src.generative_functions.static.StaticGenerativeFunction.simulate()\u001b[0m parameter \u001b[1m\u001b[34margs\u001b[0m=\u001b[1m\u001b[31m[0.5]\u001b[0m violates type hint \u001b[1m\u001b[32m<class 'tuple'>\u001b[0m, as \u001b[1m\u001b[33mlist \u001b[0m\u001b[1m\u001b[31m[0.5]\u001b[0m not instance of \u001b[1m\u001b[32mtuple\u001b[0m.\n",
      "Method\u001b[1m\u001b[35m genjax._src.generative_functions.static.StaticGenerativeFunction.simulate()\u001b[0m parameter \u001b[1m\u001b[34margs\u001b[0m=\u001b[1m\u001b[31m0.5\u001b[0m violates type hint \u001b[1m\u001b[32m<class 'tuple'>\u001b[0m, as \u001b[1m\u001b[33mfloat \u001b[0m\u001b[1m\u001b[31m0.5\u001b[0m not instance of \u001b[1m\u001b[32mtuple\u001b[0m.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StaticTrace(\n",
       "  gen_fn=StaticGenerativeFunction(\n",
       "    source=Closure(dyn_args=(), fn=<function f at 0x2c7b6e3e0>),\n",
       "  ),\n",
       "  args=(0.5,),\n",
       "  retval=<jax.Array(1, dtype=int32)>,\n",
       "  addresses=AddressVisitor(visited=[('v',)]),\n",
       "  subtraces=[\n",
       "    DistributionTrace(\n",
       "      gen_fn=ExactDensityFromCallables(\n",
       "        sampler=Closure(dyn_args=(), fn=<function tfp_distribution.<locals>.sampler at 0x2c589e5c0>),\n",
       "        logpdf_evaluator=Closure(dyn_args=(), fn=<function tfp_distribution.<locals>.logpdf at 0x2c589e660>),\n",
       "      ),\n",
       "      args=(0.5,),\n",
       "      value=<jax.Array(1, dtype=int32)>,\n",
       "      score=<jax.Array(-0.474077, dtype=float32)>,\n",
       "    ),\n",
       "  ],\n",
       "  score=<jax.Array(-0.474077, dtype=float32)>,\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from genjax import bernoulli, gen\n",
    "import jax\n",
    "\n",
    "#1] JAX expects arrays/tuples everywhere\n",
    "\n",
    "@gen\n",
    "def f(p):\n",
    "    v = bernoulli(p) @\"v\"\n",
    "    return v\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "# First way of failing\n",
    "try:\n",
    "    f.simulate(key, 0.5)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# Second way of failing\n",
    "try:\n",
    "    f.simulate(key, [0.5])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# Third way of failing\n",
    "try:\n",
    "    f.simulate(key, (0.5))\n",
    "except Exception as e:\n",
    "    print(e)    \n",
    "    \n",
    "# Correct way\n",
    "f.simulate(key, (0.5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "0.525923\n",
      "\n",
      "[0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0]\n",
      "\n",
      "[0 0 0 0 2 0 2 2 0 0 2 0 0 0 0 0 0 2 0 0 0 2 2 2 1 0 0 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# 2] We rely on Tensor Flow Probability and it sometimes does unintuitive things.\n",
    "\n",
    "# Bernoulli distribution uses logits instead of probabilities\n",
    "from genjax import bernoulli, gen\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@gen\n",
    "def g(p):\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = (3.,)  # 3 is not a valid probability but a valid logit\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([g.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "    \n",
    "# Values which are stricter than 0 are considered to be the value True.\n",
    "# This means that observing that the value of \"v\" is 4 will be considered possible while intuitively \"v\" should only have support on 0 and 1.\n",
    "chm = C[\"v\"].set(3)\n",
    "print()\n",
    "print(g.assess(chm, (0.5,))[0]) # This should be -inf.\n",
    "print()\n",
    "\n",
    "# Alternatively, we can use the flip function which uses probabilities instead of logits.\n",
    "from genjax import flip\n",
    "@gen\n",
    "def h(p):\n",
    "    v = flip(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = (0.3,)  # 0.3 is a valid probability\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([h.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "print()    \n",
    "    \n",
    "# Categorical distribution also use logits instead of probabilities\n",
    "from genjax import categorical\n",
    "\n",
    "@gen\n",
    "def i(p):\n",
    "    v = categorical(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = ([3., 1., 2.],)  # lists of 3 logits\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([i.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 µs ± 278 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "230 µs ± 1.42 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "10.5 ms ± 128 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# 3] JAX code can be compiled for better performance.\n",
    "from jax import jit\n",
    "\n",
    "# jit is the way to force JAX to compile the code.\n",
    "# It can be used as a decorator\n",
    "@jit\n",
    "def f_v1(p):\n",
    "    return jax.lax.cond(p.sum(), lambda p: p*p, lambda p: p*p, p)\n",
    "\n",
    "# Or as a function\n",
    "f_v2 = jit(lambda p: jax.lax.cond(p.sum(), lambda p: p*p, lambda p: p*p, p))\n",
    "\n",
    "# Baseline\n",
    "def f_v3(p):\n",
    "    jax.lax.cond(p.sum(), lambda p: p*p, lambda p: p*p, p)\n",
    "\n",
    "arg = jax.numpy.eye(500)\n",
    "# Warmup to force jit compilation\n",
    "f_v1(arg)\n",
    "f_v2(arg)\n",
    "# Runtime comparison\n",
    "%timeit f_v1(arg)\n",
    "%timeit f_v2(arg)\n",
    "%timeit f_v3(arg)\n",
    "# Notice that the first and second have the same performance while the third is much slower (~50x on a mac m2 cpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4] Going from Python to JAX\n",
    "# For loops\n",
    "def python_loop(x):\n",
    "    for i in range(100):\n",
    "        x = 2*x\n",
    "    return x\n",
    "\n",
    "def jax_loop(x):\n",
    "    jax.lax.fori_loop(0, 100, lambda i, x: 2*x, x)\n",
    "    \n",
    "# Conditional statements\n",
    "def python_cond(x):\n",
    "    if x.sum() > 0:\n",
    "        return x*x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def jax_cond(x):\n",
    "    jax.lax.cond(x.sum(), lambda x: x*x, lambda x: x, x)\n",
    "    \n",
    "# While loops\n",
    "def python_while(x):\n",
    "    while x.sum() > 0:\n",
    "        x = x*x\n",
    "    return x\n",
    "\n",
    "def jax_while(x):\n",
    "    jax.lax.while_loop(lambda x: x.sum() > 0, lambda x: x*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[]. let b:f32[] = mul a a in (b,) }\n",
      "\n",
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = while[\n",
      "      body_jaxpr={ lambda ; c:f32[]. let d:f32[] = mul c c in (d,) }\n",
      "      body_nconsts=0\n",
      "      cond_jaxpr={ lambda ; e:f32[]. let f:bool[] = gt e 0.0 in (f,) }\n",
      "      cond_nconsts=0\n",
      "    ] a\n",
      "  in (b,) }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5] Is my thing compiling or is it blocked at traced time? \n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# In Jax, the first time you run a function, it is traced, which produces a Jaxpr, a representation of the computation that Jax can optimize.\n",
    "\n",
    "# So in order to debug whether a function is running or not, if it passes the first check that Python let's you write it, you can check if it is running by checking if it is traced, before actually running it on data.\n",
    "\n",
    "# This is done by calling `make_jaxpr` on the function. If it returns a Jaxpr, then the function is traced and ready to be run on data.\n",
    "def im_fine(x):\n",
    "    return x*x\n",
    "\n",
    "print(jax.make_jaxpr(im_fine)(1.0))\n",
    "print()\n",
    "\n",
    "\n",
    "def i_wont_be_so_fine(x):\n",
    "    return jax.lax.while_loop(lambda x: x > 0, lambda x: x*x, x)\n",
    "\n",
    "print(jax.make_jaxpr(i_wont_be_so_fine)(1.0))\n",
    "print()\n",
    "\n",
    "# Try running the function for 8 seconds\n",
    "def run_process():\n",
    "    ctx = multiprocessing.get_context('spawn')\n",
    "    p = ctx.Process(target=i_wont_be_so_fine, args=(1.0,))\n",
    "    p.start()\n",
    "    time.sleep(8)\n",
    "    if p.is_alive():\n",
    "        print(\"I'm still running\")\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "result = subprocess.run(['python', 'genjax/docs/sharp-edges-notebooks/basics/script.py'], capture_output=True, text=True)\n",
    "\n",
    "# Print the output\n",
    "print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 6] Using random keys for generative functions\n",
    "\n",
    "# In GenJAX, we use explicit random keys to generate random numbers. This is done by splitting a key into multiple keys, and using them to generate random numbers.\n",
    "from genjax import bernoulli, gen, beta\n",
    "\n",
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(0.0, u) @ \"p\"\n",
    "    v = bernoulli(p) @ \"v\" # sweet\n",
    "    return v\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key, 20)\n",
    "jitted = jit(beta_bernoulli_process.simulate)\n",
    "print(jnp.array([jitted(key, (0.5,)).get_sample()[\"v\"] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surprise surprise:  float32\n",
      "\n",
      "true_fun and false_fun output must have identical types, got\n",
      "DIFFERENT ShapedArray(int32[3]) vs. ShapedArray(float32[3]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/hglb3g2s4z16d49xygdv5_m40000gp/T/ipykernel_68053/2447684554.py:7: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'>  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  x = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\n",
      "/var/folders/9j/hglb3g2s4z16d49xygdv5_m40000gp/T/ipykernel_68053/2447684554.py:14: UserWarning: Explicitly requested dtype <class 'numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  jnp.array([1, 2, 3], dtype=np.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7] JAX uses 32-bit floats by default\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "x = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\n",
    "print(\"surprise surprise: \", x.dtype)\n",
    "print()\n",
    "\n",
    "# A common TypeError occurs when one tries using np instead of jnp, which is the JAX version of numpy, the former uses 64-bit floats by default, while the JAX version uses 32-bit floats by default.\n",
    "\n",
    "# This on its own gives a UserWarning\n",
    "jnp.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "# This will truncate the array to 32-bit floats and also give a UserWarning\n",
    "innocent_looking_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "@jax.jit\n",
    "def innocent_looking_function(x):\n",
    "    return jax.lax.cond(x.sum(), lambda x: x*x, lambda x: innocent_looking_array, x)\n",
    "\n",
    "input = jnp.array([1.0, 2.0, 3.0])\n",
    "innocent_looking_function(input)\n",
    "\n",
    "try:\n",
    "    # This actually raises a TypeError\n",
    "    innocent_looking_array = np.array([1, 2, 3], dtype=np.float64)\n",
    "    @jax.jit\n",
    "    def innocent_looking_function(x):\n",
    "        return jax.lax.cond(x.sum(), lambda x: x*x, lambda x: innocent_looking_array, x)\n",
    "\n",
    "    input = jnp.array([1, 2, 3])\n",
    "    innocent_looking_function(input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 ms ± 92.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "3.14 s ± 468 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# 8] Beware to OOM on the GPU which happens faster than you might think\n",
    "\n",
    "# Here's a simple HMM model that can be run on the GPU.\n",
    "# By simply changing N from 300 to 1000, the code will typically run out of memory on the GPU as it will take ~300GB of memory.\n",
    "\n",
    "import genjax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "N = 300\n",
    "n_repeats = 100\n",
    "variance = jnp.eye(N)\n",
    "initial_state = jax.random.normal(jax.random.PRNGKey(0), (N,))\n",
    "\n",
    "@genjax.scan_combinator(max_length=100)\n",
    "@genjax.gen\n",
    "def hmm(x, c):\n",
    "    new_x = genjax.mv_normal(x, variance) @ \"new_x\"\n",
    "    return new_x, None\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "jitted = jit(hmm.repeat(num_repeats=n_repeats).simulate)\n",
    "trace = jitted(key, (initial_state, None))\n",
    "%timeit jitted(subkey, (initial_state, None))\n",
    "\n",
    "# If you are running out of memory, you can try de-batching one of the computations, or using a smaller batch size.\n",
    "# For instance, in this example, we can de-batch the repeat combinator, which will reduce the memory usage by a factor of 100, at the cost of some performance.\n",
    "jitted = jit(hmm.simulate)\n",
    "\n",
    "def hmm_debatched(key, initial_state):\n",
    "    keys = jax.random.split(key, n_repeats)\n",
    "    traces = {}\n",
    "    for i in range(n_repeats):\n",
    "        trace = jitted(keys[i], (initial_state, None))\n",
    "        traces[i] = trace\n",
    "    return traces\n",
    "    \n",
    "key = jax.random.PRNGKey(0)\n",
    "# About 4x slower on arm64 CPU and 40x on a Google Colab GPU\n",
    "%timeit hmm_debatched(key, initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
