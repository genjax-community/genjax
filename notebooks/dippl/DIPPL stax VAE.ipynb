{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895f0720-9c96-4a46-8d8a-cdb50eaf8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import inspect\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "from jax import jit, lax, random\n",
    "from jax.example_libraries import stax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "import numpyro\n",
    "from numpyro import optim\n",
    "from numpyro.examples.datasets import MNIST, load_dataset\n",
    "import adevjax\n",
    "import genjax\n",
    "from dataclasses import dataclass\n",
    "from genjax import dippl\n",
    "from genjax import gensp\n",
    "from genjax import select, dirac\n",
    "\n",
    "RESULTS_DIR = os.path.abspath(\n",
    "    os.path.join(os.path.dirname(inspect.getfile(lambda: None)), \".results\")\n",
    ")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def encoder(hidden_dim, z_dim):\n",
    "    return stax.serial(\n",
    "        stax.Dense(hidden_dim, W_init=stax.randn()),\n",
    "        stax.Softplus,\n",
    "        stax.FanOut(2),\n",
    "        stax.parallel(\n",
    "            stax.Dense(z_dim, W_init=stax.randn()),\n",
    "            stax.serial(stax.Dense(z_dim, W_init=stax.randn()), stax.Exp),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(hidden_dim, out_dim):\n",
    "    return stax.serial(\n",
    "        stax.Dense(hidden_dim, W_init=stax.randn()),\n",
    "        stax.Softplus,\n",
    "        stax.Dense(out_dim, W_init=stax.randn()),\n",
    "        stax.Sigmoid,\n",
    "    )\n",
    "\n",
    "\n",
    "# Define our gradient estimator using our loss language.\n",
    "def svi_update(\n",
    "    model,\n",
    "    guide,\n",
    "    optimizer,\n",
    "):\n",
    "    def _inner(key, encoder_params, decoder_params, data):\n",
    "        v_chm = genjax.value_choice_map(\n",
    "            genjax.choice_map({\"image\": data.reshape((28 * 28,))})\n",
    "        )\n",
    "\n",
    "        @dippl.loss\n",
    "        def vae_loss(key, encoder_params, decoder_params):\n",
    "            key, sub_key = jax.random.split(key)\n",
    "            v = dippl.upper(guide)(sub_key, encoder_params, v_chm)\n",
    "            merged = gensp.merge(v, v_chm)\n",
    "            dippl.lower(model)(key, merged, decoder_params)\n",
    "\n",
    "        loss, (\n",
    "            encoder_params_grad,\n",
    "            decoder_params_grad,\n",
    "        ) = vae_loss.value_and_grad_estimate(key, (encoder_params, decoder_params))\n",
    "        return (encoder_params_grad, decoder_params_grad), loss\n",
    "\n",
    "    def batch_update(svi_state, batch):\n",
    "        (key, optimizer_state) = svi_state\n",
    "        encoder_params, decoder_params = optimizer.get_params(optimizer_state)\n",
    "        key, sub_key = jax.random.split(key)\n",
    "        sub_keys = jax.random.split(sub_key, len(batch))\n",
    "        (encoder_grads, decoder_grads), loss = jax.vmap(\n",
    "            _inner, in_axes=(0, None, None, 0)\n",
    "        )(sub_keys, encoder_params, decoder_params, batch)\n",
    "        encoder_grads, decoder_grads = jtu.tree_map(\n",
    "            jnp.mean, (encoder_grads, decoder_grads)\n",
    "        )\n",
    "        optimizer_state = optimizer.update(\n",
    "            (encoder_grads, decoder_grads), optimizer_state\n",
    "        )\n",
    "        return (key, optimizer_state), loss\n",
    "\n",
    "    return batch_update\n",
    "\n",
    "\n",
    "@jit\n",
    "def binarize(rng_key, batch):\n",
    "    return random.bernoulli(rng_key, batch).astype(batch.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4075c2-7832-4c9e-b0a8-bff113c8199f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "<function invoke_closed_over at 0x7fdc488597e0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCustomJVPException\u001b[0m                        Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/custom_derivatives.py:363\u001b[0m, in \u001b[0;36mCustomJVPCallPrimitive.bind\u001b[0;34m(self, fun, jvp, symbolic_zeros, *args)\u001b[0m\n\u001b[1;32m    362\u001b[0m tracers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(top_trace\u001b[38;5;241m.\u001b[39mfull_raise, args)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mtop_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_custom_jvp_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    364\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43msymbolic_zeros\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_zeros\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    365\u001b[0m _, env_trace_todo \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mmerge_linear_aux(env_trace_todo1, env_trace_todo2)\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/interpreters/ad.py:382\u001b[0m, in \u001b[0;36mJVPTrace.process_custom_jvp_call\u001b[0;34m(self, _, __, f_jvp, tracers, symbolic_zeros)\u001b[0m\n\u001b[1;32m    381\u001b[0m   tangents_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(replace_internal_symbolic_zeros, tangents_in)\n\u001b[0;32m--> 382\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mf_jvp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m primals_out, tangents_out \u001b[38;5;241m=\u001b[39m split_list(outs, [\u001b[38;5;28mlen\u001b[39m(outs) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/linear_util.py:205\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;66;03m# As above does for the first half of the transformation, exceptions\u001b[39;00m\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# raised in the second half of the transformation also require us to\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# clean up references here.\u001b[39;00m\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/custom_derivatives.py:416\u001b[0m, in \u001b[0;36mprocess_env_traces\u001b[0;34m(primitive, level, jvp_was_run, *args)\u001b[0m\n\u001b[1;32m    415\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, outs)\n\u001b[0;32m--> 416\u001b[0m outs, cur_todo \u001b[38;5;241m=\u001b[39m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp_was_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m todo\u001b[38;5;241m.\u001b[39mappend(cur_todo)\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/custom_derivatives.py:373\u001b[0m, in \u001b[0;36mCustomJVPCallPrimitive.post_process\u001b[0;34m(self, trace, out_tracers, jvp_was_run)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_process\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, out_tracers, jvp_was_run: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 373\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_process_custom_jvp_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_tracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp_was_run\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/interpreters/ad.py:389\u001b[0m, in \u001b[0;36mJVPTrace.post_process_custom_jvp_call\u001b[0;34m(self, out_tracers, _)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_process_custom_jvp_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, out_tracers, _):\n\u001b[0;32m--> 389\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m CustomJVPException()\n",
      "\u001b[0;31mCustomJVPException\u001b[0m: Detected differentiation of a custom_jvp function with respect to a closed-over value. That isn't supported because the custom JVP rule only specifies how to differentiate the custom_jvp function with respect to explicit input parameters. Try passing the closed-over value into the custom_jvp function as an argument, and adapting the custom_jvp rule.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m optimizer_state \u001b[38;5;241m=\u001b[39m adam\u001b[38;5;241m.\u001b[39minit((encoder_params, decoder_params))\n\u001b[1;32m     54\u001b[0m svi_state \u001b[38;5;241m=\u001b[39m (key, optimizer_state)\n\u001b[0;32m---> 55\u001b[0m svi_state \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvi_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mepoch_train\u001b[0;34m(svi_state, rng_key, train_idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m     svi_state, loss \u001b[38;5;241m=\u001b[39m svi_updater(svi_state, batch)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m svi_state\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfori_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvi_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m, in \u001b[0;36mepoch_train.<locals>.body_fn\u001b[0;34m(i, val)\u001b[0m\n\u001b[1;32m     44\u001b[0m rng_key_binarize \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mfold_in(rng_key, i)\n\u001b[1;32m     45\u001b[0m batch \u001b[38;5;241m=\u001b[39m binarize(rng_key_binarize, train_fetch(i, train_idx)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m svi_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvi_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m svi_state\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36msvi_update.<locals>.batch_update\u001b[0;34m(svi_state, batch)\u001b[0m\n\u001b[1;32m     79\u001b[0m key, sub_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m     80\u001b[0m sub_keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(sub_key, \u001b[38;5;28mlen\u001b[39m(batch))\n\u001b[0;32m---> 81\u001b[0m (encoder_grads, decoder_grads), loss \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m encoder_grads, decoder_grads \u001b[38;5;241m=\u001b[39m jtu\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m     85\u001b[0m     jnp\u001b[38;5;241m.\u001b[39mmean, (encoder_grads, decoder_grads)\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m optimizer_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     88\u001b[0m     (encoder_grads, decoder_grads), optimizer_state\n\u001b[1;32m     89\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m, in \u001b[0;36msvi_update.<locals>._inner\u001b[0;34m(key, encoder_params, decoder_params, data)\u001b[0m\n\u001b[1;32m     67\u001b[0m     merged \u001b[38;5;241m=\u001b[39m gensp\u001b[38;5;241m.\u001b[39mmerge(v, v_chm)\n\u001b[1;32m     68\u001b[0m     dippl\u001b[38;5;241m.\u001b[39mlower(model)(key, merged, decoder_params)\n\u001b[1;32m     70\u001b[0m loss, (\n\u001b[1;32m     71\u001b[0m     encoder_params_grad,\n\u001b[1;32m     72\u001b[0m     decoder_params_grad,\n\u001b[0;32m---> 73\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mvae_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad_estimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (encoder_params_grad, decoder_params_grad), loss\n",
      "File \u001b[0;32m~/miniconda3/envs/pyro/lib/python3.10/site-packages/adevjax/core.py:462\u001b[0m, in \u001b[0;36mExpectation.value_and_grad_estimate\u001b[0;34m(self, key, primals)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner\u001b[39m(primals):\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_closed_over(\u001b[38;5;28mself\u001b[39m, key, primals)\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_inner\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyro/lib/python3.10/site-packages/adevjax/core.py:460\u001b[0m, in \u001b[0;36mExpectation.value_and_grad_estimate.<locals>._inner\u001b[0;34m(primals)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner\u001b[39m(primals):\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvoke_closed_over\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Research/adevjax/jax/jax/_src/custom_derivatives.py:261\u001b[0m, in \u001b[0;36mcustom_jvp.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m custom_jvp_call_p\u001b[38;5;241m.\u001b[39mbind(flat_fun, flat_jvp, \u001b[38;5;241m*\u001b[39margs_flat,\n\u001b[1;32m    259\u001b[0m                                   symbolic_zeros\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_zeros)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ad\u001b[38;5;241m.\u001b[39mCustomJVPException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 261\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun)\n\u001b[1;32m    263\u001b[0m _, (out_tree, _) \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mmerge_linear_aux(out_type1, out_type2)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, out_flat)\n",
      "\u001b[0;31mException\u001b[0m: <function invoke_closed_over at 0x7fdc488597e0>"
     ]
    }
   ],
   "source": [
    "hidden_dim = 10\n",
    "z_dim = 100\n",
    "learning_rate = 1.0e-3\n",
    "batch_size = 64\n",
    "\n",
    "encoder_nn_init, encoder_nn_apply = encoder(hidden_dim, z_dim)\n",
    "decoder_nn_init, decoder_nn_apply = decoder(hidden_dim, 28 * 28)\n",
    "\n",
    "# Model + guide close over the neural net apply functions.\n",
    "@genjax.gen\n",
    "def decoder_model(decoder_params):\n",
    "    latent = dippl.mv_normal_diag_reparam(jnp.zeros(z_dim), jnp.ones(z_dim)) @ \"latent\"\n",
    "    image = decoder_nn_apply(decoder_params, latent)\n",
    "    noisy_image = dippl.mv_normal_diag_reparam(image, jnp.ones(784)) @ \"image\"\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def encoder_model(encoder_params, chm):\n",
    "    image = chm.get_leaf_value()[\"image\"]\n",
    "    μ, Σ_scale = encoder_nn_apply(encoder_params, image)\n",
    "    x = dippl.mv_normal_diag_reparam(μ, Σ_scale) @ \"latent\"\n",
    "\n",
    "\n",
    "model = gensp.choice_map_distribution(decoder_model, select(\"latent\", \"image\"), None)\n",
    "guide = gensp.choice_map_distribution(encoder_model, select(\"latent\"), None)\n",
    "\n",
    "adam = optim.Adam(learning_rate)\n",
    "svi_updater = svi_update(model, guide, adam)\n",
    "rng_key = PRNGKey(0)\n",
    "train_init, train_fetch = load_dataset(MNIST, batch_size=batch_size, split=\"train\")\n",
    "num_train, train_idx = train_init()\n",
    "rng_key, rng_key_binarize, rng_key_init = random.split(rng_key, 3)\n",
    "encoder_init_key, decoder_init_key = random.split(rng_key_init)\n",
    "_, encoder_params = encoder_nn_init(encoder_init_key, (784,))\n",
    "_, decoder_params = decoder_nn_init(decoder_init_key, (z_dim,))\n",
    "sample_batch = binarize(rng_key_binarize, train_fetch(0, train_idx)[0])\n",
    "num_train, train_idx = train_init()\n",
    "\n",
    "\n",
    "@jit\n",
    "def epoch_train(svi_state, rng_key, train_idx):\n",
    "    def body_fn(i, val):\n",
    "        svi_state = val\n",
    "        rng_key_binarize = random.fold_in(rng_key, i)\n",
    "        batch = binarize(rng_key_binarize, train_fetch(i, train_idx)[0])\n",
    "        svi_state, loss = svi_updater(svi_state, batch)\n",
    "        return svi_state\n",
    "\n",
    "    return lax.fori_loop(0, num_train, body_fn, svi_state)\n",
    "\n",
    "\n",
    "key = random.PRNGKey(314159)\n",
    "optimizer_state = adam.init((encoder_params, decoder_params))\n",
    "svi_state = (key, optimizer_state)\n",
    "svi_state = epoch_train(svi_state, key, train_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
