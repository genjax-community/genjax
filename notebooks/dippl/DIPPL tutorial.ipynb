{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54553df1-01ec-4f04-a031-14eb76fa8f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.21024\n",
      "16.906754\n",
      "12.197225\n",
      "10.847298\n",
      "10.0\n",
      "9.152702\n",
      "7.8027754\n",
      "0.7899256\n",
      "0.09672737\n"
     ]
    }
   ],
   "source": [
    "import genjax\n",
    "from genjax import dippl\n",
    "from genjax import gensp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import adevjax\n",
    "\n",
    "console = genjax.pretty(show_locals=True)\n",
    "key = jax.random.PRNGKey(314159)\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def abnormal_flipper():\n",
    "    flip = dippl.flip_enum(0.5) @ \"flip\"\n",
    "    v = jax.lax.cond(flip, lambda: 10.0, lambda: 0.0)\n",
    "\n",
    "    # score primitive -- accumulates onto log prob\n",
    "    # only active inside genjax.gensp terms\n",
    "    gensp.accum_score(v)\n",
    "\n",
    "\n",
    "# Lift to GenSP.\n",
    "lifted_model = gensp.choice_map_distribution(\n",
    "    abnormal_flipper, genjax.select(\"flip\"), None\n",
    ")\n",
    "\n",
    "# Our variational family, parametrized by p.\n",
    "@genjax.gen\n",
    "def variational_family(p):\n",
    "    flip = dippl.flip_enum(p) @ \"flip\"\n",
    "\n",
    "\n",
    "# Lift to GenSP.\n",
    "lifted_family = gensp.choice_map_distribution(\n",
    "    variational_family, genjax.select(\"flip\"), None\n",
    ")\n",
    "\n",
    "# Now, we define a grad estimator using the loss language\n",
    "# (this is lightweight syntax over GenSP's ADEV compatible interfaces\n",
    "# and ADEV primitives, including `add_cost`)\n",
    "def elbo_grad(key, p):\n",
    "\n",
    "    # Loss is defined imperatively using two interfaces:\n",
    "    # * `upper` invokes `random_weighted` and returns the sample `v`,\n",
    "    #      and accumulates (-w) via ADEV's `add_cost` primitive.\n",
    "    #\n",
    "    # * `lower` invokes `estimate_logpdf` and accumulates (w)\n",
    "    #      via ADEV's `add_cost` primitive.\n",
    "    @dippl.loss\n",
    "    def flip_variational_loss(p):\n",
    "        v = dippl.upper(lifted_family)(p)\n",
    "        dippl.lower(lifted_model)(v)\n",
    "\n",
    "    # We can automatically derive `jvp` and `grad` estimators using ADEV.\n",
    "    (p_grad,) = flip_variational_loss.grad_estimate(key, (p,))\n",
    "    return p_grad\n",
    "\n",
    "\n",
    "# Trials: look at grads for increasing `p` (overwhelmingly beneficial for the objective).\n",
    "# Expected behavior: the gradient should decrease with increasing `p`\n",
    "# (until numerical instability)\n",
    "key, sub_key = jax.random.split(key)\n",
    "jitted = jax.jit(elbo_grad)\n",
    "for p in [0.0001, 0.001, 0.1, 0.3, 0.5, 0.7, 0.9, 0.9999, 0.99995]:\n",
    "    key, sub_key = jax.random.split(key)\n",
    "    p_grad = jitted(sub_key, p)\n",
    "    print(p_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
