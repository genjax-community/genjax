{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83743a0d-5d4a-484d-b227-ac9aabbf4027",
   "metadata": {},
   "source": [
    "---\n",
    "title: Implementing the builtin modeling language\n",
    "date: \"December 7, 2022\"\n",
    "abstract: This notebook covers the design decisions, implementation ingredients, and interface implementations for the builtin modeling language in GenJAX. It assumes familiarity with the generative function interface, as well as familiarity with JAX's support for composable interpreters and staging.\n",
    "callout-appearance: simple\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ab67a8-1e1a-48c5-aec0-3cfca1c88855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Pretty printing.\n",
    "console = genjax.console(width=70)\n",
    "\n",
    "# Reproducibility.\n",
    "key = jax.random.PRNGKey(314159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd9982-253d-4aee-aa34-57a52cc1d126",
   "metadata": {},
   "source": [
    "One key property of the generative function interface is that it enables a separation between model and inference code - providing an abstraction layer that facilitates the development of modular model pieces, and then inference pieces that abstract over the implementation of the interface.\n",
    "\n",
    "Now, implementing the interface on objects, and composing them in various ways (by e.g. specializing the implementation of the interface functions to support any intended composition) is a valid way to construct new generative functions. In fact, this is the pattern which generative function combinators follow - they accept generative functions as input, and produce new generative functions whose implementations are specialized to represent some specific pattern of computation.\n",
    "\n",
    "Explicitly constructing generative functions using languages of objects, however, can often feel unwieldy. Part of the way that GenJAX (and [Gen.jl](https://github.com/probcomp/Gen.jl)) alleviates this restriction is by exposing languages _which construct generative functions from programs_. This drastically increases the expressivity available to the programmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29eb432-660c-4ea7-a18d-175ed4b3b085",
   "metadata": {},
   "source": [
    "In GenJAX, here's an example of the `BuiltinGenerativeFunction` language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82c6c32-1537-48ec-9f56-48feae483f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "@genjax.Static\n",
    "def model(x):\n",
    "    y = genjax.trace(\"y\", genjax.normal)(x, 1.0)\n",
    "    z = genjax.trace(\"z\", genjax.normal)(y + x, 1.0)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46533eab-11dd-420e-aeeb-fcf00016d26d",
   "metadata": {},
   "source": [
    "When we apply one of the interface functions to this object, we get the associated data types that we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2566152-ed4b-4d83-a5f6-6d5a9a136def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StaticTrace(gen_fn=StaticGenerativeFunction(source=<function model at 0x169dbdbd0>), args=(1.0,), retval=Array(-0.0015921, dtype=float32), address_choices=Trie(inner={'y': DistributionTrace(gen_fn=TFPDistribution(make_distribution=<class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>), args=(1.0, 1.0), value=Array(-0.67220366, dtype=float32), score=Array(-2.317071, dtype=float32)), 'z': DistributionTrace(gen_fn=TFPDistribution(make_distribution=<class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>), args=(Array(0.32779634, dtype=float32), 1.0), value=Array(-0.0015921, dtype=float32), score=Array(-0.9731869, dtype=float32))}), cache=Trie(inner={}), score=Array(-3.290258, dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "tr = model.simulate(sub_key, (1.0,))\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32824e2b-6d75-44d9-b464-b6a8e7b750a6",
   "metadata": {},
   "source": [
    "How exactly do we do this? In this notebook, you're going to find out. You'll also get a chance to explore some of the capabilities which JAX exposes to library designers. Ideally, you'll also get a sense of some of the limitations of JAX (and GenJAX) - which are restricted to support programs which are amenable to GPU/TPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6a761-4ab5-4d6a-b717-8ac17ba4950c",
   "metadata": {},
   "source": [
    "## The magic of JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dda2b5-b59f-40b9-9b10-149c3096befa",
   "metadata": {},
   "source": [
    "Let's examine the generative function object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738bd478-2f90-4a37-9ded-d95f159a11f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">StaticGenerativeFunction</span>\n",
       "└── source\n",
       "    └── &lt;function model&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mStaticGenerativeFunction\u001b[0m\n",
       "└── source\n",
       "    └── <function model>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c59b06-39a5-48d0-891d-70404109ce69",
   "metadata": {},
   "source": [
    "All the decorator `genjax.gen` does is wrap the function into this object. It holds a reference to the function we defined above.\n",
    "\n",
    "But clearly, we need to somehow get inside that function - because we're recording data onto the `BuiltinTrace` which come from intermediate results of the execution of the function.\n",
    "\n",
    "That's where JAX comes in - JAX provides a way to trace pure, numerical Python programs - enabling us to construct program transformations which return new functions that compute different semantics from the original function.^[Program tracing is an approach which has its roots in automatic differentiation. If you're interesting in this technique, I cannot recommend [Autodidax: JAX core from scratch](https://jax.readthedocs.io/en/latest/autodidax.html) enough. It will introduce you to enough interesting PL ideas to keep you occupied for months, if not years.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1069eb-63ca-4fd2-83f4-9bedab525f8a",
   "metadata": {},
   "source": [
    "Let's utilize one of JAX's interpreters to construct an intermediate representation of the function which our generative function object holds reference to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0189640f-e337-4ed7-8c50-3516f06a094f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let _normal = { lambda ; a:key<fry>[]. let\n",
       "    b:f32[1] = pjit[\n",
       "      name=_normal_real\n",
       "      jaxpr={ lambda ; c:key<fry>[]. let\n",
       "          d:f32[1] = pjit[\n",
       "            name=_uniform\n",
       "            jaxpr={ lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "                h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "                i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "                j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "                k:u32[1] = shift_right_logical j 9\n",
       "                l:u32[1] = or k 1065353216\n",
       "                m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "                n:f32[1] = sub m 1.0\n",
       "                o:f32[1] = sub i h\n",
       "                p:f32[1] = mul n o\n",
       "                q:f32[1] = add p h\n",
       "                r:f32[1] = max h q\n",
       "              in (r,) }\n",
       "          ] c -0.9999999403953552 1.0\n",
       "          s:f32[1] = erf_inv d\n",
       "          t:f32[1] = mul 1.4142135381698608 s\n",
       "        in (t,) }\n",
       "    ] a\n",
       "  in (b,) } in\n",
       "let _normal_real = { lambda ; c:key<fry>[]. let\n",
       "    d:f32[1] = pjit[\n",
       "      name=_uniform\n",
       "      jaxpr={ lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "          h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "          i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "          j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "          k:u32[1] = shift_right_logical j 9\n",
       "          l:u32[1] = or k 1065353216\n",
       "          m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "          n:f32[1] = sub m 1.0\n",
       "          o:f32[1] = sub i h\n",
       "          p:f32[1] = mul n o\n",
       "          q:f32[1] = add p h\n",
       "          r:f32[1] = max h q\n",
       "        in (r,) }\n",
       "    ] c -0.9999999403953552 1.0\n",
       "    s:f32[1] = erf_inv d\n",
       "    t:f32[1] = mul 1.4142135381698608 s\n",
       "  in (t,) } in\n",
       "let _uniform = { lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "    h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "    i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "    j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "    k:u32[1] = shift_right_logical j 9\n",
       "    l:u32[1] = or k 1065353216\n",
       "    m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "    n:f32[1] = sub m 1.0\n",
       "    o:f32[1] = sub i h\n",
       "    p:f32[1] = mul n o\n",
       "    q:f32[1] = add p h\n",
       "    r:f32[1] = max h q\n",
       "  in (r,) } in\n",
       "{ lambda ; u:f32[]. let\n",
       "    v:f32[] = trace[\n",
       "      _jaxpr={ lambda ; w:f32[] x:f32[]. let\n",
       "          y:key<fry>[] = random_seed[impl=fry] 0\n",
       "          z:u32[2] = random_unwrap y\n",
       "          ba:key<fry>[] = random_wrap[impl=fry] z\n",
       "          bb:f32[1] = pjit[name=_normal jaxpr=_normal] ba\n",
       "          bc:f32[1] = mul bb 1.0\n",
       "          bd:f32[1] = add bc 0.0\n",
       "          be:f32[] = convert_element_type[new_dtype=float32 weak_type=False] x\n",
       "          bf:f32[1] = mul bd be\n",
       "          bg:f32[1] = add bf w\n",
       "          bh:f32[] = reshape[dimensions=None new_sizes=()] bg\n",
       "        in (bh,) }\n",
       "      in_tree=PyTreeDef((CustomNode(TFPDistribution[(<class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>,)], []), CustomNode(PytreeConst[('y',)], []), *, *))\n",
       "      num_consts=0\n",
       "      out_tree=<function transformation_with_aux.<locals>.<lambda> at 0x16acf15a0>\n",
       "    ] u 1.0\n",
       "    bi:f32[] = convert_element_type[new_dtype=float32 weak_type=False] u\n",
       "    bj:f32[] = add v bi\n",
       "    bk:f32[] = trace[\n",
       "      _jaxpr={ lambda ; bl:f32[] bm:f32[]. let\n",
       "          bn:key<fry>[] = random_seed[impl=fry] 0\n",
       "          bo:u32[2] = random_unwrap bn\n",
       "          bp:key<fry>[] = random_wrap[impl=fry] bo\n",
       "          bq:f32[1] = pjit[name=_normal jaxpr=_normal] bp\n",
       "          br:f32[1] = mul bq 1.0\n",
       "          bs:f32[1] = add br 0.0\n",
       "          bt:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bm\n",
       "          bu:f32[1] = mul bs bt\n",
       "          bv:f32[1] = add bu bl\n",
       "          bw:f32[] = reshape[dimensions=None new_sizes=()] bv\n",
       "        in (bw,) }\n",
       "      in_tree=PyTreeDef((CustomNode(TFPDistribution[(<class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>,)], []), CustomNode(PytreeConst[('z',)], []), *, *))\n",
       "      num_consts=0\n",
       "      out_tree=<function transformation_with_aux.<locals>.<lambda> at 0x16acf1990>\n",
       "    ] bj 1.0\n",
       "  in (bk,) }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaxpr = jax.make_jaxpr(model.source)(1.0)\n",
    "jaxpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61c89c-f0e2-4596-836a-443175cabebe",
   "metadata": {},
   "source": [
    "So `jax.make_jaxpr` takes a function `f :: A -> B` and returns a function `f :: A -> Jaxpr`, where `Jaxpr` is the program representation above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f07ad4-3535-4505-a1b3-d2017b04ba41",
   "metadata": {},
   "source": [
    "When we run this function using Python's interpreter, JAX lifts the input to something called a `Tracer`, JAX keeps an internal stack of interpreters which redirect infix operations on `Tracer` instances and modify their behavior. Additionally, JAX exposes new primitives (like all the `NumPy` primitives) which wrap a function called `bind`. `bind` takes in `Tracer` arguments, looks through them (and the interpreter stack), selects the interpreter which should handle the call - and then the interpreter is allowed to `process_primitive` - invoking the semantics which the interpreter defines for that primitive.\n",
    "\n",
    "`jax.make_jaxpr` uses the above process to walk the program, and construct the above intermediate representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23495292-d55b-4966-a391-69c7f6b322b5",
   "metadata": {},
   "source": [
    "Now, the point of having this representation is that we can transform it further! We can lower it to other representations (including things like XLA - the linear algebra accelerator that JAX utilizes to go high performance). We could also write _another interpreter_ which walks this representation, invokes other primitives with `bind`, etc - deferring further transformation to the next interpreter in line.\n",
    "\n",
    "This (admittedly rough description) above is the secret behind JAX's compositional transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf16bbe-2e93-4918-a726-96c9dfcee347",
   "metadata": {},
   "source": [
    "## New semantics via program transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841cb8f-eab5-46b0-89a9-f23a598f13fe",
   "metadata": {},
   "source": [
    "Let's examine the representation once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a8e578-c11c-4125-8871-c6a0f24f777d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let _normal = { lambda ; a:key<fry>[]. let\n",
       "    b:f32[1] = pjit[\n",
       "      name=_normal_real\n",
       "      jaxpr={ lambda ; c:key<fry>[]. let\n",
       "          d:f32[1] = pjit[\n",
       "            name=_uniform\n",
       "            jaxpr={ lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "                h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "                i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "                j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "                k:u32[1] = shift_right_logical j 9\n",
       "                l:u32[1] = or k 1065353216\n",
       "                m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "                n:f32[1] = sub m 1.0\n",
       "                o:f32[1] = sub i h\n",
       "                p:f32[1] = mul n o\n",
       "                q:f32[1] = add p h\n",
       "                r:f32[1] = max h q\n",
       "              in (r,) }\n",
       "          ] c -0.9999999403953552 1.0\n",
       "          s:f32[1] = erf_inv d\n",
       "          t:f32[1] = mul 1.4142135381698608 s\n",
       "        in (t,) }\n",
       "    ] a\n",
       "  in (b,) } in\n",
       "let _normal_real = { lambda ; c:key<fry>[]. let\n",
       "    d:f32[1] = pjit[\n",
       "      name=_uniform\n",
       "      jaxpr={ lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "          h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "          i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "          j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "          k:u32[1] = shift_right_logical j 9\n",
       "          l:u32[1] = or k 1065353216\n",
       "          m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "          n:f32[1] = sub m 1.0\n",
       "          o:f32[1] = sub i h\n",
       "          p:f32[1] = mul n o\n",
       "          q:f32[1] = add p h\n",
       "          r:f32[1] = max h q\n",
       "        in (r,) }\n",
       "    ] c -0.9999999403953552 1.0\n",
       "    s:f32[1] = erf_inv d\n",
       "    t:f32[1] = mul 1.4142135381698608 s\n",
       "  in (t,) } in\n",
       "let _uniform = { lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "    h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "    i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "    j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "    k:u32[1] = shift_right_logical j 9\n",
       "    l:u32[1] = or k 1065353216\n",
       "    m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "    n:f32[1] = sub m 1.0\n",
       "    o:f32[1] = sub i h\n",
       "    p:f32[1] = mul n o\n",
       "    q:f32[1] = add p h\n",
       "    r:f32[1] = max h q\n",
       "  in (r,) } in\n",
       "{ lambda ; u:f32[]. let\n",
       "    v:f32[] = trace[\n",
       "      _jaxpr={ lambda ; w:f32[] x:f32[]. let\n",
       "          y:key<fry>[] = random_seed[impl=fry] 0\n",
       "          z:u32[2] = random_unwrap y\n",
       "          ba:key<fry>[] = random_wrap[impl=fry] z\n",
       "          bb:f32[1] = pjit[name=_normal jaxpr=_normal] ba\n",
       "          bc:f32[1] = mul bb 1.0\n",
       "          bd:f32[1] = add bc 0.0\n",
       "          be:f32[] = convert_element_type[new_dtype=float32 weak_type=False] x\n",
       "          bf:f32[1] = mul bd be\n",
       "          bg:f32[1] = add bf w\n",
       "          bh:f32[] = reshape[dimensions=None new_sizes=()] bg\n",
       "        in (bh,) }\n",
       "      in_tree=PyTreeDef((CustomNode(TFPDistribution[(<class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>,)], []), CustomNode(PytreeConst[('y',)], []), *, *))\n",
       "      num_consts=0\n",
       "      out_tree=<function transformation_with_aux.<locals>.<lambda> at 0x16acf15a0>\n",
       "    ] u 1.0\n",
       "    bi:f32[] = convert_element_type[new_dtype=float32 weak_type=False] u\n",
       "    bj:f32[] = add v bi\n",
       "    bk:f32[] = trace[\n",
       "      _jaxpr={ lambda ; bl:f32[] bm:f32[]. let\n",
       "          bn:key<fry>[] = random_seed[impl=fry] 0\n",
       "          bo:u32[2] = random_unwrap bn\n",
       "          bp:key<fry>[] = random_wrap[impl=fry] bo\n",
       "          bq:f32[1] = pjit[name=_normal jaxpr=_normal] bp\n",
       "          br:f32[1] = mul bq 1.0\n",
       "          bs:f32[1] = add br 0.0\n",
       "          bt:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bm\n",
       "          bu:f32[1] = mul bs bt\n",
       "          bv:f32[1] = add bu bl\n",
       "          bw:f32[] = reshape[dimensions=None new_sizes=()] bv\n",
       "        in (bw,) }\n",
       "      in_tree=PyTreeDef((CustomNode(TFPDistribution[(<class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>,)], []), CustomNode(PytreeConst[('z',)], []), *, *))\n",
       "      num_consts=0\n",
       "      out_tree=<function transformation_with_aux.<locals>.<lambda> at 0x16acf1990>\n",
       "    ] bj 1.0\n",
       "  in (bk,) }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaxpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151f7ca-0ee2-4be9-8b41-bce1aded6355",
   "metadata": {},
   "source": [
    "You'll notice that there is an intrinsic called `trace` here - which looks suspiciously similar to `genjax.trace` above.\n",
    "\n",
    "`trace` is a custom primitive that GenJAX defines - by defining a new primitive, we can place a stub in the intermediate representation, which we can further transform to implement the semantics we wish to express."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4ce0a-f42c-4fd0-8829-e6dcc3ad4386",
   "metadata": {},
   "source": [
    "### A high level view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5baf73-9033-40d3-bd75-652255a0d1ed",
   "metadata": {},
   "source": [
    "Now, we need to transform it! Here's where some serious design decisions enter into the picture.\n",
    "\n",
    "One thing you might notice about the `Jaxpr` above is that the the arity of the function is fixed, and so is the arity of the return value. But when we call `simulate` on our `model` - we get out something which is not a `h :: f32[]` (it's actually a [`jax.Pytree`](https://jax.readthedocs.io/en/latest/pytrees.html) with a lot more data - so we'd expect a lot more return values in the `Jaxpr`^[JAX flattens/unflattens `Pytree` instances on each side of the IR boundary - the IR is strongly typed, but only natively supports a few base types, and a few composite array types.]. \n",
    "\n",
    "What gives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641280e-5faf-4021-8140-1296679f3acf",
   "metadata": {},
   "source": [
    "Here's where JAX's support for compositional application of interpreters comes into play. \n",
    "\n",
    "Instead of attempting to modify the IR above to change the arity of everything (a process which the authors expect would be quite painful, and buggy) - we can write another interpreter which walks the IR and evaluates it, but that interpreter can keep track of the state that we want to put into the `BuiltinTrace` at the end of the interface invocation.\n",
    "\n",
    "Then, we can _stage out that interpreter_ to support JIT compilation, etc. I'll describe the process below in pseudo-types:\n",
    "\n",
    "We start with `f :: A -> B`, and we stage it to get a new function `f' :: Type[A] -> Jaxpr`, then we write an interpreter `I` with signature `I :: (Jaxpr, A) -> (B, State)`. The application of `I` itself can also be staged.\n",
    "\n",
    "So this is really nice - we don't have to munge the IR manually, we just get to write an interpreter to do the transformation for us. That's the power that JAX provides for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d671743-8460-41c5-83fd-5793ad60768e",
   "metadata": {},
   "source": [
    "### Interpreter design decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ab8c2-f1c6-4139-a7e2-ce9ae0cf04c4",
   "metadata": {},
   "source": [
    "With the high-level view in mind, we'll examine two of the interface implementations. The first is `simulate` - likely the easiest implementation to understand^[For this notebook, we're going to ignore the inference math that we wish to support!]. The second is `update`.\n",
    "\n",
    "Now, in GenJAX, the interpreter is written to be re-usable for each of the interface functions. Because we've chosen to re-use the interpreter (and parametrize the transformation semantics by configuring the interpreter in other ways -- besides the implementation), you're going to see some complexity right out the gate.\n",
    "\n",
    "The reason why this complexity is there is because we wish to expose _incremental computing optimizations_ in `update`. To support this customization, the interpreter can best be described as a _propagation interpreter_ - similar to Julia's abstract interpretation machinery (if you're familiar). A propagation interpreter treats the `Jaxpr` as an undirected graph - and performs interpretation by iterating until a fixpoint condition is satisfied. \n",
    "\n",
    "The high level pattern from the previous section is still true! But if you've written interpreters for something like [Structure and Interpretation of Computer Programs](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs) before - this interpreter might be a slight shock to the system.\n",
    "\n",
    "Here's a boiled down form of the `simulate_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe955d1e-2814-4a22-ac24-04f15e0b5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_transform(f, **kwargs):\n",
    "    def _inner(key, args):\n",
    "        # Step 1: stage out the function to a `Jaxpr`.\n",
    "        closed_jaxpr, (flat_args, in_tree, out_tree) = stage(f)(key, *args, **kwargs)\n",
    "        jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n",
    "\n",
    "        # Step 2: create a `Simulate` instance, which we parametrize\n",
    "        # the propagation interpreter with.\n",
    "        #\n",
    "        # `Bare` is an instance of something called a `Cell` - the\n",
    "        # objects which the propagation interpreter reasons about.\n",
    "        handler = Simulate()\n",
    "        final_env, ret_state = propagate(\n",
    "            Bare,\n",
    "            bare_propagation_rules,\n",
    "            jaxpr,\n",
    "            [Bare.new(v) for v in consts],\n",
    "            list(map(Bare.new, flat_args)),\n",
    "            [Bare.unknown(var.aval) for var in jaxpr.outvars],\n",
    "            handler=handler,\n",
    "        )\n",
    "\n",
    "        # Step 3: when the interpreter finishes, we read the values\n",
    "        # out of its environment.\n",
    "        flat_out = safe_map(final_env.read, jaxpr.outvars)\n",
    "        flat_out = map(lambda v: v.get_val(), flat_out)\n",
    "        key_and_returns = jtu.tree_unflatten(out_tree, flat_out)\n",
    "        key, *retvals = key_and_returns\n",
    "        retvals = tuple(retvals)\n",
    "\n",
    "        # Here's the handler state - remember the signature from\n",
    "        # above `I :: (Jaxpr, A) -> (B, State)`, these fields\n",
    "        # below are the `State`.\n",
    "        score = handler.score\n",
    "        chm = handler.choice_state\n",
    "        cache = handler.cache_state\n",
    "\n",
    "        # This returns all the things which we want to put\n",
    "        # into `BuiltinTrace`.\n",
    "        return key, (f, args, retvals, chm, score), cache\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dde278-917e-45f2-b4a4-2f109c5537a4",
   "metadata": {},
   "source": [
    "And, just to show you that this is the key behind how we implement `simulate`, I've copied the `BuiltinGenerativeFunction` class method for `simulate` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415adb58-ef4f-4d68-92b3-90c52be6508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(self, key, args, **kwargs):\n",
    "    assert isinstance(args, Tuple)\n",
    "    key, (f, args, r, chm, score), cache = simulate_transform(self.source, **kwargs)(\n",
    "        key, args\n",
    "    )\n",
    "    return key, BuiltinTrace(self, args, r, chm, cache, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7ce6e-af9e-400b-bee7-c84efabd1c92",
   "metadata": {},
   "source": [
    "We'll discuss `propagate` in a moment - but a few high-level things.\n",
    "\n",
    "Note that the `simulate` method can be staged out / used with JAX's interfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e0b602-30ed-4226-9a76-615e704f40c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">StaticTrace</span>\n",
       "├── gen_fn\n",
       "│   └── <span style=\"font-weight: bold\">StaticGenerativeFunction</span>\n",
       "│       └── source\n",
       "│           └── &lt;function model&gt;\n",
       "├── args\n",
       "│   └── <span style=\"font-weight: bold\">tuple</span>\n",
       "│       └──  f32[]\n",
       "├── retval\n",
       "│   └──  f32[]\n",
       "├── address_choices\n",
       "│   └── <span style=\"font-weight: bold\">Trie</span>\n",
       "│       ├── <span style=\"font-weight: bold\">:y</span>\n",
       "│       │   └── <span style=\"font-weight: bold\">DistributionTrace</span>\n",
       "│       │       ├── gen_fn\n",
       "│       │       │   └── <span style=\"font-weight: bold\">TFPDistribution</span>\n",
       "│       │       │       └── make_distribution\n",
       "│       │       │           └── (const) &lt;class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'&gt;\n",
       "│       │       ├── args\n",
       "│       │       │   └── <span style=\"font-weight: bold\">tuple</span>\n",
       "│       │       │       ├──  f32[]\n",
       "│       │       │       └──  f32[]\n",
       "│       │       ├── value\n",
       "│       │       │   └──  f32[]\n",
       "│       │       └── score\n",
       "│       │           └──  f32[]\n",
       "│       └── <span style=\"font-weight: bold\">:z</span>\n",
       "│           └── <span style=\"font-weight: bold\">DistributionTrace</span>\n",
       "│               ├── gen_fn\n",
       "│               │   └── <span style=\"font-weight: bold\">TFPDistribution</span>\n",
       "│               │       └── make_distribution\n",
       "│               │           └── (const) &lt;class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'&gt;\n",
       "│               ├── args\n",
       "│               │   └── <span style=\"font-weight: bold\">tuple</span>\n",
       "│               │       ├──  f32[]\n",
       "│               │       └──  f32[]\n",
       "│               ├── value\n",
       "│               │   └──  f32[]\n",
       "│               └── score\n",
       "│                   └──  f32[]\n",
       "├── cache\n",
       "│   └── <span style=\"font-weight: bold\">Trie</span>\n",
       "└── score\n",
       "    └──  f32[]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mStaticTrace\u001b[0m\n",
       "├── gen_fn\n",
       "│   └── \u001b[1mStaticGenerativeFunction\u001b[0m\n",
       "│       └── source\n",
       "│           └── <function model>\n",
       "├── args\n",
       "│   └── \u001b[1mtuple\u001b[0m\n",
       "│       └──  f32[]\n",
       "├── retval\n",
       "│   └──  f32[]\n",
       "├── address_choices\n",
       "│   └── \u001b[1mTrie\u001b[0m\n",
       "│       ├── \u001b[1m:y\u001b[0m\n",
       "│       │   └── \u001b[1mDistributionTrace\u001b[0m\n",
       "│       │       ├── gen_fn\n",
       "│       │       │   └── \u001b[1mTFPDistribution\u001b[0m\n",
       "│       │       │       └── make_distribution\n",
       "│       │       │           └── (const) <class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>\n",
       "│       │       ├── args\n",
       "│       │       │   └── \u001b[1mtuple\u001b[0m\n",
       "│       │       │       ├──  f32[]\n",
       "│       │       │       └──  f32[]\n",
       "│       │       ├── value\n",
       "│       │       │   └──  f32[]\n",
       "│       │       └── score\n",
       "│       │           └──  f32[]\n",
       "│       └── \u001b[1m:z\u001b[0m\n",
       "│           └── \u001b[1mDistributionTrace\u001b[0m\n",
       "│               ├── gen_fn\n",
       "│               │   └── \u001b[1mTFPDistribution\u001b[0m\n",
       "│               │       └── make_distribution\n",
       "│               │           └── (const) <class 'tensorflow_probability.substrates.jax.distributions.normal.Normal'>\n",
       "│               ├── args\n",
       "│               │   └── \u001b[1mtuple\u001b[0m\n",
       "│               │       ├──  f32[]\n",
       "│               │       └──  f32[]\n",
       "│               ├── value\n",
       "│               │   └──  f32[]\n",
       "│               └── score\n",
       "│                   └──  f32[]\n",
       "├── cache\n",
       "│   └── \u001b[1mTrie\u001b[0m\n",
       "└── score\n",
       "    └──  f32[]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jitted = jax.jit(model.simulate)\n",
    "key, sub_key = jax.random.split(key)\n",
    "tr = jitted(sub_key, (1.0,))\n",
    "console.print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d2e29-1f22-4b7e-8103-8465f91f2fb2",
   "metadata": {},
   "source": [
    "That's because `simulate_transform` and the interpreter implementation itself for `propagate` are all JAX traceable.\n",
    "\n",
    "The only difference between the `BuiltinTrace` which we first generated at the top of the notebook and this one is that `jax.jit` will lift the `1.0` argument to a `Tracer` type, versus the non-jitted interpreter which just uses the Python `float` value.\n",
    "\n",
    "And again, we can also stage out our `simulate` implementation and get a `Jaxpr` back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff30d73a-2c1e-4ea0-abec-d644d57c68a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let _normal = { lambda ; a:key<fry>[]. let\n",
       "    b:f32[1] = pjit[\n",
       "      name=_normal_real\n",
       "      jaxpr={ lambda ; c:key<fry>[]. let\n",
       "          d:f32[1] = pjit[\n",
       "            name=_uniform\n",
       "            jaxpr={ lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "                h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "                i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "                j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "                k:u32[1] = shift_right_logical j 9\n",
       "                l:u32[1] = or k 1065353216\n",
       "                m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "                n:f32[1] = sub m 1.0\n",
       "                o:f32[1] = sub i h\n",
       "                p:f32[1] = mul n o\n",
       "                q:f32[1] = add p h\n",
       "                r:f32[1] = max h q\n",
       "              in (r,) }\n",
       "          ] c -0.9999999403953552 1.0\n",
       "          s:f32[1] = erf_inv d\n",
       "          t:f32[1] = mul 1.4142135381698608 s\n",
       "        in (t,) }\n",
       "    ] a\n",
       "  in (b,) } in\n",
       "let _normal_real = { lambda ; c:key<fry>[]. let\n",
       "    d:f32[1] = pjit[\n",
       "      name=_uniform\n",
       "      jaxpr={ lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "          h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "          i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "          j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "          k:u32[1] = shift_right_logical j 9\n",
       "          l:u32[1] = or k 1065353216\n",
       "          m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "          n:f32[1] = sub m 1.0\n",
       "          o:f32[1] = sub i h\n",
       "          p:f32[1] = mul n o\n",
       "          q:f32[1] = add p h\n",
       "          r:f32[1] = max h q\n",
       "        in (r,) }\n",
       "    ] c -0.9999999403953552 1.0\n",
       "    s:f32[1] = erf_inv d\n",
       "    t:f32[1] = mul 1.4142135381698608 s\n",
       "  in (t,) } in\n",
       "let _uniform = { lambda ; e:key<fry>[] f:f32[] g:f32[]. let\n",
       "    h:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] f\n",
       "    i:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n",
       "    j:u32[1] = random_bits[bit_width=32 shape=(1,)] e\n",
       "    k:u32[1] = shift_right_logical j 9\n",
       "    l:u32[1] = or k 1065353216\n",
       "    m:f32[1] = bitcast_convert_type[new_dtype=float32] l\n",
       "    n:f32[1] = sub m 1.0\n",
       "    o:f32[1] = sub i h\n",
       "    p:f32[1] = mul n o\n",
       "    q:f32[1] = add p h\n",
       "    r:f32[1] = max h q\n",
       "  in (r,) } in\n",
       "{ lambda ; u:u32[2] v:f32[]. let\n",
       "    w:key<fry>[] = random_wrap[impl=fry] u\n",
       "    x:key<fry>[2] = random_split[shape=(2,)] w\n",
       "    y:u32[2,2] = random_unwrap x\n",
       "    z:u32[1,2] = slice[limit_indices=(1, 2) start_indices=(0, 0) strides=(1, 1)] y\n",
       "    ba:u32[2] = squeeze[dimensions=(0,)] z\n",
       "    bb:u32[1,2] = slice[\n",
       "      limit_indices=(2, 2)\n",
       "      start_indices=(1, 0)\n",
       "      strides=(1, 1)\n",
       "    ] y\n",
       "    bc:u32[2] = squeeze[dimensions=(0,)] bb\n",
       "    bd:key<fry>[] = random_wrap[impl=fry] bc\n",
       "    be:f32[1] = pjit[name=_normal jaxpr=_normal] bd\n",
       "    bf:f32[1] = mul be 1.0\n",
       "    bg:f32[1] = add bf 0.0\n",
       "    bh:f32[1] = mul bg 1.0\n",
       "    bi:f32[] = convert_element_type[new_dtype=float32 weak_type=False] v\n",
       "    bj:f32[1] = add bh bi\n",
       "    bk:f32[] = reshape[dimensions=None new_sizes=()] bj\n",
       "    bl:f32[] = div bk 1.0\n",
       "    bm:f32[] = convert_element_type[new_dtype=float32 weak_type=False] v\n",
       "    bn:f32[] = div bm 1.0\n",
       "    bo:f32[] = sub bl bn\n",
       "    bp:f32[] = integer_pow[y=2] bo\n",
       "    bq:f32[] = mul -0.5 bp\n",
       "    br:f32[] = log 1.0\n",
       "    bs:f32[] = add 0.9189385175704956 br\n",
       "    bt:f32[] = sub bq bs\n",
       "    bu:f32[] = add 0.0 bt\n",
       "    bv:f32[] = add bk v\n",
       "    bw:key<fry>[] = random_wrap[impl=fry] ba\n",
       "    bx:key<fry>[2] = random_split[shape=(2,)] bw\n",
       "    by:u32[2,2] = random_unwrap bx\n",
       "    bz:u32[1,2] = slice[\n",
       "      limit_indices=(1, 2)\n",
       "      start_indices=(0, 0)\n",
       "      strides=(1, 1)\n",
       "    ] by\n",
       "    _:u32[2] = squeeze[dimensions=(0,)] bz\n",
       "    ca:u32[1,2] = slice[\n",
       "      limit_indices=(2, 2)\n",
       "      start_indices=(1, 0)\n",
       "      strides=(1, 1)\n",
       "    ] by\n",
       "    cb:u32[2] = squeeze[dimensions=(0,)] ca\n",
       "    cc:key<fry>[] = random_wrap[impl=fry] cb\n",
       "    cd:f32[1] = pjit[name=_normal jaxpr=_normal] cc\n",
       "    ce:f32[1] = mul cd 1.0\n",
       "    cf:f32[1] = add ce 0.0\n",
       "    cg:f32[1] = mul cf 1.0\n",
       "    ch:f32[1] = add cg bv\n",
       "    ci:f32[] = reshape[dimensions=None new_sizes=()] ch\n",
       "    cj:f32[] = div ci 1.0\n",
       "    ck:f32[] = div bv 1.0\n",
       "    cl:f32[] = sub cj ck\n",
       "    cm:f32[] = integer_pow[y=2] cl\n",
       "    cn:f32[] = mul -0.5 cm\n",
       "    co:f32[] = log 1.0\n",
       "    cp:f32[] = add 0.9189385175704956 co\n",
       "    cq:f32[] = sub cn cp\n",
       "    cr:f32[] = add bu cq\n",
       "  in (v, ci, v, 1.0, bk, bt, bv, 1.0, ci, cq, cr) }"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(model.simulate)(key, (1.0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ffc40a-2007-414a-8062-35667501bfe4",
   "metadata": {},
   "source": [
    "Giving us our pure, array math code. You can't help but admit that that's pretty elegant! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6bd64e-7ef8-4deb-adc0-e7989420bb59",
   "metadata": {},
   "source": [
    "## How does `propagate` work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba03967-f424-4195-9d5d-4dcb3b13cdc0",
   "metadata": {},
   "source": [
    "Now, in this section - we're going to talk about the nitty gritty of `propagate` itself. What exactly is this interpreter doing? Let's examine the context surrounding the call to `propagate`:\n",
    "\n",
    "```python\n",
    "def simulate_transform(f, **kwargs):\n",
    "    def _inner(key, args):\n",
    "        closed_jaxpr, (flat_args, in_tree, out_tree) = stage(f)(\n",
    "            key, *args, **kwargs\n",
    "        )\n",
    "        jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n",
    "        handler = Simulate()\n",
    "        final_env, ret_state = propagate(\n",
    "            # A lattice type\n",
    "            Bare,\n",
    "            \n",
    "            # Lattice propagation rules\n",
    "            bare_propagation_rules,\n",
    "            \n",
    "            # The Jaxpr which we wish to interpret\n",
    "            jaxpr,\n",
    "            \n",
    "            # Trace-time constants\n",
    "            [Bare.new(v) for v in consts],\n",
    "            \n",
    "            # Input cells\n",
    "            list(map(Bare.new, flat_args)),\n",
    "            \n",
    "            # Output cells\n",
    "            [Bare.unknown(var.aval) for var in jaxpr.outvars],\n",
    "            \n",
    "            # How we handle `trace`.\n",
    "            handler=handler,\n",
    "        )\n",
    "        ...\n",
    "\n",
    "    return _inner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2c0e7-1af0-43c9-9519-ae6f5c39c80d",
   "metadata": {},
   "source": [
    "First, we stage our model function into a `Jaxpr` - when we perform the staging process, everything (e.g. custom datatypes which are `Pytree` implementors) gets flattened out to array leaves.\n",
    "\n",
    "After we stage, we collect all the data which we want to use to initialize our interpreter's environment with - but we encounter our first bit of complexity. \n",
    "\n",
    "What is `Bare`? And what is a `Cell`? Let's start with the latter question: a `Cell` is an abstract type which represents a _lattice value_.\n",
    "\n",
    "To understand what a _lattice value_ is - it's worth gaining a high-level picture of what `propagate` attempts to do. `propagate` is an interpreter based on mixed concrete/abstract interpretation - it treats the `Jaxpr` as a graph - where the operations are nodes in the graph, and the SSA values (e.g. the named registers like `ci`, `cj`, etc) are edges.\n",
    "\n",
    "The interpreter will iterate over the graph - attempting to update information about the edges by applying _propagation rules_ (hence the name, `propagate`) which we define (`bare_propagation_rules`, above).\n",
    "\n",
    "A propagation rule accepts a list of input cells (the SSA edges which flow into the operation) and a list of output cells. It returns a new modified list of input cells, and a new modified list of output cells, as well as a state value (in this notebook, we won't discuss the state value - it's unneeded for the interfaces we will describe). \n",
    "\n",
    "The way the interpreter works is that it keeps a queue of nodes and an environment which maps SSA values to lattice values. We pop a node off the queue, grab the existing lattice values for input SSA values and output SSA values, attempt to update them using a propagation rule, and then store the update in the environment. In addition, after we attempt to update the cells - _we determine if the update has changed the information level of any of the cells_. If the information level has changed for any cell (as measured using the partial order on lattice values), we add any nodes which the SSA value associated with that cell flows into back onto the queue.\n",
    "\n",
    "This process describes an iterative algorithm which attempts to compute an information fixpoint - defined by a state transition function (which operates on the state of all cells in the `Jaxpr` - the environment) which we get to customize using propagation rules.\n",
    "\n",
    "I'm not going to inline any of the implementation of this interpreter into this notebook. I'll refer the reader to [the implementation of the interpreter](https://github.com/probcomp/genjax/blob/main/src/genjax/core/propagate.py).^[Note that the ideas behind this interpreter are quite widespread - but the original implementation (which the GenJAX authors modified) came from [Oryx](https://github.com/jax-ml/oryx), and that implementation initially came from Roy Frostig (as far as we can tell).]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33732e-ae1d-469d-9d3b-76941638a485",
   "metadata": {},
   "source": [
    "### What happens in `simulate`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0ed87-9fc4-4646-9775-db3486cd94e4",
   "metadata": {},
   "source": [
    "Great - so how do we utilize this interpreter idea to implement the `simulate_transform` described above?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
