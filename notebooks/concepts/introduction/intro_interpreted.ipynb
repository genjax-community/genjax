{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e424b02a6e639ef",
   "metadata": {},
   "source": [
    "## Introduction to Interpreted GenJAX\n",
    "\n",
    "This notebook will give a tour of the interpreted dialect of GenJAX, the probabilistic computing system developed by the [MIT Probabilistic Computing Laboratory](http://probcomp.csail.mit.edu/). This dialect is meant to offer access to the Gen model of automatic inference in a way that avoids the constraints imposed by the acceleration technology in JAX. JAX acceleration offers an immense benefit in the speed at which inference can be done, but requires more up-front design work on the modeling and inference technique.\n",
    "\n",
    "In particular, you have to commit to fixed size for the vectors and tensors used in your model, as well as avoid native Python control flow in favor of a representation for branching computation that is compatible with the numerical linear algebra that the accelerated code must use.\n",
    "\n",
    "For the present, we'll set those concerns aside and proceed with a simple inference task in the easy-going interpreted dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152875b5dfaa18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from genjax import interpreted\n",
    "\n",
    "key = jax.random.PRNGKey(314159)\n",
    "console = genjax.console(enforce_checkify=True, width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1aa72",
   "metadata": {},
   "source": [
    "A few notes about the prefatory material above. GenJAX uses JAX, and JAX provides its own flavor of numpy, which we will call `jnp` in this notebook. The main difference between jnp and numpy is that vectors and tensors are immutable once constructed: instead of changing them, you must make a copy with the updates you require. We could use regular numpy in the interpreted dialect, but it turns out that it's not difficult to work with jnp even in the interpreted case.\n",
    "\n",
    "GenJAX allows the production of reproducible scientific work through the use of a \"splittable\" random number generator. You can set the initial seed of the generator as we have done above. Then, when we need random numbers within a function, we will split the generator and hand one fork to the function and keep the other fork at the topmost level. In this way, provided that the notebook cells are evaluated from top to bottom, all the random choices will be made in the same way each time. We encourage this technique for your own work to produce reproducible scientific communication.\n",
    "\n",
    "Let's create our first generative function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interpreted\n",
    "def g(x):\n",
    "    b = genjax.flip(x) @ \"b\"\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbf999",
   "metadata": {},
   "source": [
    "The `@interpreted` decoration is a bridge between an ordinary Python function and the [Generative Function](https://www.gen.dev/docs/stable/ref/gfi/#Generative-Functions) interface, which is at the heart of the Gen model of probabilistic programming. You can regard the `@` sign as something like the $\\sim$ operator in statistics literature (although we use it backwards) to describe b as a random variable with a Bernoulli distribution (like tossing a coin with $x$ as the probability of heads). The GenJAX system uses the string name `\"b\"` to record the name of the random variable; it cannot see the name of the Python variable to which you assigned the value, but of course it's convenient to use the same name in both cases. (It's worth remembering that there's also a `genjax.bernoulli` function but its argument is log-odds.)\n",
    "\n",
    "The decoration has equipped our function $g$ with the method `simulate`, which will draw a value from the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc14c434d3de4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "tr = g.simulate(sub_key, (0.3,))\n",
    "console.print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16602c8",
   "metadata": {},
   "source": [
    "Running the function has produced a tree structure called a _trace_ which records the result of random choices made during the function's execution. As generative functions call other generative functions, the trace will become more elaborate. The value itself is in an array; to see it, we can call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d02bce6c22b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.get_retval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c46873",
   "metadata": {},
   "source": [
    "If we'd like to draw more of a sample, we'll need more subkeys to get the randomness we need, which we can do like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4277e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "key, *sub_keys = jax.random.split(key, n_samples + 1)\n",
    "[g.simulate(k, (0.3,)).get_retval().item() for k in sub_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f63e12",
   "metadata": {},
   "source": [
    "### Compound Models\n",
    "Our little function `h` didn't do much: it was just a wrapper for `genjax.flip`, which is already a generative function. Let's create some more involved functions that mix distributions in interesting ways. We're going to consider a simple dataset based on a quadratic function with some outliers. The task is to write an inference algorithm that can infer the coefficients of the hidden polynomial while automatically classifying outliers (thus denying them the opportunity to skew the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28423194e6895e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(coefficients):\n",
    "    \"\"\"Given coefficients of a polynomial a_0, a_1, ..., return a function\n",
    "    computing a_0 x^0 + a_1 x^1 + ...\"\"\"\n",
    "\n",
    "    def f(x):\n",
    "        powers_of_x = jnp.array([x**i for i in range(len(coefficients))])\n",
    "        return jnp.sum(coefficients * powers_of_x)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "@interpreted\n",
    "def model_y(x, f):\n",
    "    \"\"\"Given x and f, model f(x) plus a small amount of gaussian noise.\"\"\"\n",
    "    y = genjax.normal(f(x), 0.3) @ \"value\"\n",
    "    return y\n",
    "\n",
    "\n",
    "@interpreted\n",
    "def outlier_model(x, f):\n",
    "    \"\"\"Like model_y, except this time we allow a huge variance in the noise,\n",
    "    to model an outlying value\"\"\"\n",
    "    y = genjax.normal(f(x), 30.0) @ \"value\"\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83effd670ec47d71",
   "metadata": {},
   "source": [
    "Now we have a generative functions for a polynomial model with inliers and outliers. The next step is to _generate_ candidate polynomials with inlying and outlying points by welding these small generative functions together into a more elaborate model which represents our prior belief about the structure of the data we might observe. This involves flipping an (unfair) coin to determine whether we have an inlier or outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b332610cbff4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interpreted\n",
    "def kernel(xs, f):\n",
    "    y = []\n",
    "    for i, x in enumerate(xs):\n",
    "        is_outlier = genjax.flip(0.1) @ (\"outlier\", i)\n",
    "\n",
    "        if is_outlier:\n",
    "            model = outlier_model\n",
    "        else:\n",
    "            model = model_y\n",
    "\n",
    "        y.append(model(x, f) @ (\"y\", i))\n",
    "\n",
    "    return jnp.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ee8aa",
   "metadata": {},
   "source": [
    "We pause here to note one universal feature of GenJAX: you must give every traced value a unique name. Since we are generating a vector of $y$ values, we label each of them with the tuple $(\\mathbf{y}, i)$. \n",
    "\n",
    "Finally we draw the polynomial coefficients from a multivariate normal distribution (which is fancier than we need. We're using it with a diagonal covariance matrix, which amounts to individual independent selections with no cross-correlation, but it makes the code easy to write and allows us to batch up the selection of all the polynomial's coefficients into one random variable `alpha`.) It's at this point that we are committing to a polynomial of degree $\\le 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31196ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interpreted\n",
    "def model(xs):\n",
    "    coefficients = genjax.mv_normal(jnp.zeros(3), 2.0 * jnp.identity(3)) @ \"alpha\"\n",
    "    f = polynomial(coefficients)\n",
    "    ys = kernel(xs, f) @ \"ys\"\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee10af75418d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.arange(0, 10, 0.5)\n",
    "key, sub_key = jax.random.split(key)\n",
    "tr = model.simulate(sub_key, (data,))\n",
    "tr.get_retval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cef5da1022fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.get_retval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5811e",
   "metadata": {},
   "source": [
    "We hope that this is an example generated by a degree 2 polynomial possibly with some outliers... but is it? It's common in the Gen world to \"visualize the prior.\" Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f4a5e9db8de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prior(key, w, h):\n",
    "    f, axes = plt.subplots(w, h, figsize=(8, 8), sharex=True, sharey=True)\n",
    "    for ax in axes.flatten():\n",
    "        key, sub_key = jax.random.split(key)\n",
    "        tr = model.simulate(sub_key, (data,))\n",
    "        ax.scatter(data, tr.get_retval())\n",
    "\n",
    "\n",
    "key, sub_key = jax.random.split(key)\n",
    "visualize_prior(sub_key, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d495a",
   "metadata": {},
   "source": [
    "Not bad. With the preliminaries out of the way, let's turn to the inference task that motivated this notebook. The idea is to observe some data in the wild--a \"ground truth\"--and test our hypothesis that it is a polynomial-with-outliers by inferring the parameters of our model. To do that, behind our back, we will select some polynomial coefficients and then manually inject an obvious outlying value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6081392f5dd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\n",
    "ys = jnp.array(2.0 * xs + 1.5 + xs**2)\n",
    "ys = ys.at[2].set(50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0d448a478a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_data, ax_data = plt.subplots(figsize=(6, 6))\n",
    "ax_data.scatter(xs, ys, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ffbbea34f39eb",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The next step is to _constrain_ the y values to the observed data while letting the _model parameters_ roam freely. To do this, we construct a [ChoiceMap](https://probcomp.github.io/genjax/genjax/library/core/datatypes.html#genjax.core.ChoiceMap). At this point, the significance of the names of the random variables we have chosen with the `@` operator becomes clear, as well as the influence of the structure of the functions we have written. If you look back at those functions, you will see that the $(\\mathbf{y}, i)$ values are nested under the variable $\\bf{ys}$, and that each of those is assigned a $\\bf{value}$ in the `outlier` and `model` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca595cbfac11555",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = genjax.choice_map()\n",
    "for i, y in enumerate(ys):\n",
    "    observations[\"ys\", \"y\", i, \"value\"] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f788c",
   "metadata": {},
   "source": [
    "GenJAX provides numerous sophisticated inference algorithms designed to operate seamlessly with generative functions. For this introduction we will content ourselves with a simple one: Sequential Importance resampling. We won't spend too much time understanding this technique--it is described greater mathematical detail in other notebooks here--but will simply say that we generate a batch of random numbers, score them against the ground truth, and then use the scores as the weight of a random categorical choice (a kind of survival of the fittest), and repeat until, hopefully, the model explains our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474726ff349834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_importance_resampling(model, n_samples):\n",
    "    def _inner(key, observations, model_args):\n",
    "        \"\"\"Generate a list of importance samples. Each such sample returns a tuple of\n",
    "        (trace, log_weight or \"score\"). Treat the list as a weighted ensemble of\n",
    "        choices, and draw one. This is the result of one SIR step.\"\"\"\n",
    "        resample_key, sub_key = jax.random.split(key)\n",
    "        sub_keys = jax.random.split(sub_key, n_samples)\n",
    "        tr_lw_pairs = [\n",
    "            model.importance(sub_key, observations, model_args) for sub_key in sub_keys\n",
    "        ]\n",
    "        lws = [tr_lw[1] for tr_lw in tr_lw_pairs]\n",
    "        index = genjax.categorical.sample(resample_key, lws)\n",
    "        return tr_lw_pairs[index][0]\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45ca3fb96d8d72",
   "metadata": {},
   "source": [
    "The next step is to \"sample from the posterior.\" Here we will see if the SIR algorithm is converging toward an explanation of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc11c4c664e4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "model_args = (xs,)\n",
    "key, *sub_keys = jax.random.split(key, N + 1)\n",
    "samples = [\n",
    "    sampling_importance_resampling(model, 2 * N)(sub_key, observations, model_args)\n",
    "    for sub_key in sub_keys\n",
    "]\n",
    "coefficients = [s[\"alpha\"] for s in samples]\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3a52d73921ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_values(ax_inf, xs, coefficients, **kwargs):\n",
    "    f = polynomial(coefficients)\n",
    "    ax_inf.plot(xs, [f(x) for x in xs], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652551f7cae760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inf, ax_inf = plt.subplots(figsize=(6, 6))\n",
    "ax_inf.scatter(xs, ys, color=\"blue\")\n",
    "for cs in coefficients:\n",
    "    plot_polynomial_values(ax_inf, xs, cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb3631",
   "metadata": {},
   "source": [
    "Not too bad. That took a while, but it does appear that the sampling procedure has accomplished both of its goals: it has found plausible polynomial coefficients, and further, it has declined to allow the outlier to overly influence the result. To verify that, we can calculate what fraction of the samples classified $y_2$ as an outlier like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d699ae0ec6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_at_2 = [s[\"ys\", \"outlier\", 2] for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db993a37d6ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(outlier_at_2) / len(outlier_at_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4bd61",
   "metadata": {},
   "source": [
    "We invite you to dive deeper into GenJAX, but learning more about how easily JAX can be used to accelerate the computation we just performed, as well as the many state of the art inference techniques the Gen system offers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac6159",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b134ec6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf41f61e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa923ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
