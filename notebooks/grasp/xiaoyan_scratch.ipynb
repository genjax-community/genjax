{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3219b5-23ac-4d4f-8362-31a6ce71ad33",
   "metadata": {},
   "source": [
    "## ELBO\n",
    "\n",
    "$$E_{z \\sim Q(\\theta)} [ \\log P(z, x) - \\log Q(z; \\theta)] \\leq \\log P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28bcd55-d749-4ccf-ab0d-82239747cfa0",
   "metadata": {},
   "source": [
    "$$E_{z \\sim Q}[\\frac{P(z, x)}{Q(z)}] = P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e46842-56c1-46a9-ba98-edb959ed3c48",
   "metadata": {},
   "source": [
    "1. You can sample from $Q$\n",
    "2. You can exactly evaluate the joint density $P(z, x)$ pointwise at any point $(z, x)$\n",
    "Original constraint (vanilla VI):\n",
    "3. You can exactly evaluate the density $Q(z; \\theta)$ (you can't use something like this: $Q(z, y; \\theta)$ -- to get $Q(z; \\theta) = \\int Q(z, y; \\theta) \\ dy$)\n",
    "\n",
    "New idea (from Alex's work on RAVI / GenSP):\n",
    "\n",
    "3. (Loose, new) You can estimate the density of $Q(z; \\theta)$ -- even if you have a original $Q(z, y; \\theta)$, you \"pseudo\"-marginalize out $y$ -- which basically means you use inference to construct a _density estimator_ for $Q(z; \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036265dc-e480-4f71-927c-bf7db0fa27a3",
   "metadata": {},
   "source": [
    "## IWAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a5714-2c61-429d-90d3-5a625a58d264",
   "metadata": {},
   "source": [
    "Modified ELBO -- still an evidence lower bound, but essentially you use importance sampling to make the bound tighter.\n",
    "\n",
    "This objective is equivalent to using a proposal $Q = \\text{SIR}(\\text{target} = P, \\text{proposal} = Q'(\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda3722-cab5-424c-95dc-7802867cfe26",
   "metadata": {},
   "source": [
    "## Gradient automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0699cb4-07c5-4f7d-903d-94b647e55043",
   "metadata": {},
   "source": [
    "$$\\nabla_\\theta E_{z \\sim Q(\\theta)} [ \\log P(z, x) - \\log Q(z; \\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9efe2-ce5b-4077-98f6-45fed6666bda",
   "metadata": {},
   "source": [
    "Alex + Mathieu: Automatic differentiation of expected values (POPL 2023)\n",
    "\n",
    "$$L := E_{x \\sim Q(\\theta)}[f(x; \\theta)]$$\n",
    "\n",
    "ADEV -- will give you unbiased $\\hat{\\nabla_\\theta}$ gradient estimators of the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a39b2-671e-4820-afcf-8982b0c29081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7f1d28f-a6fd-4174-b409-e6de41fad914",
   "metadata": {},
   "source": [
    "Inclusive KL optimization: $$E_{z \\sim P(z | x)}[\\log P(z, x) - \\log Q(z, x)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fa4c0-c7d4-4012-801e-4ba477c1c8b2",
   "metadata": {},
   "source": [
    "What people actually do: $$L(\\theta) = E_{z \\sim SIR(P, Q(\\phi), 1000)}[\\log P(z, x) - \\log Q(z;x, \\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8b61d-7c92-48a2-aef7-94e9573b7937",
   "metadata": {},
   "source": [
    "This sort of setting is often used in something called reweighted wake-sleep (RWS) --\n",
    "\n",
    "model-based RL -- with policies that include inference (not necessarily good, two objectives -- might confuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d0f74-6434-4a14-8a9b-e7adcb637f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
