{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de0133f-4e97-401c-b216-e0bc64664444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple\n",
    "import pyro\n",
    "import optax\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import pyro.contrib.examples.multi_mnist as multi_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from matplotlib import rcParams\n",
    "from scipy.interpolate import griddata\n",
    "import genjax\n",
    "import adevjax\n",
    "from genjax import grasp\n",
    "\n",
    "key = jax.random.PRNGKey(314159)\n",
    "console = genjax.pretty()\n",
    "sns.set_theme(style=\"white\")\n",
    "# font_path = (\n",
    "#     \"/home/femtomc/.local/share/fonts/Unknown Vendor/TrueType/Lato/Lato_Bold.ttf\"\n",
    "# )\n",
    "# font_manager.fontManager.addfont(font_path)\n",
    "# custom_font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "# rcParams[\"font.family\"] = custom_font_name\n",
    "\n",
    "console = genjax.pretty()\n",
    "key = jax.random.PRNGKey(314159)\n",
    "label_fontsize = 70  # Set the desired font size here\n",
    "\n",
    "smoke_test = \"CI\" in os.environ\n",
    "assert pyro.__version__.startswith(\"1.8.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4985e835-14c9-43f0-8652-34da112dd6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mFigure\u001b[0m\u001b[39m size 80\u001b[0m\u001b[1;36m0x200\u001b[0m\u001b[39m with \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m Axes\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inpath = \"./data/air/.data\"\n",
    "X_np, Y = multi_mnist.load(inpath)\n",
    "X_np = X_np.astype(np.float32)\n",
    "X_np /= 255.0\n",
    "mnist = jnp.array(X_np)\n",
    "true_counts = jnp.array([len(objs) for objs in Y])\n",
    "\n",
    "\n",
    "def show_images(imgs):\n",
    "    fig = plt.figure(figsize=(8, 2))\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax = plt.subplot(1, len(imgs), i + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(img, cmap=\"gray_r\")\n",
    "\n",
    "\n",
    "show_images(mnist[9:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65227bc6-fd5e-486d-9cdc-0bfdf6cbde95",
   "metadata": {},
   "source": [
    "## Defining the variational ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78d35c-3e79-4ea7-95d9-82106cb22ad7",
   "metadata": {},
   "source": [
    "### Utilities / learnable pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38987718-1631-423e-8b70-95a469742beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from genjax import Pytree\n",
    "import equinox as eqx\n",
    "from genjax.typing import Any\n",
    "from genjax.typing import Tuple\n",
    "from genjax.typing import FloatArray\n",
    "from genjax.typing import Int\n",
    "from genjax.typing import IntArray\n",
    "from genjax.typing import PRNGKey\n",
    "from genjax.typing import typecheck\n",
    "\n",
    "# Utilities for defining the model and the guide.\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Decoder(Pytree):\n",
    "    dense_1: Any\n",
    "    dense_2: Any\n",
    "\n",
    "    def flatten(self):\n",
    "        return (self.dense_1, self.dense_2), ()\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls, key1, key2):\n",
    "        dense_1 = eqx.nn.Linear(50, 200, key=key1)\n",
    "        dense_2 = eqx.nn.Linear(200, 400, key=key2)\n",
    "        return Decoder(dense_1, dense_2)\n",
    "\n",
    "    def __call__(self, z_what):\n",
    "        v = self.dense_1(z_what)\n",
    "        v = jax.nn.leaky_relu(v)\n",
    "        v = self.dense_2(v)\n",
    "        return jax.nn.sigmoid(v)\n",
    "\n",
    "\n",
    "# Create our decoder.\n",
    "key, sub_key1, sub_key2 = jax.random.split(key, 3)\n",
    "decoder = Decoder.new(sub_key1, sub_key2)\n",
    "\n",
    "# Create our RNN for the guide.\n",
    "key, sub_key = jax.random.split(key)\n",
    "rnn = eqx.nn.LSTMCell(2554, 256, key=sub_key)\n",
    "\n",
    "# Create our baseline RNN and predict layer for the guide.\n",
    "key, sub_key = jax.random.split(key)\n",
    "bl_rnn = eqx.nn.LSTMCell(2554, 256, key=sub_key)\n",
    "key, sub_key = jax.random.split(key)\n",
    "bl_predict = eqx.nn.Linear(256, 1, key=sub_key)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Encoder(Pytree):\n",
    "    dense_1: Any\n",
    "    dense_2: Any\n",
    "\n",
    "    def flatten(self):\n",
    "        return (self.dense_1, self.dense_2), ()\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls, key1, key2):\n",
    "        dense_1 = eqx.nn.Linear(400, 200, key=key1)\n",
    "        dense_2 = eqx.nn.Linear(200, 100, key=key2)\n",
    "        return Encoder(dense_1, dense_2)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        v = self.dense_1(data)\n",
    "        v = jax.nn.leaky_relu(v)\n",
    "        v = self.dense_2(v)\n",
    "        return v[0:50], jax.nn.softplus(v[50:])\n",
    "\n",
    "\n",
    "key, sub_key1, sub_key2 = jax.random.split(key, 3)\n",
    "encoder = Encoder.new(sub_key1, sub_key2)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Predict(Pytree):\n",
    "    dense: Any\n",
    "\n",
    "    def flatten(self):\n",
    "        return (self.dense,), ()\n",
    "\n",
    "    @classmethod\n",
    "    def new(cls, key):\n",
    "        dense = eqx.nn.Linear(256, 7, key=key)\n",
    "        return Predict(dense)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        a = self.dense(h)\n",
    "        z_pres_p = jax.nn.sigmoid(a[0:1])\n",
    "        z_where_loc = a[1:4]\n",
    "        z_where_scale = jax.nn.softplus(a[4:])\n",
    "        return z_pres_p, z_where_loc, z_where_scale\n",
    "\n",
    "\n",
    "key, sub_key = jax.random.split(key)\n",
    "predict = Predict.new(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82446361-62cd-4cec-a8aa-d76b477603d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# STN #\n",
    "#######\n",
    "\n",
    "# modified from https://github.com/kevinzakka/spatial-transformer-network/blob/master/stn/transformer.py\n",
    "\n",
    "\n",
    "def affine_grid_generator(height, width, theta):\n",
    "    \"\"\"\n",
    "    This function returns a sampling grid, which when\n",
    "    used with the bilinear sampler on the input feature\n",
    "    map, will create an output feature map that is an\n",
    "    affine transformation [1] of the input feature map.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    - height: desired height of grid/output. Used\n",
    "      to downsample or upsample.\n",
    "\n",
    "    - width: desired width of grid/output. Used\n",
    "      to downsample or upsample.\n",
    "\n",
    "    - theta: affine transform matrices of shape (num_batch, 2, 3).\n",
    "      For each image in the batch, we have 6 theta parameters of\n",
    "      the form (2x3) that define the affine transformation T.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - normalized grid (-1, 1) of shape (num_batch, 2, H, W).\n",
    "      The 2nd dimension has 2 components: (x, y) which are the\n",
    "      sampling points of the original image for each point in the\n",
    "      target image.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    [1]: the affine transformation allows cropping, translation,\n",
    "         and isotropic scaling.\n",
    "    \"\"\"\n",
    "    num_batch = theta.shape[0]\n",
    "\n",
    "    # create normalized 2D grid\n",
    "    x = jnp.linspace(-1.0, 1.0, width)\n",
    "    y = jnp.linspace(-1.0, 1.0, height)\n",
    "    x_t, y_t = jnp.meshgrid(x, y)\n",
    "\n",
    "    # flatten\n",
    "    x_t_flat = jnp.reshape(x_t, [-1])\n",
    "    y_t_flat = jnp.reshape(y_t, [-1])\n",
    "\n",
    "    # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
    "    ones = jnp.ones_like(x_t_flat)\n",
    "    sampling_grid = jnp.stack([x_t_flat, y_t_flat, ones])\n",
    "\n",
    "    # repeat grid num_batch times\n",
    "    sampling_grid = jnp.expand_dims(sampling_grid, axis=0)\n",
    "    sampling_grid = jnp.tile(sampling_grid, [num_batch, 1, 1])\n",
    "\n",
    "    # transform the sampling grid - batch multiply\n",
    "    batch_grids = jnp.matmul(theta, sampling_grid)\n",
    "    # batch grid has shape (num_batch, 2, H*W)\n",
    "\n",
    "    # reshape to (num_batch, 2, H, W)\n",
    "    batch_grids = jnp.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "\n",
    "    return batch_grids\n",
    "\n",
    "\n",
    "def bilinear_sampler(img, x, y):\n",
    "    \"\"\"\n",
    "    Performs bilinear sampling of the input images according to the\n",
    "    normalized coordinates provided by the sampling grid. Note that\n",
    "    the sampling is done identically for each channel of the input.\n",
    "\n",
    "    To test if the function works properly, output image should be\n",
    "    identical to input image when theta is initialized to identity\n",
    "    transform.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    - img: batch of images in (B, H, W, C) layout.\n",
    "    - grid: x, y which is the output of affine_grid_generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - out: interpolated images according to grids. Same size as grid.\n",
    "    \"\"\"\n",
    "    H = jnp.shape(img)[1]\n",
    "    W = jnp.shape(img)[2]\n",
    "    max_y = H - 1\n",
    "    max_x = W - 1\n",
    "    zero = jnp.zeros([], dtype=int)\n",
    "\n",
    "    # rescale x and y to [0, W-1/H-1]\n",
    "    x = 0.5 * ((x + 1.0) * max_x - 1)\n",
    "    y = 0.5 * ((y + 1.0) * max_y - 1)\n",
    "\n",
    "    # grab 4 nearest corner points for each (x_i, y_i)\n",
    "    x0 = jnp.floor(x).astype(int)\n",
    "    x1 = x0 + 1\n",
    "    y0 = jnp.floor(y).astype(int)\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # clip to range [0, H-1/W-1] to not violate img boundaries\n",
    "    x0 = jnp.clip(x0, zero, max_x)\n",
    "    x1 = jnp.clip(x1, zero, max_x)\n",
    "    y0 = jnp.clip(y0, zero, max_y)\n",
    "    y1 = jnp.clip(y1, zero, max_y)\n",
    "\n",
    "    # get pixel value at corner coords\n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "\n",
    "    # recast as float for delta calculation\n",
    "    x0 = x0.astype(float)\n",
    "    x1 = x1.astype(float)\n",
    "    y0 = y0.astype(float)\n",
    "    y1 = y1.astype(float)\n",
    "\n",
    "    # calculate deltas\n",
    "    wa = (x1 - x) * (y1 - y)\n",
    "    wb = (x1 - x) * (y - y0)\n",
    "    wc = (x - x0) * (y1 - y)\n",
    "    wd = (x - x0) * (y - y0)\n",
    "\n",
    "    # add dimension for addition\n",
    "    wa = jnp.expand_dims(wa, axis=3)\n",
    "    wb = jnp.expand_dims(wb, axis=3)\n",
    "    wc = jnp.expand_dims(wc, axis=3)\n",
    "    wd = jnp.expand_dims(wd, axis=3)\n",
    "\n",
    "    # compute output\n",
    "    out = wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_pixel_value(img, x, y):\n",
    "    \"\"\"\n",
    "    Utility function to get pixel value for coordinate\n",
    "    vectors x and y from a  4D tensor image.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    - img: tensor of shape (B, H, W, C)\n",
    "    - x: flattened tensor of shape (B*H*W,)\n",
    "    - y: flattened tensor of shape (B*H*W,)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - output: tensor of shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    batch_size, height, width = jnp.shape(x)\n",
    "\n",
    "    batch_idx = jnp.arange(0, batch_size)\n",
    "    batch_idx = jnp.reshape(batch_idx, (batch_size, 1, 1))\n",
    "    b = jnp.tile(batch_idx, (1, height, width))\n",
    "\n",
    "    indices = jnp.stack([b, y, x], 3)\n",
    "\n",
    "    return gather_nd(img, indices)\n",
    "\n",
    "\n",
    "# from: https://github.com/google/jax/discussions/6119\n",
    "def gather_nd_unbatched(params, indices):\n",
    "    return params[tuple(jnp.moveaxis(indices, -1, 0))]\n",
    "\n",
    "\n",
    "def gather_nd(params, indices, batch=False):\n",
    "    if not batch:\n",
    "        return gather_nd_unbatched(params, indices)\n",
    "    else:\n",
    "        return vmap(gather_nd_unbatched, (0, 0), 0)(params, indices)\n",
    "\n",
    "\n",
    "def expand_z_where(z_where):\n",
    "    # Takes 3-dimensional vectors, and massages them into 2x3 matrices with elements like so:\n",
    "    # [s,x,y] -> [[s,0,x],\n",
    "    #             [0,s,y]]\n",
    "    n = 1\n",
    "    expansion_indices = jnp.array([1, 0, 2, 0, 1, 3])\n",
    "    z_where = jnp.expand_dims(z_where, axis=0)\n",
    "    out = jnp.concatenate((jnp.broadcast_to(jnp.zeros([1, 1]), (n, 1)), z_where), 1)\n",
    "    return jnp.reshape(out[:, expansion_indices], (n, 2, 3))\n",
    "\n",
    "\n",
    "def object_to_image(z_where, obj):\n",
    "    n = 1\n",
    "    theta = expand_z_where(z_where)\n",
    "    grid = affine_grid_generator(50, 50, theta)\n",
    "    x_s = grid[:, 0, :, :]\n",
    "    y_s = grid[:, 1, :, :]\n",
    "    out = bilinear_sampler(jnp.reshape(obj, (n, 20, 20, 1)), x_s, y_s)\n",
    "    return jnp.reshape(out, (50, 50))\n",
    "\n",
    "\n",
    "def z_where_inv(z_where):\n",
    "    # Take a batch of z_where vectors, and compute their \"inverse\".\n",
    "    # That is, for each row compute:\n",
    "    # [s,x,y] -> [1/s,-x/s,-y/s]\n",
    "    # These are the parameters required to perform the inverse of the\n",
    "    # spatial transform performed in the generative model.\n",
    "    n = 1\n",
    "    out = jnp.array([1, *(-z_where[1:])])\n",
    "    out = out / z_where[0]\n",
    "    return out\n",
    "\n",
    "\n",
    "def image_to_object(z_where, image):\n",
    "    n = 1\n",
    "    theta_inv = expand_z_where(z_where_inv(z_where))\n",
    "    grid = affine_grid_generator(20, 20, theta_inv)\n",
    "    x_s = grid[:, 0, :, :]\n",
    "    y_s = grid[:, 1, :, :]\n",
    "    out = bilinear_sampler(jnp.reshape(image, (n, 50, 50, 1)), x_s, y_s)\n",
    "    return jnp.reshape(out, (400,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179494f-ccc0-449f-8fcc-7bb48b4783e9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60196b16-596f-4b7d-b040-85b8a2808423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# Model #\n",
    "#########\n",
    "\n",
    "# Fixed constants.\n",
    "z_where_prior_loc = jnp.array([3.0, 0.0, 0.0])\n",
    "z_where_prior_scale = jnp.array([0.2, 1.0, 1.0])\n",
    "z_what_prior_loc = jnp.zeros(50, dtype=float)\n",
    "z_what_prior_scale = jnp.ones(50, dtype=float)\n",
    "z_pres_prior = -4.59512\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "@typecheck\n",
    "def step(\n",
    "    t: Int,\n",
    "    decoder: Decoder,\n",
    "    prev_x: FloatArray,\n",
    "    prev_z_pres: IntArray,\n",
    "):\n",
    "    z_pres = grasp.flip_reinforce(z_pres_prior * (2 - prev_z_pres[0])) @ f\"z_pres_{t}\"\n",
    "    z_pres = jnp.array([z_pres.astype(int)])\n",
    "    z_where = (\n",
    "        grasp.mv_normal_diag_reparam(z_where_prior_loc, z_where_prior_scale)\n",
    "        @ f\"z_where_{t}\"\n",
    "    )\n",
    "    z_what = (\n",
    "        grasp.mv_normal_diag_reparam(z_what_prior_loc, z_what_prior_scale)\n",
    "        @ f\"z_what_{t}\"\n",
    "    )\n",
    "    y_att = decoder(z_what)\n",
    "    y = object_to_image(z_where, y_att)\n",
    "    x = prev_x + (y * z_pres)\n",
    "    return x, z_pres\n",
    "\n",
    "\n",
    "# TODO: Make sure that this works, where t is a static int.\n",
    "@genjax.gen\n",
    "@typecheck\n",
    "def model(decoder: Decoder):\n",
    "    x = jnp.zeros((50, 50), dtype=float)\n",
    "    z_pres = jnp.ones(1, dtype=int)\n",
    "    for t in range(3):\n",
    "        x, z_pres = step.inline(t, decoder, x, z_pres)\n",
    "    obs = grasp.mv_normal_diag_reparam(x, 0.3 * jnp.ones_like(x)) @ \"obs\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee69ad5-2136-4933-ad71-42f01483dab6",
   "metadata": {},
   "source": [
    "#### Samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41747dee-44a0-4cd6-8087-4ce4d526fcee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "\n",
       "├── \u001b[1m:z_pres_0\u001b[0m\n",
       "│   └──  bool[]\n",
       "├── \u001b[1m:z_pres_2\u001b[0m\n",
       "│   └──  bool[]\n",
       "├── \u001b[1m:obs\u001b[0m\n",
       "│   └──  f32[50,50]\n",
       "├── \u001b[1m:z_what_0\u001b[0m\n",
       "│   └──  f32[50]\n",
       "├── \u001b[1m:z_pres_1\u001b[0m\n",
       "│   └──  bool[]\n",
       "├── \u001b[1m:z_what_1\u001b[0m\n",
       "│   └──  f32[50]\n",
       "├── \u001b[1m:z_where_1\u001b[0m\n",
       "│   └──  f32[3]\n",
       "├── \u001b[1m:z_where_0\u001b[0m\n",
       "│   └──  f32[3]\n",
       "├── \u001b[1m:z_what_2\u001b[0m\n",
       "│   └──  f32[50]\n",
       "└── \u001b[1m:z_where_2\u001b[0m\n",
       "    └──  f32[3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = jax.jit(model.simulate)(key, (decoder,))\n",
    "tr.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8394159-926b-4607-8df0-bbcc49657ae0",
   "metadata": {},
   "source": [
    "### Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33dd2fbf-df45-4e48-ac82-9d005ffaab01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# Guide #\n",
    "#########\n",
    "\n",
    "\n",
    "def baseline_step(bl_rnn, bl_predict, img_arr, prev):\n",
    "    prev_bl_h, prev_bl_c, prev_z_where, prev_z_what, prev_z_pres = prev\n",
    "    rnn_input = jnp.concatenate([img_arr, prev_z_where, prev_z_what, prev_z_pres])\n",
    "    bl_h, bl_c = bl_rnn(rnn_input, (prev_bl_h, prev_bl_c))\n",
    "    bl_value = bl_predict(bl_h) * prev_z_pres[0]\n",
    "    return bl_value, bl_h, bl_c\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "@typecheck\n",
    "def guide_step(\n",
    "    t: Int,\n",
    "    rnn: eqx.nn.LSTMCell,\n",
    "    encoder: Encoder,\n",
    "    predict: Predict,\n",
    "    bl_rnn: eqx.nn.LSTMCell,\n",
    "    bl_predict: eqx.nn.Linear,\n",
    "    img_arr,\n",
    "    prev: Tuple,\n",
    "):\n",
    "    (\n",
    "        prev_bl_h,\n",
    "        prev_bl_c,\n",
    "        prev_z_where,\n",
    "        prev_z_what,\n",
    "        prev_z_pres,\n",
    "        prev_h,\n",
    "        prev_c,\n",
    "    ) = prev\n",
    "\n",
    "    # Baseline step.\n",
    "    bl_value, bl_h, bl_c = baseline_step(\n",
    "        bl_rnn,\n",
    "        bl_predict,\n",
    "        img_arr,\n",
    "        (prev_bl_h, prev_bl_c, prev_z_where, prev_z_what, prev_z_pres),\n",
    "    )\n",
    "    rnn_input = jnp.concatenate([img_arr, prev_z_where, prev_z_what, prev_z_pres])\n",
    "    h, c = rnn(rnn_input, (prev_h, prev_c))\n",
    "    z_pres_p, z_where_loc, z_where_scale = predict(h)\n",
    "\n",
    "    # Baseline applied here.\n",
    "    # z_pres = (\n",
    "    #    grasp.baseline(grasp.flip_reinforce)(\n",
    "    #        bl_value[0], z_pres_p[0] * (2 - prev_z_pres[0])\n",
    "    #    )\n",
    "    #    @ f\"z_pres_{t}\"\n",
    "    # )\n",
    "    z_pres = grasp.flip_enum(0.01) @ f\"z_pres_{t}\"\n",
    "    # z_pres = grasp.flip_reinforce(z_pres_p[0] * (2 - prev_z_pres[0])) @ f\"z_pres_{t}\"\n",
    "\n",
    "    z_pres = jnp.array([z_pres.astype(int)])\n",
    "    z_where = grasp.mv_normal_diag_reparam(z_where_loc, z_where_scale) @ f\"z_where_{t}\"\n",
    "    x_att = image_to_object(z_where, img_arr)\n",
    "    z_what_loc, z_what_scale = encoder(x_att)\n",
    "    z_what = grasp.mv_normal_diag_reparam(z_what_loc, z_what_scale) @ f\"z_what_{t}\"\n",
    "    return bl_h, bl_c, z_where, z_what, z_pres, h, c, bl_value\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "@typecheck\n",
    "def guide(\n",
    "    data: genjax.ChoiceMap,\n",
    "    rnn: eqx.nn.LSTMCell,\n",
    "    encoder: Encoder,\n",
    "    predict: Predict,\n",
    "    bl_rnn: eqx.nn.LSTMCell,\n",
    "    bl_predict: eqx.nn.Linear,\n",
    "):\n",
    "    bl_h = jnp.zeros(256)\n",
    "    bl_c = jnp.zeros(256)\n",
    "    z_where = jnp.zeros(3)\n",
    "    z_what = jnp.zeros(50)\n",
    "    z_pres = jnp.ones(1)\n",
    "    h = jnp.zeros(256)\n",
    "    c = jnp.zeros(256)\n",
    "    img = data[\"obs\"]\n",
    "    img_arr = img.flatten()\n",
    "    bl_values = []\n",
    "\n",
    "    for t in range(3):\n",
    "        (bl_h, bl_c, z_where, z_what, z_pres, h, c, bl_value) = guide_step.inline(\n",
    "            t,\n",
    "            rnn,\n",
    "            encoder,\n",
    "            predict,\n",
    "            bl_rnn,\n",
    "            bl_predict,\n",
    "            img_arr,\n",
    "            (bl_h, bl_c, z_where, z_what, z_pres, h, c),\n",
    "        )\n",
    "        bl_values.append(bl_value)\n",
    "\n",
    "    return bl_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89bc83-29fc-4e0e-a47d-dab2acf0c920",
   "metadata": {},
   "source": [
    "#### Samples from the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2984c5cc-2b4c-40f5-ad0c-a2c3d1508f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "\n",
       "├── \u001b[1m:z_pres_0\u001b[0m\n",
       "│   └──  bool[]\n",
       "├── \u001b[1m:z_pres_2\u001b[0m\n",
       "│   └──  bool[]\n",
       "├── \u001b[1m:z_what_0\u001b[0m\n",
       "│   └──  f32[50]\n",
       "├── \u001b[1m:z_pres_1\u001b[0m\n",
       "│   └──  bool[]\n",
       "├── \u001b[1m:z_what_1\u001b[0m\n",
       "│   └──  f32[50]\n",
       "├── \u001b[1m:z_where_1\u001b[0m\n",
       "│   └──  f32[3]\n",
       "├── \u001b[1m:z_where_0\u001b[0m\n",
       "│   └──  f32[3]\n",
       "├── \u001b[1m:z_what_2\u001b[0m\n",
       "│   └──  f32[50]\n",
       "└── \u001b[1m:z_where_2\u001b[0m\n",
       "    └──  f32[3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chm = genjax.choice_map({\"obs\": jnp.ones((50, 50))})\n",
    "tr = jax.jit(guide.simulate)(key, (data_chm, rnn, encoder, predict, bl_rnn, bl_predict))\n",
    "tr.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b2f0a-dee4-470d-8bfa-c71a9e466690",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac32a7cd-d44b-46dd-bea2-8ddac55aa88a",
   "metadata": {},
   "source": [
    "### Make sure grads are working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc1805-57c9-4c44-8ace-2b0fa068c9c7",
   "metadata": {},
   "source": [
    "#### Define ELBO objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435472dd-c312-4f5f-8fc4-f2c0125491ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = genjax.choice_map({\"obs\": jnp.ones((50, 50))})\n",
    "objective = grasp.elbo(model, guide, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fc0c2-6adb-4d7d-8ab1-0c1fccc16a65",
   "metadata": {},
   "source": [
    "#### Go go grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1ec3a5-e506-41bb-9a97-0c13bcbfb108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jitted = jax.jit(objective.value_and_grad_estimate)\n",
    "loss, ((decoder_grads,), (_, rnn_grads, encoder_grads, predict_grads, _, _)) = jitted(\n",
    "    key, ((decoder,), (data, rnn, encoder, predict, bl_rnn, bl_predict))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817c347c-fdff-4822-9091-2d58887f5baa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mArray\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m-13435.164\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mfloat32\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5b4af-8df8-4bca-b0f2-c576dd0ba659",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf9c5ed-4369-4a68-8ce8-af7754576fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_loader(\n",
    "    data,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "):\n",
    "    N = len(data)\n",
    "    data_idxs = np.arange(N)\n",
    "    num_batch = int(np.ceil(N // batch_size))\n",
    "\n",
    "    def init():\n",
    "        return (\n",
    "            num_batch,\n",
    "            np.random.permutation(data_idxs) if shuffle else data_idxs,\n",
    "        )\n",
    "\n",
    "    def get_batch(i=0, idxs=data_idxs):\n",
    "        ret_idx = jax.lax.dynamic_slice_in_dim(idxs, i * batch_size, batch_size)\n",
    "        return jax.lax.index_take(data, (ret_idx,), axes=(0,))\n",
    "\n",
    "    return init, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bce54cb-d22a-43b8-b296-6067484552d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Count Accuracy #\n",
    "##################\n",
    "\n",
    "\n",
    "def count_accuracy(data, true_counts, guide, batch_size=1000):\n",
    "    global prng_key\n",
    "    assert data.shape[0] == true_counts.shape[0], \"Size mismatch.\"\n",
    "    assert data.shape[0] % batch_size == 0, \"Input size must be multiple of batch_size.\"\n",
    "\n",
    "    def eval_guide(key, data, params):\n",
    "        data_chmp = genjax.choice_map({\"obs\": data})\n",
    "        return guide.simulate(key, (data_chmp, *params))\n",
    "\n",
    "    batch_eval_guide = jax.jit(jax.vmap(eval_guide, in_axes=(0, 0, None)))\n",
    "\n",
    "    @jax.jit\n",
    "    def evaluate_count_accuracy(key, params):\n",
    "        def evaluate_batch(counts, batch_id):\n",
    "            data_batch = jax.lax.dynamic_slice_in_dim(\n",
    "                data, batch_id * batch_size, batch_size\n",
    "            )\n",
    "            true_counts_batch = jax.lax.dynamic_slice_in_dim(\n",
    "                true_counts, batch_id * batch_size, batch_size\n",
    "            )\n",
    "            data_chmp = genjax.choice_map({\"obs\": data_batch})\n",
    "            # evaluate guide\n",
    "            keys = jax.random.split(jax.random.fold_in(key, batch_id), batch_size)\n",
    "            tr = batch_eval_guide(keys, data_batch, params)\n",
    "            z_where = [tr[f\"z_where_{i}\"] for i in range(3)]\n",
    "            z_pres = [tr[f\"z_pres_{i}\"] for i in range(3)]\n",
    "            # compute stats\n",
    "            inferred_counts = sum(z for z in z_pres)\n",
    "            true_counts_m = jax.nn.one_hot(true_counts_batch, 3)\n",
    "            inferred_counts_m = jax.nn.one_hot(inferred_counts, 4)\n",
    "            counts += (true_counts_m.T @ inferred_counts_m).astype(int)\n",
    "            error_ind = 1 - (true_counts_batch == inferred_counts).astype(int)\n",
    "            # error_ix = error_ind.nonzero()[0]\n",
    "            # error_latent = jnp.take(latents_to_tensor((z_where, z_pres)), error_ix, 0)\n",
    "            return counts, error_ind\n",
    "\n",
    "        counts = jnp.zeros((3, 4), dtype=int)\n",
    "        counts, error_indices = jax.lax.scan(\n",
    "            evaluate_batch, counts, jnp.arange(data.shape[0] // batch_size)\n",
    "        )\n",
    "\n",
    "        acc = jnp.sum(jnp.diag(counts)).astype(float) / data.shape[0]\n",
    "        error_indices = jnp.concatenate(\n",
    "            error_indices\n",
    "        )  # .nonzero()[0]  # <- not JIT compilable\n",
    "        return acc, counts, error_indices\n",
    "\n",
    "    return evaluate_count_accuracy\n",
    "\n",
    "\n",
    "# Combine z_pres and z_where (as returned by the model and guide) into\n",
    "# a single tensor, with size:\n",
    "# [batch_size, num_steps, z_where_size + z_pres_size]\n",
    "def latents_to_tensor(z):\n",
    "    return jnp.stack(\n",
    "        [\n",
    "            jnp.concatenate((z_where, z_pres.reshape(-1, 1)), 1)\n",
    "            for z_where, z_pres in zip(*z)\n",
    "        ]\n",
    "    ).transpose(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3660daf7-584d-4fd9-a26b-371d32fb5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Visualization  #\n",
    "##################\n",
    "\n",
    "\n",
    "def bounding_box(z_where, x_size):\n",
    "    \"\"\"This doesn't take into account interpolation, but it's close\n",
    "    enough to be usable.\"\"\"\n",
    "    w = x_size / z_where.s\n",
    "    h = x_size / z_where.s\n",
    "    xtrans = -z_where.x / z_where.s * x_size / 2.0\n",
    "    ytrans = -z_where.y / z_where.s * x_size / 2.0\n",
    "    x = (x_size - w) / 2 + xtrans  # origin is top left\n",
    "    y = (x_size - h) / 2 + ytrans\n",
    "    return (x, y), w, h\n",
    "\n",
    "\n",
    "z_obj = namedtuple(\"z\", [\"s\", \"x\", \"y\", \"pres\"])\n",
    "\n",
    "\n",
    "# Map a tensor of latents (as produced by latents_to_tensor) to a list\n",
    "# of z_obj named tuples.\n",
    "def tensor_to_objs(latents):\n",
    "    return [[z_obj._make(step) for step in z] for z in latents]\n",
    "\n",
    "\n",
    "def visualize_model(model, guide):\n",
    "    def reconstruct_digits(key, data, params):\n",
    "        decoder, rnn, encoder, predict, bl_rnn, bl_predict = params\n",
    "        data_chmp = genjax.choice_map({\"obs\": data})\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        tr = guide.simulate(\n",
    "            key1, (data_chmp, rnn, encoder, predict, bl_rnn, bl_predict)\n",
    "        )\n",
    "        _, tr = model.importance(key2, tr, (decoder,))\n",
    "        reconstruction = tr.get_retval()\n",
    "        z_where = [tr[f\"z_where_{i}\"] for i in range(3)]\n",
    "        z_pres = [tr[f\"z_pres_{i}\"] for i in range(3)]\n",
    "        return reconstruction, (z_where, z_pres)\n",
    "\n",
    "    batch_reconstruct_digits = jax.jit(\n",
    "        jax.vmap(reconstruct_digits, in_axes=(0, 0, None))\n",
    "    )\n",
    "\n",
    "    def visualize(key, params, examples_to_viz):\n",
    "        keys = jax.random.split(key, examples_to_viz.shape[0])\n",
    "        recons, z = batch_reconstruct_digits(keys, examples_to_viz, params)\n",
    "        z_wheres = tensor_to_objs(latents_to_tensor(z))\n",
    "        draw_many(examples_to_viz.reshape(-1, 50, 50), z_wheres, title=\"Original\")\n",
    "        draw_many(recons, z_wheres, title=\"Reconstruction\")\n",
    "\n",
    "    return visualize\n",
    "\n",
    "\n",
    "def colors(k):\n",
    "    return [\"r\", \"g\", \"b\"][k % 3]\n",
    "\n",
    "\n",
    "def draw_one(img, z):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=\"gray_r\")\n",
    "    for k, z in enumerate(z):\n",
    "        if z.pres > 0:\n",
    "            (x, y), w, h = bounding_box(z, img.shape[0])\n",
    "            plt.gca().add_patch(\n",
    "                Rectangle(\n",
    "                    (x, y), w, h, linewidth=1, edgecolor=colors(k), facecolor=\"none\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def draw_many(imgs, zs, title):\n",
    "    plt.figure(figsize=(8, 1.9))\n",
    "    plt.title(title)\n",
    "    plt.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
    "    plt.box(False)\n",
    "    for i, (img, z) in enumerate(zip(imgs, zs)):\n",
    "        plt.subplot(1, len(imgs), i + 1)\n",
    "        draw_one(img, z)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da48a46a-fdb1-4b8d-b4e5-d96d27b898d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (decoder, rnn, encoder, predict)\n",
    "evaluate_accuracy = count_accuracy(mnist, true_counts, guide, batch_size=1000)\n",
    "\n",
    "visualize_examples = mnist[5:10]\n",
    "visualize = visualize_model(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf743cf-aa9e-44db-9078-6a89f1d26f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1.0e-4\n",
    "baseline_learning_rate = 1.0e-2\n",
    "num_epochs = 200\n",
    "\n",
    "\n",
    "def svi_update(model, guide, guide_optimizer, baseline_optimizer):\n",
    "    def batch_updater(\n",
    "        key, params, baseline_params, guide_opt_state, baseline_opt_state, data_batch\n",
    "    ):\n",
    "\n",
    "        # First, ELBO grads.\n",
    "        def elbo_grads(key, params, baseline_params, data):\n",
    "            (decoder, rnn, encoder, predict) = params\n",
    "            (bl_rnn, bl_predict) = baseline_params\n",
    "            data = genjax.choice_map({\"obs\": data})\n",
    "\n",
    "            @genjax.gen\n",
    "            def inlined_guide(\n",
    "                data: genjax.ChoiceMap,\n",
    "                rnn: eqx.nn.LSTMCell,\n",
    "                encoder: Encoder,\n",
    "                predict: Predict,\n",
    "            ):\n",
    "                # Inline the guide into a new generative function.\n",
    "                return guide.inline(data, rnn, encoder, predict, bl_rnn, bl_predict)\n",
    "\n",
    "            objective = grasp.elbo(model, inlined_guide, data)\n",
    "\n",
    "            loss, (\n",
    "                (decoder_grads,),\n",
    "                (_, rnn_grads, encoder_grads, predict_grads),\n",
    "            ) = objective.value_and_grad_estimate(\n",
    "                key, ((decoder,), (data, rnn, encoder, predict))\n",
    "            )\n",
    "            return loss, (decoder_grads, rnn_grads, encoder_grads, predict_grads)\n",
    "\n",
    "        key, sub_key = jax.random.split(key)\n",
    "        sub_keys = jax.random.split(sub_key, len(data_batch))\n",
    "\n",
    "        # First, ELBO grads.\n",
    "        loss, guide_grads = jax.vmap(elbo_grads, in_axes=(0, None, None, 0))(\n",
    "            sub_keys, params, baseline_params, data_batch\n",
    "        )\n",
    "\n",
    "        guide_grads = jtu.tree_map(\n",
    "            lambda v: -1.0 * jnp.mean(v, axis=0),\n",
    "            guide_grads,\n",
    "        )\n",
    "        guide_updates, opt_state = guide_optimizer.update(\n",
    "            guide_grads, guide_opt_state, params\n",
    "        )\n",
    "        params = optax.apply_updates(params, guide_updates)\n",
    "\n",
    "        # Now, baseline grads.\n",
    "        def baseline_grads(key, params, baseline_params, data):\n",
    "            (decoder, rnn, encoder, predict) = params\n",
    "            (bl_rnn, bl_predict) = baseline_params\n",
    "            data = genjax.choice_map({\"obs\": data})\n",
    "\n",
    "            @adevjax.adev\n",
    "            def baseline_loss(bl_rnn, bl_predict):\n",
    "                key = adevjax.reap_key()\n",
    "                tr = guide.simulate(\n",
    "                    key, (data, rnn, encoder, predict, bl_rnn, bl_predict)\n",
    "                )\n",
    "                qw = tr.get_score()\n",
    "                chm = data.safe_merge(tr.strip())\n",
    "                key = adevjax.reap_key()\n",
    "                (_, pw) = model.assess(key, chm, (decoder,))\n",
    "                Z = pw - qw\n",
    "                bvs = jnp.array(tr.get_retval())\n",
    "                return jnp.sum((bvs - Z) ** 2)\n",
    "\n",
    "            loss, baseline_params_grad = adevjax.E(\n",
    "                baseline_loss\n",
    "            ).value_and_grad_estimate(\n",
    "                key,\n",
    "                baseline_params,\n",
    "            )\n",
    "            return loss, baseline_params_grad\n",
    "\n",
    "        # Now, baseline grads.\n",
    "        sub_keys = jax.random.split(key, len(data_batch))\n",
    "        _, baseline_params_grads = jax.vmap(baseline_grads, in_axes=(0, None, None, 0))(\n",
    "            sub_keys, params, baseline_params, data_batch\n",
    "        )\n",
    "\n",
    "        baseline_params_grads = jtu.tree_map(\n",
    "            lambda v: jnp.mean(v, axis=0),\n",
    "            baseline_params_grads,\n",
    "        )\n",
    "        baseline_updates, baseline_opt_state = baseline_optimizer.update(\n",
    "            baseline_params_grads, baseline_opt_state, baseline_params\n",
    "        )\n",
    "        baseline_params = optax.apply_updates(baseline_params, baseline_updates)\n",
    "\n",
    "        # From ELBO loss.\n",
    "        loss = jnp.mean(loss)\n",
    "\n",
    "        return params, baseline_params, guide_opt_state, baseline_opt_state, loss\n",
    "\n",
    "    return batch_updater\n",
    "\n",
    "\n",
    "guide_adam = optax.adam(learning_rate)\n",
    "baseline_adam = optax.adam(baseline_learning_rate)\n",
    "svi_updater = svi_update(model, guide, guide_adam, baseline_adam)\n",
    "train_init, train_fetch = data_loader(jnp.array(mnist), batch_size)\n",
    "num_batch, train_idx = train_init()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def epoch_train(\n",
    "    guide_opt_state, baseline_opt_state, params, baseline_params, key, train_idx\n",
    "):\n",
    "    def body_fn(carry, xs):\n",
    "        idx, params, baseline_params, guide_opt_state, baseline_opt_state, loss = carry\n",
    "        updater_key = jax.random.fold_in(key, idx)\n",
    "        batch = train_fetch(idx, train_idx)\n",
    "        (\n",
    "            params,\n",
    "            baseline_params,\n",
    "            guide_opt_state,\n",
    "            baseline_opt_state,\n",
    "            loss,\n",
    "        ) = svi_updater(\n",
    "            updater_key,\n",
    "            params,\n",
    "            baseline_params,\n",
    "            guide_opt_state,\n",
    "            baseline_opt_state,\n",
    "            batch,\n",
    "        )\n",
    "        idx += 1\n",
    "        return (\n",
    "            idx,\n",
    "            params,\n",
    "            baseline_params,\n",
    "            guide_opt_state,\n",
    "            baseline_opt_state,\n",
    "            loss,\n",
    "        ), loss\n",
    "\n",
    "    idx = 0\n",
    "    (\n",
    "        _,\n",
    "        params,\n",
    "        baseline_params,\n",
    "        guide_opt_state,\n",
    "        baseline_opt_state,\n",
    "        _,\n",
    "    ), losses = jax.lax.scan(\n",
    "        body_fn,\n",
    "        (idx, params, baseline_params, guide_opt_state, baseline_opt_state, 0.0),\n",
    "        None,\n",
    "        length=num_batch,\n",
    "    )\n",
    "    return params, baseline_params, guide_opt_state, baseline_opt_state, losses\n",
    "\n",
    "\n",
    "# Train.\n",
    "key = jax.random.PRNGKey(314159)\n",
    "params = (decoder, rnn, encoder, predict)\n",
    "baseline_params = (bl_rnn, bl_predict)\n",
    "guide_opt_state = guide_adam.init(params)\n",
    "baseline_opt_state = baseline_adam.init(baseline_params)\n",
    "losses = []\n",
    "accuracy = []\n",
    "\n",
    "# Warm up.\n",
    "_ = epoch_train(\n",
    "    guide_opt_state, baseline_opt_state, params, baseline_params, key, train_idx\n",
    ")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "acc_time = 0.0\n",
    "for i in range(0, num_epochs):\n",
    "    key, sub_key = jax.random.split(key)\n",
    "    num_batch, train_idx = train_init()\n",
    "    start = time.perf_counter() - t0\n",
    "    params, baseline_params, guide_opt_state, baseline_opt_state, loss = epoch_train(\n",
    "        guide_opt_state, baseline_opt_state, params, baseline_params, sub_key, train_idx\n",
    "    )\n",
    "    stop = time.perf_counter() - t0\n",
    "    acc_time += stop - start\n",
    "    print(f\"Epoch={i}, total_epoch_step_time={acc_time:.2f}, loss={jnp.mean(loss):.2f}\")\n",
    "    if i % 20 == 0:\n",
    "        acc, counts, error_ix = evaluate_accuracy(\n",
    "            sub_key, (*params[1:], *baseline_params)\n",
    "        )\n",
    "        accuracy.append(acc)\n",
    "        print(\"accuracy={}, counts={}\".format(acc, counts))\n",
    "    if i % 20 == 0:\n",
    "        visualize(sub_key, (*params, *baseline_params), visualize_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99639ced-cee6-4075-af37-20167d5d7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.linspace(0, num_epochs, len(losses)), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.linspace(0, num_epochs, len(accuracy)), accuracy)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
