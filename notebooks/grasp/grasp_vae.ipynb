{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9aa208d-522f-48eb-8ac1-514670e04f2a",
   "metadata": {},
   "source": [
    "## Normal VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92511a8b-aa9f-45de-88c2-b9a6f49cf370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-187.93683 -144.51834 -547.5518\n",
      "-139.85727 -118.564735 -169.98514\n",
      "-123.77806 -109.60018 -139.81021\n",
      "-118.91979 -106.06482 -132.41243\n",
      "-116.35184 -104.01479 -131.79811\n",
      "-114.69463 -102.30388 -130.27652\n",
      "-113.44112 -98.60271 -127.18927\n",
      "-112.491615 -99.09976 -125.307045\n",
      "-111.76993 -100.36415 -129.1171\n",
      "-111.16662 -98.11888 -124.34346\n",
      "-110.5944 -98.047516 -124.48819\n",
      "-110.06027 -96.33754 -124.5229\n",
      "-109.5917 -95.3978 -124.2038\n",
      "-109.114815 -95.299706 -121.260666\n",
      "-108.67189 -94.92757 -122.95674\n",
      "-108.33361 -96.40257 -119.856186\n",
      "-107.9306 -96.775154 -119.23195\n",
      "-107.578476 -96.47286 -119.86563\n",
      "-107.33212 -93.09282 -120.17706\n",
      "-107.03577 -94.34917 -118.64399\n",
      "-106.69416 -94.38687 -123.18965\n",
      "-106.41662 -93.07442 -119.54758\n",
      "-106.21359 -92.49185 -118.49284\n",
      "-105.96578 -93.572914 -120.898605\n",
      "-105.680405 -91.41397 -118.411606\n",
      "-105.53937 -91.37651 -118.02678\n",
      "-105.31824 -92.12115 -120.74809\n",
      "-105.07397 -92.26828 -118.74666\n",
      "-104.904274 -95.018585 -117.779755\n",
      "-104.77623 -93.3788 -115.33646\n",
      "-104.638824 -91.705025 -116.20999\n",
      "-104.44725 -93.903564 -119.29326\n",
      "-104.296074 -93.34227 -117.364815\n",
      "-104.15099 -92.30052 -115.281525\n",
      "-104.02975 -93.00334 -115.11577\n",
      "-103.946754 -93.25758 -117.08095\n",
      "-103.78287 -92.44561 -118.09549\n",
      "-103.75163 -92.16219 -114.851166\n",
      "-103.60894 -91.871216 -117.45451\n",
      "-103.58537 -89.171165 -116.415146\n",
      "-103.40181 -92.08441 -115.41337\n",
      "-103.284004 -88.22171 -115.64754\n",
      "-103.228836 -92.24033 -115.26735\n",
      "-103.16113 -91.84203 -116.67559\n",
      "-103.08649 -90.73426 -114.4061\n",
      "-103.00305 -88.94258 -116.88591\n",
      "-102.83975 -89.76305 -115.20885\n",
      "-102.84682 -91.09502 -114.801895\n",
      "-102.83094 -90.76488 -116.46367\n",
      "-102.71206 -89.87186 -115.28256\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import jit\n",
    "from jax import lax\n",
    "from jax import random\n",
    "from jax.example_libraries import stax\n",
    "from jax.random import PRNGKey\n",
    "from numpyro import optim\n",
    "from numpyro.examples.datasets import MNIST\n",
    "from numpyro.examples.datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "\n",
    "import genjax\n",
    "from genjax import nablasp\n",
    "from genjax import gensp\n",
    "from genjax import select\n",
    "\n",
    "import adevjax\n",
    "\n",
    "\n",
    "RESULTS_DIR = os.path.abspath(\n",
    "    os.path.join(os.path.dirname(inspect.getfile(lambda: None)), \".results\")\n",
    ")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "hidden_dim = 400\n",
    "z_dim = 10\n",
    "learning_rate = 1.0e-3\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "def encoder(hidden_dim, z_dim):\n",
    "    return stax.serial(\n",
    "        stax.Dense(hidden_dim, W_init=stax.randn()),\n",
    "        stax.Softplus,\n",
    "        stax.FanOut(2),\n",
    "        stax.parallel(\n",
    "            stax.Dense(z_dim, W_init=stax.randn()),\n",
    "            stax.serial(stax.Dense(z_dim, W_init=stax.randn()), stax.Exp),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(hidden_dim, out_dim):\n",
    "    return stax.serial(\n",
    "        stax.Dense(hidden_dim, W_init=stax.randn()),\n",
    "        stax.Softplus,\n",
    "        stax.Dense(out_dim, W_init=stax.randn()),\n",
    "    )\n",
    "\n",
    "\n",
    "tfp_flip = genjax.TFPDistribution(\n",
    "    lambda logits: tfp.distributions.Bernoulli(logits=logits)\n",
    ")\n",
    "\n",
    "# Define our gradient estimator using our loss language.\n",
    "def svi_update(\n",
    "    model,\n",
    "    guide,\n",
    "    optimizer,\n",
    "):\n",
    "    def _inner(key, encoder_params, decoder_params, data):\n",
    "        img_chm = genjax.value_choice_map(\n",
    "            genjax.choice_map({\"image\": data.reshape((28 * 28,))})\n",
    "        )\n",
    "\n",
    "        @adevjax.adev\n",
    "        def elbo_loss(encoder_params, decoder_params):\n",
    "            key = adevjax.grab_key()\n",
    "            w_q, latent_chm = guide.random_weighted(key, encoder_params, img_chm)\n",
    "            merged = gensp.merge(latent_chm, img_chm)\n",
    "            key = adevjax.grab_key()\n",
    "            w_p = model.estimate_logpdf(key, merged, decoder_params)\n",
    "            return w_p - w_q\n",
    "\n",
    "        # @dippl.loss\n",
    "        # def vae_loss(encoder, decoder):\n",
    "        #    tgt = gensp.target(model, (decoder, ), img_chm)\n",
    "        #    key = adevjax.grab_key()\n",
    "        #    w = dippl.iwae_importance(10, guide, (encoder, v_chm)).estimate_normalizing_constant(key, tgt)\n",
    "        #    return w\n",
    "\n",
    "        loss, (encoder_params_grad, decoder_params_grad,) = adevjax.E(\n",
    "            elbo_loss\n",
    "        ).value_and_grad_estimate(key, (encoder_params, decoder_params))\n",
    "\n",
    "        return (encoder_params_grad, decoder_params_grad), loss\n",
    "\n",
    "    def batch_update(key, svi_state, batch):\n",
    "        encoder_params, decoder_params = optimizer.get_params(svi_state)\n",
    "        sub_keys = jax.random.split(key, len(batch))\n",
    "        (encoder_grads, decoder_grads), loss = jax.vmap(\n",
    "            _inner, in_axes=(0, None, None, 0)\n",
    "        )(sub_keys, encoder_params, decoder_params, batch)\n",
    "        encoder_grads, decoder_grads = jtu.tree_map(\n",
    "            lambda v: -1.0 * jnp.mean(v, axis=0), (encoder_grads, decoder_grads)\n",
    "        )\n",
    "        svi_state = optimizer.update(\n",
    "            (encoder_grads, decoder_grads),\n",
    "            svi_state,  # just (encoder_params, decoder_params)\n",
    "        )\n",
    "        return svi_state, jnp.mean(loss)\n",
    "\n",
    "    return batch_update\n",
    "\n",
    "\n",
    "@jit\n",
    "def binarize(rng_key, batch):\n",
    "    return random.bernoulli(rng_key, batch).astype(batch.dtype)\n",
    "\n",
    "\n",
    "encoder_nn_init, encoder_nn_apply = encoder(hidden_dim, z_dim)\n",
    "decoder_nn_init, decoder_nn_apply = decoder(hidden_dim, 28 * 28)\n",
    "\n",
    "# Model + guide close over the neural net apply functions.\n",
    "@genjax.gen\n",
    "def decoder_model(decoder_params):\n",
    "    latent = genjax.tfp_mv_normal_diag(jnp.zeros(z_dim), jnp.ones(z_dim)) @ \"latent\"\n",
    "    logits = decoder_nn_apply(decoder_params, latent)\n",
    "    _ = genjax.tfp_bernoulli(logits) @ \"image\"\n",
    "\n",
    "\n",
    "model = gensp.choice_map_distribution(decoder_model)\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def encoder_model(encoder_params, chm):\n",
    "    image = chm.get_leaf_value()[\"image\"]\n",
    "    μ, Σ_scale = encoder_nn_apply(encoder_params, image)\n",
    "    _ = nablasp.mv_normal_diag_reparam(μ, Σ_scale) @ \"latent\"\n",
    "\n",
    "\n",
    "guide = gensp.choice_map_distribution(encoder_model)\n",
    "\n",
    "adam = optim.Adam(learning_rate)\n",
    "svi_updater = svi_update(model, guide, adam)\n",
    "train_init, train_fetch = load_dataset(MNIST, batch_size=batch_size, split=\"train\")\n",
    "num_train, train_idx = train_init()\n",
    "rng_key = PRNGKey(0)\n",
    "encoder_init_key, decoder_init_key = random.split(rng_key)\n",
    "_, encoder_params = encoder_nn_init(encoder_init_key, (784,))\n",
    "_, decoder_params = decoder_nn_init(decoder_init_key, (z_dim,))\n",
    "num_train, train_idx = train_init()\n",
    "\n",
    "\n",
    "@jit\n",
    "def epoch_train(svi_state, key1, key2, train_idx):\n",
    "    def body_fn(carry, xs):\n",
    "        idx, svi_state, loss = carry\n",
    "        rng_key_binarize = random.fold_in(key1, idx)\n",
    "        batch = binarize(rng_key_binarize, train_fetch(idx, train_idx)[0])\n",
    "        updater_key = random.fold_in(key2, idx)\n",
    "        svi_state, loss = svi_updater(updater_key, svi_state, batch)\n",
    "        idx += 1\n",
    "        return (idx, svi_state, loss), loss\n",
    "\n",
    "    idx = 0\n",
    "    (_, svi_state, _), losses = lax.scan(\n",
    "        body_fn, (idx, svi_state, 0.0), None, length=num_train\n",
    "    )\n",
    "    return svi_state, losses\n",
    "\n",
    "\n",
    "# Train.\n",
    "key = random.PRNGKey(314159)\n",
    "svi_state = adam.init((encoder_params, decoder_params))\n",
    "for _ in range(0, num_epochs):\n",
    "    key, key1, key2 = jax.random.split(key, 3)\n",
    "    num_train, train_idx = train_init()\n",
    "    svi_state, loss = epoch_train(svi_state, key1, key2, train_idx)\n",
    "    print(jnp.mean(loss), jnp.max(loss), jnp.min(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d785979-fa9f-497e-abdc-ee0720177461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8e244e4610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "_, updated_decoder_params = adam.get_params(svi_state)\n",
    "latent = genjax.tfp_mv_normal_diag.sample(key, jnp.zeros(z_dim), jnp.ones(z_dim))\n",
    "updated_out = jax.nn.sigmoid(decoder_nn_apply(updated_decoder_params, latent)).reshape(\n",
    "    28, 28\n",
    ")\n",
    "plt.imshow(updated_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d380f-75e1-44e0-b8aa-7d472d7275ff",
   "metadata": {},
   "source": [
    "## IWAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180b164-19eb-49a3-a379-38f40303180c",
   "metadata": {},
   "source": [
    "# Define our gradient estimator using our loss language.\n",
    "def iwae_svi_update(\n",
    "    model,\n",
    "    guide,\n",
    "    optimizer,\n",
    "):\n",
    "    def _inner(key, encoder_params, decoder_params, data):\n",
    "        img_chm = genjax.choice_map({\"image\": data.reshape((28 * 28,))})\n",
    "        v_chm = genjax.value_choice_map(img_chm)\n",
    "\n",
    "        @adevjax.adev\n",
    "        def iwae_elbo_loss(encoder_params, decoder_params):\n",
    "            tgt = gensp.target(model, (decoder_params,), img_chm)\n",
    "            key = adevjax.grab_key()\n",
    "            w = nablasp.iwae_importance(\n",
    "                10, guide, (encoder_params, v_chm)\n",
    "            ).estimate_normalizing_constant(key, tgt)\n",
    "            return w\n",
    "\n",
    "        loss, (encoder_params_grad, decoder_params_grad) = adevjax.E(\n",
    "            iwae_elbo_loss\n",
    "        ).value_and_grad_estimate(key, (encoder_params, decoder_params))\n",
    "\n",
    "        return (encoder_params_grad, decoder_params_grad), loss\n",
    "\n",
    "    def batch_update(key, svi_state, batch):\n",
    "        encoder_params, decoder_params = optimizer.get_params(svi_state)\n",
    "        sub_keys = jax.random.split(key, len(batch))\n",
    "        (encoder_grads, decoder_grads), loss = jax.vmap(\n",
    "            _inner, in_axes=(0, None, None, 0)\n",
    "        )(sub_keys, encoder_params, decoder_params, batch)\n",
    "        encoder_grads, decoder_grads = jtu.tree_map(\n",
    "            lambda v: -1.0 * jnp.mean(v, axis=0), (encoder_grads, decoder_grads)\n",
    "        )\n",
    "        svi_state = optimizer.update(\n",
    "            (encoder_grads, decoder_grads),\n",
    "            svi_state,  # just (encoder_params, decoder_params)\n",
    "        )\n",
    "        return svi_state, jnp.mean(loss)\n",
    "\n",
    "    return batch_update\n",
    "\n",
    "\n",
    "adam = optim.Adam(learning_rate)\n",
    "iwae_svi_updater = iwae_svi_update(decoder_model, guide, adam)\n",
    "\n",
    "# Reset training.\n",
    "train_init, train_fetch = load_dataset(MNIST, batch_size=batch_size, split=\"train\")\n",
    "num_train, train_idx = train_init()\n",
    "rng_key = PRNGKey(0)\n",
    "encoder_init_key, decoder_init_key = random.split(rng_key)\n",
    "_, encoder_params = encoder_nn_init(encoder_init_key, (784,))\n",
    "_, decoder_params = decoder_nn_init(decoder_init_key, (z_dim,))\n",
    "num_train, train_idx = train_init()\n",
    "\n",
    "\n",
    "@jit\n",
    "def epoch_train(svi_state, key1, key2, train_idx):\n",
    "    def body_fn(carry, xs):\n",
    "        idx, svi_state, loss = carry\n",
    "        rng_key_binarize = random.fold_in(key1, idx)\n",
    "        batch = binarize(rng_key_binarize, train_fetch(idx, train_idx)[0])\n",
    "        updater_key = random.fold_in(key2, idx)\n",
    "        svi_state, loss = iwae_svi_updater(updater_key, svi_state, batch)\n",
    "        idx += 1\n",
    "        return (idx, svi_state, loss), loss\n",
    "\n",
    "    idx = 0\n",
    "    (_, svi_state, _), losses = lax.scan(\n",
    "        body_fn, (idx, svi_state, 0.0), None, length=num_train\n",
    "    )\n",
    "    return svi_state, losses\n",
    "\n",
    "\n",
    "# Train.\n",
    "key = random.PRNGKey(314159)\n",
    "svi_state = adam.init((encoder_params, decoder_params))\n",
    "for _ in range(0, num_epochs):\n",
    "    key, key1, key2 = jax.random.split(key, 3)\n",
    "    num_train, train_idx = train_init()\n",
    "    svi_state, loss = epoch_train(svi_state, key1, key2, train_idx)\n",
    "    print(jnp.mean(loss), jnp.max(loss), jnp.min(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec57f6f-d074-4331-8640-d9c9de24d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "_, updated_decoder_params = adam.get_params(svi_state)\n",
    "latent = genjax.tfp_mv_normal_diag.sample(key, jnp.zeros(z_dim), jnp.ones(z_dim))\n",
    "updated_out = jax.nn.sigmoid(decoder_nn_apply(updated_decoder_params, latent)).reshape(\n",
    "    28, 28\n",
    ")\n",
    "plt.imshow(updated_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca9643-ae62-4aab-ad6c-ef451d0b1e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
