{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Parallel Gibbs Sampling\n",
    "subtitle: Surprising ways in which parallelism can be introduced\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import pretty\n",
    "from genjax._src.generative_functions.combinators.scan import ScanTrace\n",
    "from genjax._src.generative_functions.static import StaticTrace\n",
    "\n",
    "pretty()\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you are familiar with the Gibbs update rule. See the dedicated cookbook entry.\n",
    "\n",
    "So far, we have mostly shown how to use GenJAX to run simulations in parallel. Whether it was a generative function on several random keys, on different arguments, they all had a similar flavor of \"simply duplicating particles for inference.\"\n",
    "\n",
    "Here we will show a different kind of example where parallelism can be used for better inference which has a very different flavor: we will do a type of MCMC update to a trace where the move itself benefits from parallel acceleration thanks to the structure of the generative function to which the update is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a simple HMM and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chain = 50\n",
    "state_size = 100\n",
    "number_runs = 1000\n",
    "# for numerical stability of the HMM, ensuring that the eigenvalues of the transition matrices are around 1.\n",
    "magic_number = jnp.exp(1)\n",
    "normalizer = 1.0 / jnp.sqrt(state_size / magic_number)\n",
    "key, subkey = jax.random.split(key)\n",
    "transition_matrix = jax.random.normal(subkey, (state_size, state_size)) * normalizer\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "observation_matrix = jax.random.normal(subkey, (state_size, state_size)) * normalizer\n",
    "latent_variance = jnp.eye(state_size)\n",
    "obs_variance = jnp.eye(state_size)\n",
    "key, subkey = jax.random.split(key)\n",
    "initial_state = jax.random.normal(subkey, (state_size,))\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def initial_state_model():\n",
    "    return (\n",
    "        genjax.mv_normal(\n",
    "            jnp.zeros(state_size, dtype=float), jnp.identity(state_size, dtype=float)\n",
    "        )\n",
    "        @ \"initial_state\"\n",
    "    )\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm_step(x, _):\n",
    "    new_x = (\n",
    "        genjax.mv_normal(jnp.matmul(transition_matrix, x), latent_variance) @ \"new_x\"\n",
    "    )\n",
    "    _ = genjax.mv_normal(jnp.matmul(observation_matrix, new_x), obs_variance) @ \"obs\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm():\n",
    "    x = initial_state_model() @ \"init\"\n",
    "    _ = hmm_step.scan(n=length_chain)(x, None) @ \"steps\"\n",
    "\n",
    "\n",
    "# Testing that the model runs\n",
    "jitted = jit(hmm.repeat(n=number_runs).simulate)\n",
    "key, subkey = jax.random.split(key)\n",
    "trace = jitted(subkey, ())\n",
    "trace.get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add observervations and run the default importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = jax.vmap(\n",
    "    lambda idx: C[\"steps\", idx, \"obs\"].set(\n",
    "        idx.astype(float) * jnp.arange(state_size) / state_size\n",
    "    )\n",
    ")(jnp.arange(length_chain))\n",
    "\n",
    "\n",
    "jitted = jit(lambda key: hmm.importance(key, chm, ()))\n",
    "key, subkey = jax.random.split(key)\n",
    "jitted(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parallel_update_logic(key):\n",
    "    simpled_jitted = jit(lambda key: hmm.simulate(key, ()))\n",
    "    key, subkey = jax.random.split(key)\n",
    "    simple_tr = simpled_jitted(subkey)\n",
    "    vars_to_update = simple_tr.get_choices()[\"steps\", ..., \"new_x\"]\n",
    "\n",
    "    magic_c_matrix = jnp.matmul(transition_matrix.T, jnp.linalg.inv(latent_variance))\n",
    "\n",
    "    # single update\n",
    "    vars_to_update = vars_to_update.at[2].set(\n",
    "        jnp.matmul(magic_c_matrix, vars_to_update[2])\n",
    "    )\n",
    "\n",
    "    # multiple updates\n",
    "    parity = 0\n",
    "    idx_to_update = jnp.arange(length_chain)[jnp.arange(length_chain) % 2 == parity]\n",
    "    idx_to_update = jnp.arange(start=parity, step=2, stop=length_chain)\n",
    "    # TODO: maybe something like that. also maybe faster/simpler with vmap?\n",
    "    updating_vals = (magic_c_matrix.T @ vars_to_update[idx_to_update].T).T\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "\n",
    "    # simple test\n",
    "    vars_to_update = jnp.zeros((length_chain, state_size))\n",
    "    updating_vals = (magic_c_matrix.T @ (vars_to_update[idx_to_update] + 1).T).T\n",
    "    print(updating_vals.shape)\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "    return vars_to_update\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "test_parallel_update_logic(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_update(key, args):\n",
    "    trace, parity = args\n",
    "    vars_to_update = trace.get_choices()[\"steps\", ..., \"new_x\"]\n",
    "    # idx_to_update = jnp.arange(length_chain)[jnp.arange(length_chain) % 2 == parity]\n",
    "    idx_to_update = jnp.arange(start=1, step=2, stop=length_chain)\n",
    "\n",
    "    # TODO: actual gibbs update, and need to use conditional rule from GaussPPL\n",
    "    magic_c_matrix = jnp.matmul(transition_matrix.T, jnp.linalg.inv(latent_variance))\n",
    "    updating_vals = (magic_c_matrix.T @ vars_to_update[idx_to_update].T).T\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "\n",
    "    # TODO: need to actually return a proper trace in order to be able to iterate the gibbs update, and need to return the vars_to_update\n",
    "    updated_one = trace.inner.subtraces[1]  # TODO:\n",
    "    inner = StaticTrace(\n",
    "        trace.inner.gen_fn,\n",
    "        trace.inner.args,\n",
    "        trace.inner.retval,\n",
    "        trace.inner.addresses,\n",
    "        [trace.inner.subtraces[0], updated_one],\n",
    "        trace.inner.score,\n",
    "    )\n",
    "    return (\n",
    "        ScanTrace(trace.get_gen_fn(), inner, trace.args, trace.retval, trace.score),\n",
    "        (parity + 1) % 2,\n",
    "    ), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference using the parallel Gibbs update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "simple_tr = hmm.simulate(subkey, ())\n",
    "\n",
    "number_gibbs_sweep = 1000\n",
    "keys = jax.random.split(key, number_gibbs_sweep)\n",
    "args = (simple_tr, 0)\n",
    "jitted_gibbs = jax.jit(lambda keys: jax.lax.scan(gibbs_update, keys, args))\n",
    "jitted_gibbs(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `vmap` to launch different MCMC chains in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = jax.random.split(key, number_runs)\n",
    "initial_states = jax.vmap(lambda key: jax.random.normal(key, (state_size,)))(keys)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(key, number_runs)\n",
    "initial_traces = jax.jit(\n",
    "    jax.vmap(lambda initial_state, key: hmm.simulate(key, (initial_state, None)))\n",
    ")(initial_states, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(key, number_gibbs_sweep)\n",
    "jitted_gibbs = jax.jit(lambda tr, keys: jax.lax.scan(gibbs_update, (tr, 0), keys))\n",
    "# TODO: need to feed the traces and keys with the right format\n",
    "# traces = jitted_gibbs(initial_traces, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the quality of the inference in this case as it's a rare instance when one can do exact inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: exact inference using GaussPPL and comparison with Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time difference between the exact and approximate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
