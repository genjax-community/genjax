{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Custom proposals\n",
    "subtitle: I'm doing importance sampling as advised but it's bad, what can I do?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing one can do is write a custom proposal for importance sampling.\n",
    "The idea is to sample from this one instead of the default one used by genjax when using `model.importance`.\n",
    "The default one is only informed by the structure of the model, and not by the posterior defined by both the model and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import Target, gen, normal, pretty, smc\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a simple model with a broad normal prior and some observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def model():\n",
    "    # Initially, the prior is a pretty broad normal distribution centred at 0\n",
    "    x = normal(0.0, 100.0) @ \"x\"\n",
    "    # We add some observations, which will shift the posterior towards these values\n",
    "    _ = normal(x, 1.0) @ \"obs1\"\n",
    "    _ = normal(x, 1.0) @ \"obs2\"\n",
    "    _ = normal(x, 1.0) @ \"obs3\"\n",
    "    return x\n",
    "\n",
    "\n",
    "# We create some data, 3 observed values at 234\n",
    "obs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run importance sampling with a default proposal,\n",
    "snd print the average weight of the samples, to give us a sense of how well the proposal is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, *sub_keys = jax.random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "args = ()\n",
    "jitted = jit(vmap(model.importance, in_axes=(0, None, None)))\n",
    "trace, weight = jitted(sub_keys, obs, args)\n",
    "print(\"The average weight is\", logsumexp(weight) - jnp.log(len(weight)))\n",
    "print(\"The maximum weight is\", weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the average and even maximum weight are quite low, which means that the proposal is not doing a great job.\n",
    "\n",
    "If there is no observations, ideally, the weight should center around 1 and be quite concentrated around that value.\n",
    "A weight much higher than 1 means that the proposal is too narrow and is missing modes. Indeed, for that to happen, one has to sample a very unlikely value under the proposal which is very likely under the target.\n",
    "A weight much lower than 1 means that the proposal is too broad and is wasting samples. This happens in this case as the default proposal uses the broad prior `normal(0.0, 100.0)` as a proposal, which is far from the observed values centred around $234.0$.\n",
    "\n",
    "If there are observations, as is the case above, the weight should center around the marginal on the observations.\n",
    "More precisely, if the model has density $p(x,y)$ where $y$ are the observations and the proposal has density $q(x)$, then a weight is given by $w = \\frac{p(x,y)}{q(x)}$ whose average value over many runs (expectations under the proposal) is $p(y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a custom proposal, which will be a normal distribution centred around the observed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def proposal(obs):\n",
    "    avg_val = jnp.array(obs).mean()\n",
    "    std = jnp.array(obs).std()\n",
    "    x = (\n",
    "        normal(avg_val, 0.1 + std) @ \"x\"\n",
    "    )  # To avoid a degenerate proposal, we add a small value to the standard deviation\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do things by hand first, let's reimplement the importance function.\n",
    "It samples from the proposal and then computes the importance weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sample(target, obs, proposal):\n",
    "    def _inner(key, target_args, proposal_args):\n",
    "        trace = proposal.simulate(key, *proposal_args)\n",
    "        # the full choice map under which we evaluate the model\n",
    "        # has the sampled values from the proposal and the observed values\n",
    "        chm = obs ^ trace.get_sample()\n",
    "        proposal_logpdf = trace.get_score()\n",
    "        target_logpdf, _ = target.assess(chm, *target_args)\n",
    "        importance_weight = target_logpdf - proposal_logpdf\n",
    "        return (trace, importance_weight)\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run importance sampling with the custom proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, *sub_keys = jax.random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "args_for_model = ()\n",
    "args_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),)\n",
    "jitted = jit(vmap(importance_sample(model, obs, proposal), in_axes=(0, None, None)))\n",
    "trace, new_weight = jitted(sub_keys, (args_for_model,), (args_for_proposal,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the new values, both average and maximum, are much higher than before, which means that the custom proposal is doing a much better job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The new average weight is\", logsumexp(new_weight) - jnp.log(len(new_weight)))\n",
    "print(\"The new maximum weight is\", new_weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same using the library functions.\n",
    "\n",
    "To do this, let's first create a target posterior distribution. It consists of the model, arguments for it, and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_posterior = Target(model, args_for_model, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we redefine the proposal slightly to take the target as argument. \n",
    "This way, it can extract the observations fro the target as we previously used.\n",
    "But the target can for instance also depend on the arguments passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def proposal(target: Target):\n",
    "    model_obs = target.constraint\n",
    "    used_obs = jnp.array([model_obs[\"obs1\"], model_obs[\"obs2\"], model_obs[\"obs3\"]])\n",
    "    avg_val = jnp.array(used_obs).mean()\n",
    "    std = jnp.array(used_obs).std()\n",
    "    x = normal(avg_val, 0.1 + std) @ \"x\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similarly to the importance_sampling notebook, we create an instance algorithm: it specifies a strategy to approximate our posterior of interest, `target_posterior`, using importance sampling with `k_particles`, and our custom proposal.\n",
    "\n",
    "To specify that we use all the traced variables from `proposal` in importance sampling (we will revisit why that may not be the case in the ravi_stack notebook) are to be used, we will use `proposal.marginal()`. This indicates that no traced variable from `proposal` is marginalized out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_particles = 1000\n",
    "alg = smc.ImportanceK(target_posterior, q=proposal.marginal(), k_particles=k_particles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will perform sampling importance resampling (SIR) with a $1000$ intermediate particles and one resampled and returned at the end which is returned. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted = jit(alg.simulate)\n",
    "key, subkey = jax.random.split(key)\n",
    "posterior_samples = jitted(subkey, (target_posterior,))\n",
    "posterior_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
