{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Stochastic Probabilities\n",
    "subtitle: How to create and use distributions with inexact likelihood evaluations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on top of the `custom_distribution` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import Pytree, Weight, pretty\n",
    "from genjax._src.generative_functions.distributions.distribution import Distribution\n",
    "from genjax.typing import Any, Tuple\n",
    "from jax.random import PRNGKey, split\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we defined a distribution for a Gaussian mixture, using the `Distribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class GaussianMixture(Distribution):\n",
    "    def random_weighted(self, key: PRNGKey, probs, means, vars) -> Tuple[Any, Weight]:\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n",
    "        key, subkey = split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "        zipped = jnp.stack([probs, means, vars], axis=1)\n",
    "        weight_recip = -jax.scipy.special.logsumexp(\n",
    "            jax.vmap(\n",
    "                lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(normal_sample)\n",
    "                + tfd.Categorical(probs=probs).log_prob(z[0])\n",
    "            )(zipped)\n",
    "        )\n",
    "\n",
    "        return normal_sample, weight_recip\n",
    "\n",
    "    def estimate_logpdf(self, key: PRNGKey, x, probs, means, vars) -> Weight:\n",
    "        zipped = jnp.stack([probs, means, vars], axis=1)\n",
    "        return jax.scipy.special.logsumexp(\n",
    "            jax.vmap(\n",
    "                lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(x)\n",
    "                + tfd.Categorical(probs=probs).log_prob(z[0])\n",
    "            )(zipped)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class above, note in `estimate_logpdf` how we computed the density as a sum over all possible paths in the that could lead to a particular outcome `x`. \n",
    "\n",
    "In fact, the same occurs in `random_weighted`: even though we know exactly the path we took to get to the sample `normal_sample`, when evaluating the reciprocal density, we also sum over all possible paths that could lead to that `value`. \n",
    "\n",
    "Precisely, this required to sum over all the possible values of the categorical distribution `cat`. We technically sampled two random values `cat_index` and `normal_sample`, but we are only interested in the distribution on `normal_sample`: we marginalized out the intermediate random variable `cat_index`. \n",
    "\n",
    "Mathematically, we have\n",
    "`p(normal_sample) = sum_{cat_index} p(normal_sample, cat_index)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenJAX supports a more general kind of distribution, that only need to be able to estimate their densities.\n",
    "The correctness criterion for this to be valid are that the estimation should be unbiased, i.e. the correct value on average.\n",
    "\n",
    "More precisely,  `estimate_logpdf` should return an unbiased density estimate, while `random_weighted` should return an unbiased estimate for the reciprocal density. In general you can't get one from the other, as the following example shows.\n",
    "\n",
    "Flip a coin and with 50% chance return 1, otherwise 3. This gives an unbiased estimator of 2.\n",
    "If we now return 1/1 with 50%, and 1/3 otherwise, the average value is 2/3, which is not 1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a Gaussian mixture distribution that only estimates its density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class StochasticGaussianMixture(Distribution):\n",
    "    def random_weighted(self, key: PRNGKey, probs, means, vars) -> Tuple[Any, Weight]:\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n",
    "        key, subkey = split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "        # We can estimate the reciprocal (marginal) density in constant time. Math magic explained at the end!\n",
    "        weight_recip = -tfd.Normal(\n",
    "            loc=means[cat_index], scale=vars[cat_index]\n",
    "        ).log_prob(normal_sample)\n",
    "        return normal_sample, weight_recip\n",
    "\n",
    "    # Given a sample `x`, we can also estimate the density in constant time\n",
    "    # Math again explained at the end.\n",
    "    # TODO: we could probably improve further with a better proposal\n",
    "    def estimate_logpdf(self, key: PRNGKey, x, probs, means, vars) -> Weight:\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        return tfd.Normal(loc=means[cat_index], scale=vars[cat_index]).log_prob(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test, we start by creating a generative function using our new distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = PRNGKey(0)\n",
    "sgm = StochasticGaussianMixture()\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def model(cat_probs, means, vars):\n",
    "    x = sgm(cat_probs, means, vars) @ \"x\"\n",
    "    y_means = jnp.repeat(x, len(means))\n",
    "    y = sgm(cat_probs, y_means, vars) @ \"y\"\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simulate from the model, assess a trace, or use importance sampling with the default proposal, seemlessly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_probs = jnp.array([0.1, 0.4, 0.2, 0.3])\n",
    "means = jnp.array([0.0, 1.0, 2.0, 3.0])\n",
    "vars = jnp.array([1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "tr = model.simulate(key, (cat_probs, means, vars))\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: currently raises a not implemented error\n",
    "# model.assess(tr.get_choices(), (cat_probs, means, vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2.0\n",
    "model.importance(key, C[\"y\"].set(y), (cat_probs, means, vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check that the distribution `sgm` unbiasedly estimates the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture()\n",
    "x = 2.0\n",
    "N = 42\n",
    "n_estimates = 2000000\n",
    "cat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N))\n",
    "cat_probs = cat_probs / jnp.sum(cat_probs)\n",
    "means = jnp.arange(0.0, N * 1.0, 1.0)\n",
    "vars = jnp.ones(N) / N\n",
    "cat_probs\n",
    "key = PRNGKey(0)\n",
    "keys = split(key, n_estimates)\n",
    "log_density = gm.estimate_logpdf(key, x, cat_probs, means, vars)  # exact value\n",
    "log_density\n",
    "jitted = jax.jit(jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None)))\n",
    "estimates = jitted(keys, x, cat_probs, means, vars)\n",
    "log_mean_estimates = jax.scipy.special.logsumexp(estimates) - jnp.log(len(estimates))\n",
    "# TODO: somehow there's a bug, it doesn't converge to the exact value\n",
    "# error is always a factor of 3.\n",
    "# np.exp(log_mean_estimates)/jnp.exp(log_density)\n",
    "log_density, log_mean_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find a way to plot decently\n",
    "# plt.hist(estimates, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of using density estimates instead of exact ones is that it can be much faster to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: doesn't shine here as the problem is too simple: it's parallel friendly linear time for the exact one.\n",
    "N = 30000\n",
    "n_estimates = 10\n",
    "key = PRNGKey(0)\n",
    "keys = split(key, n_estimates)\n",
    "cat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N))\n",
    "cat_probs = cat_probs / jnp.sum(cat_probs)\n",
    "means = jnp.arange(0.0, N * 1.0, 1.0)\n",
    "vars = jnp.ones(N) / N\n",
    "\n",
    "jitted_exact = jax.jit(gm.estimate_logpdf)\n",
    "jitted_approx = jax.jit(\n",
    "    lambda key, x, cat_probs, means, vars: jax.scipy.special.logsumexp(\n",
    "        jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None))(\n",
    "            key, x, cat_probs, means, vars\n",
    "        )\n",
    "    )\n",
    "    - jnp.log(n_estimates)\n",
    ")\n",
    "\n",
    "jitted_exact(key, x, cat_probs, means, vars)\n",
    "jitted_approx(keys, x, cat_probs, means, vars)\n",
    "%timeit jitted(keys, x, cat_probs, means, vars)\n",
    "%timeit jitted_approx(keys, x, cat_probs, means, vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we need both is that both methods will be used at different times, notably depending on whether we use the distribution in a proposal or in a model, as we now show!\n",
    "\n",
    "Let's define a simple model and a proposal which both use our `sgm` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@genjax.gen\n",
    "def model(cat_probs, means, vars):\n",
    "    x = sgm(cat_probs, means, vars) @ \"x\"\n",
    "    y_means = jnp.repeat(x, len(means))\n",
    "    y = sgm(cat_probs, y_means, vars) @ \"y\"\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def proposal(obs, cat_probs, means, vars):\n",
    "    y = obs[\"y\"]\n",
    "    new_means = jax.vmap(lambda m: (m + y) / 2)(means)\n",
    "    x = sgm(cat_probs, new_means, vars) @ \"x\"\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define importance sampling once again! Note that it is exactly the same as the usual one. This is because GenJAX implements `simulate` using `random_weighted` and `assess` using `estimate_logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensp_importance_sampling(target, proposal):\n",
    "    def _inner(key, target_args, proposal_args):\n",
    "        trace = proposal.simulate(key, *proposal_args)\n",
    "        chm = trace.get_sample()\n",
    "        proposal_logpdf = trace.get_score()\n",
    "        target_logpdf, _ = target.assess(chm, *target_args)\n",
    "        importance_weight = target_logpdf - proposal_logpdf\n",
    "        return (trace, importance_weight)\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = C[\"y\"].set(2.0)\n",
    "\n",
    "# TODO: awkward parenthesis and same problem about assess not implemented.\n",
    "# gensp_importance_sampling(model, proposal)(key, ((cat_probs, means, vars),), ((chm, cat_probs, means, vars),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for those curious about the math magic that enabled to correctly (meaning unbiasedly) estimate the pdf and its reciprocal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Math is that p(x) = sum_i p(x|z=i)p(z=i) = E_z[p(x|z)]\n",
    "# And we can estimate this expectation by sampling z from the categorical distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
