{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Stochastic Probabilities\n",
    "subtitle: How to create and use distributions with inexact likelihood evaluations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on top of the `custom_distribution` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import Pytree, Weight, pretty\n",
    "from genjax._src.generative_functions.distributions.distribution import Distribution\n",
    "from genjax.typing import Any\n",
    "\n",
    "tfd = tfp.distributions\n",
    "key = jax.random.PRNGKey(0)\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we defined a distribution for a Gaussian mixture, using the `Distribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class GaussianMixture(Distribution):\n",
    "    def random_weighted(\n",
    "        self, key: jax.random.PRNGKey, probs, means, vars\n",
    "    ) -> tuple[Weight, Any]:\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n",
    "        key, subkey = jax.random.split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "        zipped = jnp.stack([jnp.arange(0, len(probs)), means, vars], axis=1)\n",
    "        weight_recip = -jax.scipy.special.logsumexp(\n",
    "            jax.vmap(\n",
    "                lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(normal_sample)\n",
    "                + tfd.Categorical(probs=probs).log_prob(z[0])\n",
    "            )(zipped)\n",
    "        )\n",
    "\n",
    "        return weight_recip, normal_sample\n",
    "\n",
    "    def estimate_logpdf(self, key: jax.random.PRNGKey, x, probs, means, vars) -> Weight:\n",
    "        zipped = jnp.stack([jnp.arange(0, len(probs)), means, vars], axis=1)\n",
    "        return jax.scipy.special.logsumexp(\n",
    "            jax.vmap(\n",
    "                lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(x)\n",
    "                + tfd.Categorical(probs=probs).log_prob(z[0])\n",
    "            )(zipped)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class above, note in `estimate_logpdf` how we computed the density as a sum over all possible paths in the that could lead to a particular outcome `x`. \n",
    "\n",
    "In fact, the same occurs in `random_weighted`: even though we know exactly the path we took to get to the sample `normal_sample`, when evaluating the reciprocal density, we also sum over all possible paths that could lead to that `value`. \n",
    "\n",
    "Precisely, this required to sum over all the possible values of the categorical distribution `cat`. We technically sampled two random values `cat_index` and `normal_sample`, but we are only interested in the distribution on `normal_sample`: we marginalized out the intermediate random variable `cat_index`. \n",
    "\n",
    "Mathematically, we have\n",
    "`p(normal_sample) = sum_{cat_index} p(normal_sample, cat_index)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenJAX supports a more general kind of distribution, that only need to be able to estimate their densities.\n",
    "The correctness criterion for this to be valid are that the estimation should be unbiased, i.e. the correct value on average.\n",
    "\n",
    "More precisely,  `estimate_logpdf` should return an unbiased density estimate, while `random_weighted` should return an unbiased estimate for the reciprocal density. In general you can't get one from the other, as the following example shows.\n",
    "\n",
    "Flip a coin and with $50%$ chance return $1$, otherwise $3$. This gives an unbiased estimator of $2$.\n",
    "If we now return $\\frac{1}{1}$ with 50%, and $\\frac{1}{3}$ otherwise, the average value is $\\frac{2}{3}$, which is not $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a Gaussian mixture distribution that only estimates its density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class StochasticGaussianMixture(Distribution):\n",
    "    def random_weighted(\n",
    "        self, key: jax.random.PRNGKey, probs, means, vars\n",
    "    ) -> tuple[Weight, Any]:\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n",
    "        key, subkey = jax.random.split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "        # We can estimate the reciprocal (marginal) density in constant time. Math magic explained at the end!\n",
    "        weight_recip = -tfd.Normal(\n",
    "            loc=means[cat_index], scale=vars[cat_index]\n",
    "        ).log_prob(normal_sample)\n",
    "        return weight_recip, normal_sample\n",
    "\n",
    "    # Given a sample `x`, we can also estimate the density in constant time\n",
    "    # Math again explained at the end.\n",
    "    # TODO: we could probably improve further with a better proposal\n",
    "    def estimate_logpdf(self, key: jax.random.PRNGKey, x, probs, means, vars) -> Weight:\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        return tfd.Normal(loc=means[cat_index], scale=vars[cat_index]).log_prob(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test, we start by creating a generative function using our new distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgm = StochasticGaussianMixture()\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def model(cat_probs, means, vars):\n",
    "    x = sgm(cat_probs, means, vars) @ \"x\"\n",
    "    y_means = jnp.repeat(x, len(means))\n",
    "    y = sgm(cat_probs, y_means, vars) @ \"y\"\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simulate from the model, assess a trace, or use importance sampling with the default proposal, seemlessly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_probs = jnp.array([0.1, 0.4, 0.2, 0.3])\n",
    "means = jnp.array([0.0, 1.0, 2.0, 3.0])\n",
    "vars = jnp.array([1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "tr = model.simulate(subkey, (cat_probs, means, vars))\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assess currently raises a not implemented error, but we can use importance with a full trace instead\n",
    "# model.assess(tr.get_choices(), (cat_probs, means, vars))\n",
    "key, subkey = jax.random.split(key)\n",
    "_, w = model.importance(subkey, tr.get_choices(), (cat_probs, means, vars))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2.0\n",
    "key, subkey = jax.random.split(key)\n",
    "model.importance(subkey, C[\"y\"].set(y), (cat_probs, means, vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check that `estimate_logpdf` from our distribution `sgm` indeed correctly estimates the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture()\n",
    "x = 2.0\n",
    "N = 42\n",
    "n_estimates = 2000000\n",
    "cat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N))\n",
    "cat_probs = cat_probs / jnp.sum(cat_probs)\n",
    "means = jnp.arange(0.0, N * 1.0, 1.0)\n",
    "vars = jnp.ones(N) / N\n",
    "key, subkey = jax.random.split(key)\n",
    "log_density = gm.estimate_logpdf(subkey, x, cat_probs, means, vars)  # exact value\n",
    "log_density\n",
    "jitted = jax.jit(jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None)))\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, n_estimates)\n",
    "estimates = jitted(keys, x, cat_probs, means, vars)\n",
    "log_mean_estimates = jax.scipy.special.logsumexp(estimates) - jnp.log(len(estimates))\n",
    "log_density, log_mean_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of using density estimates instead of exact ones is that it can be much faster to compute. \n",
    "Here's a way to test it, though it will not shine on this example as it is too simple. \n",
    "We will explore examples in different notebooks where this shines more brightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30000\n",
    "n_estimates = 10\n",
    "cat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N))\n",
    "cat_probs = cat_probs / jnp.sum(cat_probs)\n",
    "means = jnp.arange(0.0, N * 1.0, 1.0)\n",
    "vars = jnp.ones(N) / N\n",
    "\n",
    "jitted_exact = jax.jit(gm.estimate_logpdf)\n",
    "jitted_approx = jax.jit(\n",
    "    lambda key, x, cat_probs, means, vars: jax.scipy.special.logsumexp(\n",
    "        jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None))(\n",
    "            key, x, cat_probs, means, vars\n",
    "        )\n",
    "    )\n",
    "    - jnp.log(n_estimates)\n",
    ")\n",
    "\n",
    "# warmup the jit\n",
    "key, subkey = jax.random.split(key)\n",
    "jitted_exact(subkey, x, cat_probs, means, vars)\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, n_estimates)\n",
    "jitted_approx(keys, x, cat_probs, means, vars)\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, n_estimates)\n",
    "%timeit jitted(keys, x, cat_probs, means, vars)\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, n_estimates)\n",
    "%timeit jitted_approx(keys, x, cat_probs, means, vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the reason we need both methods `random_weighted` and `estimate_logpdf` is that both methods will be used at different times, notably depending on whether we use the distribution in a proposal or in a model, as we show next.\n",
    "\n",
    "Let's define a simple model and a proposal which both use our `sgm` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@genjax.gen\n",
    "def model(cat_probs, means, vars):\n",
    "    x = sgm(cat_probs, means, vars) @ \"x\"\n",
    "    y_means = jnp.repeat(x, len(means))\n",
    "    y = sgm(cat_probs, y_means, vars) @ \"y\"\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def proposal(obs, cat_probs, means, vars):\n",
    "    y = obs[\"y\"]\n",
    "    # simple logic to propose a new x: its mean was presumably closer to y\n",
    "    new_means = jax.vmap(lambda m: (m + y) / 2)(means)\n",
    "    x = sgm(cat_probs, new_means, vars) @ \"x\"\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define importance sampling once again. Note that it is exactly the same as the usual one! \n",
    "\n",
    "This is because behind the scenes GenJAX implements `simulate` using `random_weighted` and `assess` using `estimate_logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensp_importance_sampling(target, obs, proposal):\n",
    "    def _inner(key, target_args, proposal_args):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        trace = proposal.simulate(key, *proposal_args)\n",
    "        chm = obs ^ trace.get_sample()\n",
    "        proposal_logpdf = trace.get_score()\n",
    "        # TODO: using importance instead of assess, as assess is not implemented\n",
    "        _, target_logpdf = target.importance(subkey, chm, *target_args)\n",
    "        importance_weight = target_logpdf - proposal_logpdf\n",
    "        return (trace, importance_weight)\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = C[\"y\"].set(2.0)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "gensp_importance_sampling(model, obs, proposal)(\n",
    "    subkey, ((cat_probs, means, vars),), ((obs, cat_probs, means, vars),)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for those curious about the math magic that enabled to correctly (meaning unbiasedly) estimate the pdf and its reciprocal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `estimate_logpdf`. \n",
    "We have that the marginal distribution over the returned value `x` (the sample from the normal distribution) is given by\n",
    "$$p(x) = \\sum_i p(x\\mid z=i) p(z=i)$$ \n",
    "where the sum is over the possible values of the categorical distribution, $p(x|z=i)$  is the density of the $i$-th normal at $x$, and $p(z=i)$ is the density of the categorical at the value $i$.\n",
    "\n",
    "This sum can be rewritten as the expectation under the categorical distribution $p(z)$: \n",
    "$$\\sum_i p(x\\mid z=i)p(z=i) = \\mathbb{E}_{z\\sim p(z)}[p(x\\mid z)]$$  \n",
    "This means we can get an unbiased estimate of the expectation by simply sampling a `z` and returning `p(x|z)`: the average value of this process is obviously its expectation (it's the definition on the expectation).\n",
    "In other words, we proved that the estimation strategy used in `estimate_logpdf` indeed returns an unbiased estimate of the exact marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, as we discussed above we cannot in general invert an unbiased estimate to get an unbiased estimate of the reciprocal, so one may be suspicious that the returned weight in `random_weighted` looks like the negation (in logspace) of the one returned in `estimate_logpdf`. \n",
    "Here the argument is different, based on the following identity: \n",
    "$$\\frac{1}{p(x)} = \\mathbb{E}_{z\\sim p(z\\mid x)}[\\frac{1}{p(x\\mid z)}]$$\n",
    "The idea is that we can get an unbiased estimate if we can sample from the posterior $p(z|x)$. Given an $x$, this is an intractable sampling problem in general. However, in `random_weighted`, we sample a $z$ together with the $x$, and this $z$ is an exact posterior sample of $z$ that we get \"for free\". \n",
    "Now to finish the explanation, the compact way to prove the identity is as follows.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\frac{1}{p(x)} &\\\\\n",
    "= \\frac{1}{p(x)} \\mathbb{E}_{z \\sim B}[p(z)] & \\text{$p(z)$ density w.r.t. base measure $B$ and of total mass 1}\\\\\n",
    "= \\frac{1}{p(x)} \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z\\mid x)}]   &\\text{seeing $p(z|x)$ as an importance sampler for $B$}\\\\\n",
    "= \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z\\mid x)p(x)}]  & \\text{$p(x)$ doesn't depend on $z$ moved within the expectation}\\\\\n",
    "= \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z,x)}]   & \\text{ definition of joint distribution}\\\\\n",
    "= \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z)p(x|z)}] & \\text{definition of conditional distribution}\\\\\n",
    "=  \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{1}{p(x|z)}]   & \\text{simplification}\n",
    "\\end{matrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
