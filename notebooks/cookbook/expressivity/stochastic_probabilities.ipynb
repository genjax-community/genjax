{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Stochastic Probabilities\n",
    "subtitle: How to use distributions with inexact likelihood evaluations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on top of the `custom_distribution` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from genjax import Pytree, Weight, pretty\n",
    "from genjax._src.generative_functions.distributions.distribution import Distribution\n",
    "from genjax.typing import Any, Tuple\n",
    "from jax.random import PRNGKey, split\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we defined a distribution for a Gaussian mixture, using the `Distribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class GaussianMixture(Distribution):\n",
    "    def random_weighted(self, key: PRNGKey, probs, means, vars) -> Tuple[Any, Weight]:\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n",
    "        key, subkey = split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "        zipped = jnp.stack([probs, means, vars], axis=1)\n",
    "        weight_recip = -jnp.log(\n",
    "            sum(\n",
    "                jax.vmap(\n",
    "                    lambda z: tfd.Normal(loc=z[1], scale=z[2]).prob(normal_sample)\n",
    "                    * tfd.Categorical(probs=probs).prob(z[0])\n",
    "                )(zipped)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return normal_sample, weight_recip\n",
    "\n",
    "    def estimate_logpdf(self, key: PRNGKey, x, probs, means, vars) -> Weight:\n",
    "        zipped = jnp.stack([probs, means, vars], axis=1)\n",
    "        return jnp.log(\n",
    "            sum(\n",
    "                jax.vmap(\n",
    "                    lambda z: tfd.Normal(loc=z[1], scale=z[2]).prob(x)\n",
    "                    * tfd.Categorical(probs=probs).prob(z[0])\n",
    "                )(zipped)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class above, note in `estimate_logpdf` how we computed the density as a sum over all possible paths in the that could lead to a particular outcome `x`. \n",
    "\n",
    "In fact, the same occurs in `random_weighted`: even though we know exactly the path we took to get to the sample `normal_sample`, when evaluating the reciprocal density, we also sum over all possible paths that could lead to that `value`. \n",
    "\n",
    "Precisely, this required to sum over all the possible values of the categorical distribution `cat`. We technically sampled two random values `cat_index` and `normal_sample`, but we are only interested in the distribution on `normal_sample`: we marginalized out the intermediate random variable `cat_index`. \n",
    "\n",
    "Mathematically, we have\n",
    "`p(normal_sample) = sum_{cat_index} p(normal_sample, cat_index)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenJAX supports a more general kind of distribution, that only need to be able to estimate their densities.\n",
    "The correctness criterion for this to be valid are that the estimation should be unbiased, i.e. the correct value on average.\n",
    "\n",
    "More precisely,  `estimate_logpdf` should return an unbiased density estimate, while `random_weighted` should return an unbiased estimate for the reciprocal density. In general you can't get one from the other, as the following example shows.\n",
    "\n",
    "Flip a coin and with 50% chance return 1, otherwise 3. This gives an unbiased estimator of 2.\n",
    "If we now return 1/1 with 50%, and 1/3 otherwise, the average value is 2/3, which is not 1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a Gaussian mixture distribution that only estimates its density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Pytree.dataclass\n",
    "class StochasticGaussianMixture(Distribution):\n",
    "    def random_weighted(self, key: PRNGKey, probs, means, vars) -> Tuple[Any, Weight]:\n",
    "        probs = jnp.asarray(probs)\n",
    "        means = jnp.asarray(means)\n",
    "        vars = jnp.asarray(vars)\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n",
    "        key, subkey = split(key)\n",
    "        normal_sample = normal.sample(seed=subkey)\n",
    "        # We can estimate the reciprocal in constant time\n",
    "        # The math magic will be detailed later\n",
    "        weight_recip = -tfd.Normal(\n",
    "            loc=means[cat_index], scale=vars[cat_index]\n",
    "        ).log_prob(normal_sample)\n",
    "\n",
    "        return normal_sample, weight_recip\n",
    "\n",
    "    # We can also estimate the pdf in constant time\n",
    "    # The math magic will here will also be detailed later\n",
    "    def estimate_logpdf(self, key: PRNGKey, x, probs, means, vars) -> Weight:\n",
    "        cat = tfd.Categorical(probs=probs)\n",
    "        cat_index = jnp.asarray(cat.sample(seed=key))\n",
    "        return tfd.Normal(loc=means[cat_index], scale=vars[cat_index]).log_prob(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: just run, see and plot the stochasticity, compare to the original implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we need both is that both methods will be used at different times, notably depending on whether we use the distribution in a proposal or in a model, as we now show!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add importance sampling here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for those interested, we will now explain the math magic that enabled us to get fast and unbiased density estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add the math magic here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
