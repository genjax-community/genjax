{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Differentiating probabilistic programs\n",
    "subtitle: How to take drastic differentiating measures by differentiating measures\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "from genjax._src.adev.core import Dual, expectation\n",
    "from genjax._src.adev.primitives import flip_enum\n",
    "\n",
    "key = jax.random.PRNGKey(314159)\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in the average returned value of a probabilistic program. For instance, it could be that \n",
    "a run of the program represents a run of a simulation of some form, and we would like to maximize the average reward across many simulations (or equivalently minimize a loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular technique from optimization is to use iterative methods such as (stochastic) gradient descent. \n",
    "One may think we could simply use JAX's AD system to compute gradients of probabilistic programs. \n",
    "Indeed, instead of getting a stochastic estimate of the average behaviour by running the program, we would obtain a new program and running it would get us an estimate of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try it on a simple example in JAX however, we do not get what we would like at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_model(key, theta):\n",
    "    b = jax.random.bernoulli(key, theta)\n",
    "    return jax.lax.cond(b, lambda _: 0.0, lambda _: -theta / 2, None)\n",
    "\n",
    "\n",
    "def expected_val(theta):\n",
    "    return (theta**2 - theta) / 2\n",
    "\n",
    "\n",
    "grad = jax.jit(jax.grad(jax_model, argnums=1))\n",
    "\n",
    "arg = 0.2\n",
    "vals = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    grad_val = grad(subkey, arg)\n",
    "    arg = arg - 0.01 * grad_val\n",
    "    vals.append(expected_val(arg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX seems happy to compute gradients and do some form of gradient descent, but let's see if we managed to minimize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seemed to start ok but then for some reason the curve goes back up and we end up maximizing the loss instead of minimizing it!\n",
    "\n",
    "The reason is that we failed to account from the change of contribution of the coin flip from  `bernoulli` in the differentiation process, and we will come back to this in more details in follow up notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADEV is a new algorithm that computes correct gradient estimates of expectations of probabilistic programs. It  accounts for the change to the expectation coming from a change to the underlying measure present in the expectation (from which all the randomness is drawn).\n",
    "\n",
    "GenJAX implements ADEV. Slightly rewriting the example from above using GenJAX, we can see how different the behaviour of the optimization process with the corrected gradient estimates is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@expectation\n",
    "def flip_exact_loss(theta):\n",
    "    b = flip_enum(theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda _: 0.0,\n",
    "        lambda _: -theta / 2.0,\n",
    "        theta,\n",
    "    )\n",
    "\n",
    "\n",
    "adev_grad = jax.jit(flip_exact_loss.jvp_estimate)\n",
    "\n",
    "arg = 0.2\n",
    "adev_vals = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    grad_val = adev_grad(subkey, Dual(arg, 1.0)).tangent\n",
    "    arg = arg - 0.01 * grad_val\n",
    "    adev_vals.append(expected_val(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vals)\n",
    "plt.plot(adev_vals)\n",
    "plt.legend([\"JAX\", \"ADEV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we just used a forward-mode version of ADEV in the above example. GenJAX also supports a reverse-mode version which is also fully compatible with JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_adev_grad = jax.jit(flip_exact_loss.grad_estimate)\n",
    "\n",
    "arg = 0.2\n",
    "rev_adev_vals = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    (grad_val,) = rev_adev_grad(subkey, (arg,))\n",
    "    arg = arg - 0.01 * grad_val\n",
    "    rev_adev_vals.append(expected_val(arg))\n",
    "\n",
    "plt.plot(rev_adev_vals)\n",
    "plt.legend([\"reverse mode ADEV\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
