{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "title: Differentiating probabilistic programs\n",
    "subtitle: How to take drastic differentiating measures by differentiating measures\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnusedExpression=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import and constants\n",
    "import genstudio.plot as Plot\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from genstudio.plot import js\n",
    "\n",
    "from genjax._src.adev.core import Dual, expectation\n",
    "from genjax._src.adev.primitives import flip_enum, normal_reparam\n",
    "\n",
    "key = jax.random.key(314159)\n",
    "EPOCHS = 400\n",
    "default_sigma = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We are often interested in the average returned value of a probabilistic program. For instance, it could be that\n",
    "a run of the program represents a run of a simulation of some form, and we would like to maximize the average reward across many simulations (or equivalently minimize a loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def noisy_jax_model(key, theta, sigma):\n",
    "    b = jax.random.bernoulli(key, theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda theta: jax.random.normal(key) * sigma * theta,\n",
    "        lambda theta: jax.random.normal(key) * sigma + theta / 2,\n",
    "        theta,\n",
    "    )\n",
    "\n",
    "\n",
    "def expected_val(theta):\n",
    "    return (theta - theta**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We can see that the simulation can have two \"modes\" that split further appart over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Samples\n",
    "thetas = jnp.arange(0.0, 1.0, 0.0005)\n",
    "\n",
    "\n",
    "def make_samples(key, thetas, sigma):\n",
    "    return jax.vmap(noisy_jax_model, in_axes=(0, 0, None))(\n",
    "        jax.random.split(key, len(thetas)), thetas, sigma\n",
    "    )\n",
    "\n",
    "\n",
    "key, samples_key = jax.random.split(key)\n",
    "noisy_samples = make_samples(samples_key, thetas, default_sigma)\n",
    "\n",
    "plot_options = Plot.new(\n",
    "    Plot.color_legend(),\n",
    "    {\"x\": {\"label\": \"θ\"}, \"y\": {\"label\": \"y\"}},\n",
    "    Plot.aspect_ratio(1),\n",
    "    Plot.grid(),\n",
    "    Plot.clip(),\n",
    ")\n",
    "\n",
    "samples_color_map = Plot.color_map({\"Samples\": \"rgba(0, 128, 128, 0.5)\"})\n",
    "\n",
    "\n",
    "def make_samples_plot(thetas, samples):\n",
    "    return (\n",
    "        Plot.dot({\"x\": thetas, \"y\": samples}, fill=Plot.constantly(\"Samples\"), r=2)\n",
    "        + samples_color_map\n",
    "        + plot_options\n",
    "        + Plot.clip()\n",
    "    )\n",
    "\n",
    "\n",
    "samples_plot = make_samples_plot(thetas, noisy_samples)\n",
    "\n",
    "samples_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We can also easily imagine a more noisy version of the same idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def more_noisy_jax_model(key, theta, sigma):\n",
    "    b = jax.random.bernoulli(key, theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda _: jax.random.normal(key) * sigma * theta**2 / 3,\n",
    "        lambda _: (jax.random.normal(key) * sigma + theta) / 2,\n",
    "        None,\n",
    "    )\n",
    "\n",
    "\n",
    "more_thetas = jnp.arange(0.0, 1.0, 0.0005)\n",
    "key, *keys = jax.random.split(key, len(more_thetas) + 1)\n",
    "\n",
    "noisy_sample_plot = (\n",
    "    Plot.dot(\n",
    "        {\n",
    "            \"x\": more_thetas,\n",
    "            \"y\": jax.vmap(more_noisy_jax_model, in_axes=(0, 0, None))(\n",
    "                jnp.array(keys), more_thetas, default_sigma\n",
    "            ),\n",
    "        },\n",
    "        fill=Plot.constantly(\"Samples\"),\n",
    "        r=2,\n",
    "    )\n",
    "    + samples_color_map\n",
    ")\n",
    "\n",
    "noisy_sample_plot + plot_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "As we can see better on the noisy version, the samples divide into two groups. One tends to go up as theta increases while the other stays relatively stable around 0 with a higher variance. For simplicity of the analysis, in the rest of this notebook we will stick to the simpler first example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "\n",
    "In that simple case, we can compute the exact average value of the random process as a function of $\\theta$. We have probability $\\theta$ to return $0$ and probablity $1-\\theta$ to return $\\frac{\\theta}{2}$. So overall the expected value is\n",
    "$$\\theta*0 + (1-\\theta)*\\frac{\\theta}{2} = \\frac{\\theta-\\theta^2}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We can code this and plot the result for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding exact expectation\n",
    "thetas_sparse = jnp.linspace(0.0, 1.0, 20)  # fewer points, for the plot\n",
    "exact_vals = jax.vmap(expected_val)(thetas_sparse)\n",
    "\n",
    "expected_value_plot = (\n",
    "    Plot.line(\n",
    "        {\"x\": thetas_sparse, \"y\": exact_vals},\n",
    "        strokeWidth=2,\n",
    "        stroke=Plot.constantly(\"Expected value\"),\n",
    "        curve=\"natural\",\n",
    "    )\n",
    "    + Plot.color_map({\"Expected value\": \"black\"})\n",
    "    + plot_options,\n",
    ")\n",
    "\n",
    "samples_plot + expected_value_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We can see that the curve in yellow is a perfectly reasonable differentiable function. We can use JAX to compute its derivative (more generally its gradient) at various points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_exact = jax.jit(jax.grad(expected_val))\n",
    "theta_tangent_points = [0.1, 0.3, 0.45]\n",
    "\n",
    "\n",
    "color1 = \"rgba(255,165,0,0.5)\"\n",
    "color2 = \"#FB575D\"\n",
    "\n",
    "\n",
    "def tangent_line_plot(theta_tan):\n",
    "    slope = grad_exact(theta_tan)\n",
    "    y_intercept = expected_val(theta_tan) - slope * theta_tan\n",
    "    label = f\"Tangent at θ={theta_tan}\"\n",
    "\n",
    "    return Plot.line(\n",
    "        [[0, y_intercept], [1, slope + y_intercept]],\n",
    "        stroke=Plot.constantly(label),\n",
    "    ) + Plot.color_map({\n",
    "        label: Plot.js(\n",
    "            f\"\"\"d3.interpolateHsl(\"{color1}\", \"{color2}\")({theta_tan}/{theta_tangent_points[-1]})\"\"\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "\n",
    "(\n",
    "    plot_options\n",
    "    + [tangent_line_plot(theta_tan) for theta_tan in theta_tangent_points]\n",
    "    + expected_value_plot\n",
    "    + Plot.domain([0, 1], [0, 0.4])\n",
    "    + Plot.title(\"Expectation curve and its Tangent Lines\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "A popular technique from optimization is to use iterative methods such as (stochastic) gradient ascent.\n",
    "Starting from any location, say 0.2, we can use JAX to find the maximum of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = 0.2\n",
    "vals = []\n",
    "arg_list = []\n",
    "for _ in range(EPOCHS):\n",
    "    grad_val = grad_exact(arg)\n",
    "    arg_list.append(arg)\n",
    "    vals.append(expected_val(arg))\n",
    "    arg = arg + 0.01 * grad_val\n",
    "    if arg < 0:\n",
    "        arg = 0\n",
    "        break\n",
    "    elif arg > 1:\n",
    "        arg = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We can plot the evolution of the value of the function over the iterations of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    Plot.line({\"x\": list(range(EPOCHS)), \"y\": vals})\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We can also directly visualize the points on the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    expected_value_plot\n",
    "    + Plot.dot(\n",
    "        {\"x\": arg_list, \"y\": vals},\n",
    "        fill=Plot.js(\n",
    "            f\"\"\"(_, i) => d3.interpolateHsl('{color1}', '{color2}')(i/{len(arg_list)})\"\"\"\n",
    "        ),\n",
    "    )\n",
    "    + Plot.subtitle(\"Gradient descent from start to end\")\n",
    "    + Plot.aspect_ratio(0.25)\n",
    "    + {\"width\": 600}\n",
    "    + Plot.color_map({\"start\": color1, \"end\": color2})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We have this in this example that we can compute the average value exactly. But will not be the case in general. One popular technique to approximate an average value is to use Monte Carlo Integration: we sample a bunch from the program and take the average value.\n",
    "\n",
    "As we use more and more samples we will converge to the correct result by the Central limit theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = sorted([1, 3, 5, 10, 20, 50, 100, 200, 500, 1000] * 7)\n",
    "means = []\n",
    "for n in number_of_samples:\n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(key, n)\n",
    "    samples = jax.vmap(noisy_jax_model, in_axes=(0, None, None))(\n",
    "        keys, 0.3, default_sigma\n",
    "    )\n",
    "    mean = jnp.mean(samples)\n",
    "    means.append(mean)\n",
    "\n",
    "(\n",
    "    Plot.dot(\n",
    "        {\"x\": number_of_samples, \"y\": means},\n",
    "        fill=Plot.js(\n",
    "            f\"\"\"(_, i) => d3.interpolateHsl('{color1}', '{color2}')(i/{len(number_of_samples)})\"\"\"\n",
    "        ),\n",
    "    )\n",
    "    + Plot.ruleY(\n",
    "        [expected_val(0.3)],\n",
    "        opacity=0.2,\n",
    "        strokeWidth=2,\n",
    "        stroke=Plot.constantly(\"True value\"),\n",
    "    )\n",
    "    + Plot.color_map({\"Mean estimate\": color1, \"True value\": \"green\"})\n",
    "    + Plot.color_legend()\n",
    "    + {\"x\": {\"label\": \"Number of samples\", \"type\": \"log\"}, \"y\": {\"label\": \"y\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "As we just discussed, most of the time we will not be able to compute the average value and then compute the gradient using JAX. One thing we may want to try is to use JAX on the probabilistic program to get a gradient estimate, and hope that by using more and more samples this will converge to the correct gradient that we could use in optimization. Let's try it in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_tan = 0.3\n",
    "\n",
    "slope = grad_exact(theta_tan)\n",
    "y_intercept = expected_val(theta_tan) - slope * theta_tan\n",
    "\n",
    "exact_tangent_plot = Plot.line(\n",
    "    [[0, y_intercept], [1, slope + y_intercept]],\n",
    "    strokeWidth=2,\n",
    "    stroke=Plot.constantly(\"Exact tangent at θ=0.3\"),\n",
    ")\n",
    "\n",
    "\n",
    "def slope_estimate_plot(slope_est):\n",
    "    y_intercept = expected_val(theta_tan) - slope_est * theta_tan\n",
    "    return Plot.line(\n",
    "        [[0, y_intercept], [1, slope_est + y_intercept]],\n",
    "        strokeWidth=2,\n",
    "        stroke=Plot.constantly(\"Tangent estimate\"),\n",
    "    )\n",
    "\n",
    "\n",
    "slope_estimates = [slope + i / 20 for i in range(-4, 4)]\n",
    "\n",
    "(\n",
    "    samples_plot\n",
    "    + expected_value_plot\n",
    "    + [slope_estimate_plot(slope_est) for slope_est in slope_estimates]\n",
    "    + exact_tangent_plot\n",
    "    + Plot.title(\"Expectation curve and Tangent Estimates at θ=0.3\")\n",
    "    + Plot.color_map({\n",
    "        \"Tangent estimate\": color1,\n",
    "        \"Exact tangent at θ=0.3\": color2,\n",
    "    })\n",
    "    + Plot.domain([0, 1], [0, 0.4])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_grad = jax.jit(jax.grad(noisy_jax_model, argnums=1))\n",
    "\n",
    "arg = 0.2\n",
    "vals = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    grad_val = jax_grad(subkey, arg, default_sigma)\n",
    "    arg = arg + 0.01 * grad_val\n",
    "    vals.append(expected_val(arg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "JAX seems happy to compute something and we can use the iterative technique from before, but let's see if we managed to minimize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": vals},\n",
    "        stroke=Plot.constantly(\"Attempting gradient ascent with JAX\"),\n",
    "    )\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    "    + Plot.domainX([0, EPOCHS])\n",
    "    + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n",
    "    + Plot.color_legend()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Woops! We seemed to start ok but then for some reason the curve goes back down and we end up minimizing the function instead of maximizing it!\n",
    "\n",
    "The reason is that we failed to account from the change of contribution of the coin flip from  `bernoulli` in the differentiation process, and we will come back to this in more details in follow up notebooks.\n",
    "\n",
    "For now, let's just get a sense of what the gradient estimates computed by JAX look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "theta_tangents = jnp.linspace(0, 1, 20)\n",
    "\n",
    "\n",
    "def plot_tangents(gradients, title):\n",
    "    tangents_plots = Plot.new(Plot.aspectRatio(0.5))\n",
    "\n",
    "    for theta, slope in gradients:\n",
    "        y_intercept = expected_val(theta) - slope * theta\n",
    "        tangents_plots += Plot.line(\n",
    "            [[0, y_intercept], [1, slope + y_intercept]],\n",
    "            stroke=Plot.js(\n",
    "                f\"\"\"d3.interpolateHsl(\"{color1}\", \"{color2}\")({theta}/{theta_tangents[-1]})\"\"\"\n",
    "            ),\n",
    "            opacity=0.75,\n",
    "        )\n",
    "    return Plot.new(\n",
    "        expected_value_plot,\n",
    "        Plot.domain([0, 1], [0, 0.4]),\n",
    "        tangents_plots,\n",
    "        Plot.title(title),\n",
    "        Plot.color_map({\n",
    "            f\"Tangent at θ={theta_tangents[0]}\": color1,\n",
    "            f\"Tangent at θ={theta_tangents[-1]}\": color2,\n",
    "        }),\n",
    "    )\n",
    "\n",
    "\n",
    "gradients = []\n",
    "for theta in theta_tangents:\n",
    "    key, subkey = jax.random.split(key)\n",
    "    gradients.append((theta, jax_grad(subkey, theta, default_sigma)))\n",
    "\n",
    "plot_tangents(gradients, \"Expectation curve and JAX-computed tangent estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Ouch. They do not look even remotely close to valid gradient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "ADEV is a new algorithm that computes correct gradient estimates of expectations of probabilistic programs. It  accounts for the change to the expectation coming from a change to the randomness present in the expectation.\n",
    "\n",
    "GenJAX implements ADEV. Slightly rewriting the example from above using GenJAX, we can see how different the behaviour of the optimization process with the corrected gradient estimates is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@expectation\n",
    "def flip_approx_loss(theta, sigma):\n",
    "    b = flip_enum(theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda theta: normal_reparam(0.0, sigma) * theta,\n",
    "        lambda theta: normal_reparam(theta / 2, sigma),\n",
    "        theta,\n",
    "    )\n",
    "\n",
    "\n",
    "adev_grad = jax.jit(flip_approx_loss.jvp_estimate)\n",
    "\n",
    "\n",
    "def compute_jax_vals(key, initial_theta, sigma):\n",
    "    current_theta = initial_theta\n",
    "    out = []\n",
    "    for _ in range(EPOCHS):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        gradient = jax_grad(subkey, current_theta, sigma)\n",
    "        out.append((current_theta, expected_val(current_theta), gradient))\n",
    "        current_theta = current_theta + 0.01 * gradient\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_adev_vals(key, initial_theta, sigma):\n",
    "    current_theta = initial_theta\n",
    "    out = []\n",
    "    for _ in range(EPOCHS):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        gradient = adev_grad(\n",
    "            subkey, (Dual(current_theta, 1.0), Dual(sigma, 0.0))\n",
    "        ).tangent\n",
    "        out.append((current_theta, expected_val(current_theta), gradient))\n",
    "        current_theta = current_theta + 0.01 * gradient\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_evenly_spaced(items, num_samples=5):\n",
    "    \"\"\"Select evenly spaced items from a list.\"\"\"\n",
    "    if num_samples <= 1:\n",
    "        return [items[0]]\n",
    "\n",
    "    result = [items[0]]\n",
    "    step = (len(items) - 1) / (num_samples - 1)\n",
    "\n",
    "    for i in range(1, num_samples - 1):\n",
    "        index = int(i * step)\n",
    "        result.append(items[index])\n",
    "\n",
    "    result.append(items[-1])\n",
    "    return result\n",
    "\n",
    "\n",
    "INITIAL_VAL = 0.2\n",
    "SLIDER_STEP = 0.01\n",
    "ANIMATION_STEP = 4\n",
    "\n",
    "\n",
    "button_classes = (\n",
    "    \"px-3 py-1 bg-blue-500 text-white text-sm font-medium rounded-md hover:bg-blue-600\"\n",
    ")\n",
    "\n",
    "\n",
    "def input_slider(label, value, min, max, step, on_change, default):\n",
    "    return [\n",
    "        \"label.flex.flex-col.gap-2\",\n",
    "        [\"div\", label, [\"span.font-bold.px-1\", value]],\n",
    "        [\n",
    "            \"input\",\n",
    "            {\n",
    "                \"type\": \"range\",\n",
    "                \"min\": min,\n",
    "                \"max\": max,\n",
    "                \"step\": step,\n",
    "                \"defaultValue\": default,\n",
    "                \"onChange\": on_change,\n",
    "                \"class\": \"outline-none focus:outline-none\",\n",
    "            },\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "\n",
    "def input_checkbox(label, value, on_change):\n",
    "    return [\n",
    "        \"label.flex.items-center.gap-2\",\n",
    "        [\n",
    "            \"input\",\n",
    "            {\n",
    "                \"type\": \"checkbox\",\n",
    "                \"checked\": value,\n",
    "                \"onChange\": on_change,\n",
    "                \"class\": \"h-4 w-4 rounded border-gray-300 text-blue-600 focus:ring-blue-500\",\n",
    "            },\n",
    "        ],\n",
    "        label,\n",
    "        [\"span.font-bold.px-1\", value],\n",
    "    ]\n",
    "\n",
    "\n",
    "def render_plot(initial_val, initial_sigma):\n",
    "    SLIDER_STEP = 0.01\n",
    "    ANIMATION_STEP = 4\n",
    "    COMPARISON_HEIGHT = 200\n",
    "    currentKey = key\n",
    "\n",
    "    def computeState(val, sigma):\n",
    "        jax_key, adev_key, samples_key = jax.random.split(currentKey, num=3)\n",
    "        return {\n",
    "            \"JAX_gradients\": compute_jax_vals(jax_key, val, sigma),\n",
    "            \"ADEV_gradients\": compute_adev_vals(adev_key, val, sigma),\n",
    "            \"samples\": make_samples(samples_key, thetas, sigma),\n",
    "            \"val\": val,\n",
    "            \"sigma\": sigma,\n",
    "            \"frame\": 0,\n",
    "        }\n",
    "\n",
    "    initialState = Plot.initialState(\n",
    "        computeState(initial_val, initial_sigma) | {\"show_expected_value\": True},\n",
    "        sync={\"sigma\", \"val\"},\n",
    "    )\n",
    "\n",
    "    def refresh(widget):\n",
    "        nonlocal currentKey\n",
    "        currentKey = jax.random.split(currentKey)[0]\n",
    "        widget.state.update(computeState(widget.state.val, widget.state.sigma))\n",
    "\n",
    "    onChange = Plot.onChange({\n",
    "        \"val\": lambda widget, e: widget.state.update(\n",
    "            computeState(float(e[\"value\"]), widget.state.sigma)\n",
    "        ),\n",
    "        \"sigma\": lambda widget, e: widget.state.update(\n",
    "            computeState(widget.state.val, float(e[\"value\"]))\n",
    "        ),\n",
    "    })\n",
    "\n",
    "    samples_plot = make_samples_plot(thetas, js(\"$state.samples\"))\n",
    "\n",
    "    def plot_tangents(gradients_id):\n",
    "        tangents_plots = Plot.new(Plot.aspectRatio(0.5))\n",
    "        color = \"blue\" if gradients_id == \"ADEV\" else \"orange\"\n",
    "\n",
    "        orange_to_red_plot = Plot.dot(\n",
    "            js(f\"$state.{gradients_id}_gradients\"),\n",
    "            x=\"0\",\n",
    "            y=\"1\",\n",
    "            fill=js(\n",
    "                f\"\"\"(_, i) => d3.interpolateHsl('transparent', '{color}')(i/{EPOCHS})\"\"\"\n",
    "            ),\n",
    "            filter=(js(\"(d, i) => i <= $state.frame\")),\n",
    "        )\n",
    "\n",
    "        tangents_plots += orange_to_red_plot\n",
    "\n",
    "        tangents_plots += Plot.line(\n",
    "            js(f\"\"\"$state.{gradients_id}_gradients.flatMap(([theta, expected_val, slope], i) => {{\n",
    "                        const y_intercept = expected_val - slope * theta\n",
    "                        return [[0, y_intercept, i], [1, slope + y_intercept, i]]\n",
    "                    }})\n",
    "                    \"\"\"),\n",
    "            z=\"2\",\n",
    "            stroke=Plot.constantly(f\"{gradients_id} Tangent\"),\n",
    "            opacity=js(\"(data) => data[2] === $state.frame ? 1 : 0.5\"),\n",
    "            strokeWidth=js(\"(data) => data[2] === $state.frame ? 3 : 1\"),\n",
    "            filter=js(f\"\"\"(data) => {{\n",
    "                const index = data[2];\n",
    "                if (index === $state.frame) return true;\n",
    "                if (index < $state.frame) {{\n",
    "                    const step = Math.floor({EPOCHS} / 10);\n",
    "                    return (index % step === 0);\n",
    "                }}\n",
    "                return false;\n",
    "            }}\"\"\"),\n",
    "        )\n",
    "\n",
    "        return Plot.new(\n",
    "            js(\"$state.show_expected_value ? %1 : null\", expected_value_plot),\n",
    "            Plot.domain([0, 1], [0, 0.4]),\n",
    "            tangents_plots,\n",
    "            Plot.title(f\"{gradients_id} Gradient Estimates\"),\n",
    "            Plot.color_map({\"JAX Tangent\": \"orange\", \"ADEV Tangent\": \"blue\"}),\n",
    "        )\n",
    "\n",
    "    comparison_plot = (\n",
    "        Plot.line(\n",
    "            js(\"$state.JAX_gradients.slice(0, $state.frame+1)\"),\n",
    "            x=Plot.index(),\n",
    "            y=\"2\",\n",
    "            stroke=Plot.constantly(\"Gradients from JAX\"),\n",
    "        )\n",
    "        + Plot.line(\n",
    "            js(\"$state.ADEV_gradients.slice(0, $state.frame+1)\"),\n",
    "            x=Plot.index(),\n",
    "            y=\"2\",\n",
    "            stroke=Plot.constantly(\"Gradients from ADEV\"),\n",
    "        )\n",
    "        + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    "        + Plot.domainX([0, EPOCHS])\n",
    "        + Plot.title(\"Comparison of computed gradients JAX vs ADEV\")\n",
    "        + Plot.color_legend()\n",
    "        + {\"height\": COMPARISON_HEIGHT}\n",
    "    )\n",
    "\n",
    "    optimization_plot = Plot.new(\n",
    "        Plot.line(\n",
    "            js(\"$state.JAX_gradients\"),\n",
    "            x=Plot.index(),\n",
    "            y=\"1\",\n",
    "            stroke=Plot.constantly(\"Gradient ascent with JAX\"),\n",
    "            filter=js(\"(d, i) => i <= $state.frame\"),\n",
    "        )\n",
    "        + Plot.line(\n",
    "            js(\"$state.ADEV_gradients\"),\n",
    "            x=Plot.index(),\n",
    "            y=\"1\",\n",
    "            stroke=Plot.constantly(\"Gradient ascent with ADEV\"),\n",
    "            filter=js(\"(d, i) => i <= $state.frame\"),\n",
    "        )\n",
    "        + {\n",
    "            \"x\": {\"label\": \"Iteration\"},\n",
    "            \"y\": {\"label\": \"Expected Value\"},\n",
    "        }\n",
    "        + Plot.domainX([0, EPOCHS])\n",
    "        + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n",
    "        + Plot.color_legend()\n",
    "        + {\"height\": COMPARISON_HEIGHT}\n",
    "    )\n",
    "\n",
    "    jax_tangents_plot = samples_plot + plot_tangents(\"JAX\")\n",
    "    adev_tangents_plot = samples_plot + plot_tangents(\"ADEV\")\n",
    "\n",
    "    frame_slider = Plot.Slider(\n",
    "        key=\"frame\",\n",
    "        init=0,\n",
    "        range=[0, EPOCHS],\n",
    "        step=ANIMATION_STEP,\n",
    "        fps=30,\n",
    "        label=\"Iteration:\",\n",
    "    )\n",
    "\n",
    "    controls = Plot.html([\n",
    "        \"div.flex.mb-3.gap-4.bg-gray-200.rounded-md.p-3\",\n",
    "        [\n",
    "            \"div.flex.flex-col.gap-1.w-32\",\n",
    "            input_slider(\n",
    "                label=\"Initial Value:\",\n",
    "                value=js(\"$state.val\"),\n",
    "                min=0,\n",
    "                max=1,\n",
    "                step=SLIDER_STEP,\n",
    "                on_change=js(\"(e) => $state.val = parseFloat(e.target.value)\"),\n",
    "                default=initial_val,\n",
    "            ),\n",
    "            input_slider(\n",
    "                label=\"Sigma:\",\n",
    "                value=js(\"$state.sigma\"),\n",
    "                min=0,\n",
    "                max=0.2,\n",
    "                step=0.01,\n",
    "                on_change=js(\"(e) => $state.sigma = parseFloat(e.target.value)\"),\n",
    "                default=initial_sigma,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            \"div.flex.flex-col.gap-2.flex-auto\",\n",
    "            input_checkbox(\n",
    "                label=\"Show expected value\",\n",
    "                value=js(\"$state.show_expected_value\"),\n",
    "                on_change=js(\"(e) => $state.show_expected_value = e.target.checked\"),\n",
    "            ),\n",
    "            Plot.katex(r\"\"\"\n",
    "y(\\theta) = \\mathbb{E}_{x\\sim P(\\theta)}[x] = \\int_{\\mathbb{R}}\\left[\\theta^2\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\left(\\frac{x}{\\sigma}\\right)^2} + (1-\\theta)\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\left(\\frac{x-0.5\\theta}{\\sigma}\\right)^2}\\right]dx =\\frac{\\theta-\\theta^2}{2}\n",
    "        \"\"\"),\n",
    "            [\n",
    "                \"button.w-32\",\n",
    "                {\n",
    "                    \"onClick\": lambda widget, e: refresh(widget),\n",
    "                    \"class\": button_classes,\n",
    "                },\n",
    "                \"Refresh\",\n",
    "            ],\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "    jax_code = \"\"\"def noisy_jax_model(key, theta, sigma):\n",
    "    b = jax.random.bernoulli(key, theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda theta: jax.random.normal(key) * sigma * theta,\n",
    "        lambda theta: jax.random.normal(key) * sigma + theta / 2,\n",
    "        theta,\n",
    "    )\"\"\"\n",
    "\n",
    "    adev_code = \"\"\"@expectation\n",
    "def flip_approx_loss(theta, sigma):\n",
    "    b = flip_enum(theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda theta: normal_reparam(0.0, sigma) * theta,\n",
    "        lambda theta: normal_reparam(theta / 2, sigma),\n",
    "        theta,\n",
    "    )\"\"\"\n",
    "\n",
    "    GRID = \"div.grid.grid-cols-2.gap-4\"\n",
    "    PRE = \"pre.whitespace-pre-wrap.text-2xs.p-3.rounded-md.bg-gray-100.flex-1\"\n",
    "\n",
    "    return (\n",
    "        initialState\n",
    "        | onChange\n",
    "        | controls\n",
    "        | [\n",
    "            GRID,\n",
    "            jax_tangents_plot,\n",
    "            adev_tangents_plot,\n",
    "            [PRE, jax_code],\n",
    "            [PRE, adev_code],\n",
    "            comparison_plot,\n",
    "            optimization_plot,\n",
    "        ]\n",
    "        | frame_slider\n",
    "    )\n",
    "\n",
    "\n",
    "render_plot(0.2, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In the above example, by using `jvp_estimate` we used a forward-mode version of ADEV. GenJAX also supports a reverse-mode version which is also fully compatible with JAX and can be jitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@expectation\n",
    "def flip_exact_loss(theta):\n",
    "    b = flip_enum(theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda _: 0.0,\n",
    "        lambda _: -theta / 2.0,\n",
    "        theta,\n",
    "    )\n",
    "\n",
    "\n",
    "rev_adev_grad = jax.jit(flip_exact_loss.grad_estimate)\n",
    "\n",
    "arg = 0.2\n",
    "rev_adev_vals = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    (grad_val,) = rev_adev_grad(subkey, (arg,))\n",
    "    arg = arg - 0.01 * grad_val\n",
    "    rev_adev_vals.append(expected_val(arg))\n",
    "\n",
    "(\n",
    "    Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": rev_adev_vals},\n",
    "        stroke=Plot.constantly(\"Reverse mode ADEV\"),\n",
    "    )\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    "    + Plot.domainX([0, EPOCHS])\n",
    "    + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n",
    "    + Plot.color_legend()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
