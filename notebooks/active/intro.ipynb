{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "subtitle: Morning coffee & first steps from the probabilistic base camp.\n",
    "title: Generative functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenJAX is a swiss army knife for probabilistic machine learning: it's designed to support probabilistic modeling workflows, and to make the resulting code extremely fast and parallelizable via JAX.\n",
    "\n",
    "In this introduction, we'll focus on one such workflow: writing a latent variable model (we often say: a generative model) which describes a probability distribution over latent variables and data, and then asking questions about the conditional distribution over the latent variables given data.\n",
    "\n",
    "In the following, we'll often shorten GenJAX to Gen -- because [GenJAX implements Gen](https://www.gen.dev/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from genjax import gen, normal, pretty\n",
    "from jax import jit, vmap\n",
    "from jax import random as jrand\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.rcParams[\"figure.facecolor\"] = \"none\"\n",
    "plt.rcParams[\"savefig.transparent\"] = True\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "pretty()  # pretty print the types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def model():\n",
    "    x = normal(0.0, 1.0) @ \"x\"\n",
    "    normal(x, 1.0) @ \"y\"\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gen, probabilistic models are represented by a computational object called _a generative function_. Once we create one of these objects, we can use one of several interfaces to gain access to probabilistic effects.\n",
    "\n",
    "Here's one interface: `simulate` -- this samples from the probability distribution which the program represents, and stores the result, along with other data about the invocation of the function, in a data structure called a `Trace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jrand.PRNGKey(0)\n",
    "tr = model.simulate(key, ())\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dig around in this object uses its interfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = tr.get_sample()\n",
    "chm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ChoiceMap` is a representation of _the sample_ from the probability distribution which the generative function represents. We can ask _what values were sampled_ at the addresses (the `\"x\"` and `\"y\"` syntax in our model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(chm[\"x\"], chm[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat -- all of our interfaces are JAX compatible, so we could sample 1000 times just by using `jax.vmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = jrand.split(jrand.PRNGKey(0), 1000)\n",
    "tr = jit(vmap(model.simulate, in_axes=(0, None)))(sub_keys, ())\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our samples to get a sense of the distribution we wrote down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = tr.get_sample()\n",
    "plt.scatter(chm[\"x\"], chm[\"y\"], marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traces also keep track of other data, like _the score_ of the execution (which is a value which estimates the joint probability of the random choices under the distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.get_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition of generative functions\n",
    "\n",
    "Generative functions are probabilistic building blocks. You can combine them into larger probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regression distribution.\n",
    "@gen\n",
    "def regression(x, coefficients, sigma):\n",
    "    basis_value = jnp.array([1.0, x, x**2])\n",
    "    polynomial_value = jnp.sum(basis_value * coefficients)\n",
    "    y = genjax.normal(polynomial_value, sigma) @ \"v\"\n",
    "    return y\n",
    "\n",
    "\n",
    "# Regression, with an outlier random variable.\n",
    "@gen\n",
    "def regression_with_outlier(x, coefficients):\n",
    "    is_outlier = genjax.flip(0.1) @ \"is_outlier\"\n",
    "    sigma = jnp.where(is_outlier, 30.0, 0.3)\n",
    "    is_outlier = jnp.array(is_outlier, dtype=int)\n",
    "    return regression(x, coefficients, sigma) @ \"y\"\n",
    "\n",
    "\n",
    "# The full model, sample coefficients for a curve, and then use\n",
    "# them in independent draws from the regression submodel.\n",
    "@gen\n",
    "def full_model(xs):\n",
    "    coefficients = (\n",
    "        genjax.mv_normal(\n",
    "            jnp.zeros(3, dtype=float),\n",
    "            2.0 * jnp.identity(3),\n",
    "        )\n",
    "        @ \"alpha\"\n",
    "    )\n",
    "    ys = regression_with_outlier.vmap(in_axes=(0, None))(xs, coefficients) @ \"ys\"\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine a sample from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.arange(0, 10, 0.5)\n",
    "full_model.simulate(key, (data,)).get_sample()[\"ys\", ..., \"y\", \"v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a few such samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(ax, x, y, **kwargs):\n",
    "    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(3, 3, figsize=(8, 8), sharex=True, sharey=True)\n",
    "jitted = jit(full_model.simulate)\n",
    "for ax in axes.flatten():\n",
    "    key, sub_key = jrand.split(key)\n",
    "    tr = jitted(key, (data,))\n",
    "    x = data\n",
    "    y = tr.get_retval()\n",
    "    viz(ax, x, y, marker=\".\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are samples from the distribution _over curves_ which our generative function represents.\n",
    "\n",
    "## Inference in generative functions\n",
    "\n",
    "So we've written a regression model, a distribution over curves. Our model includes an outlier component. If we observe some data for `\"y\"`, can we predict which points might be outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\n",
    "y = 2.0 * x + 1.5 + x**2\n",
    "y = y.at[2].set(50.0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've explored how generative functions represent joint distributions over random variables, but what about distributions induced by inference problems?\n",
    "\n",
    "We can create an inference problem by pairing a generative function with arguments, and _a constraint_.\n",
    "\n",
    "First, let's learn how to create one type of constraint -- a _choice map_ sample, just like the choice maps we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax import ChoiceMapBuilder as C\n",
    "\n",
    "chm = vmap(lambda idx, v: C[\"ys\", idx, \"y\", \"v\"].set(v))(jnp.arange(len(y)), y)\n",
    "chm[\"ys\", ..., \"y\", \"v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice map holds the _value constraint_ for the distributions we used in our generative function. Choice maps are a lot like arrays, with a bit of extra metadata.\n",
    "\n",
    "Now, we can specify an inference target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax import Target\n",
    "\n",
    "target = Target(full_model, (x,), chm)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Target` represents an unnormalized distribution -- in this case, the posterior of the distribution represented by our generative function with arguments `args = (x, )`.\n",
    "\n",
    "Now, we can approximate the solution to the inference problem using an inference algorithm. GenJAX exposes a standard library of approximate inference algorithms: let's use $K$-particle importance sampling for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax.inference.smc import ImportanceK\n",
    "\n",
    "alg = ImportanceK(target, k_particles=100)\n",
    "alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = jrand.split(key, 50)\n",
    "posterior_samples = jit(vmap(alg.simulate, in_axes=(0, None)))(\n",
    "    sub_keys, (target,)\n",
    ").get_retval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With samples from our approximate posterior in hand, we can check queries like \"estimate the probability that a point is an outlier\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"ys\", ..., \"is_outlier\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that our approximate posterior assigns high probability to the query \"the 3rd data point is an outlier\". Remember, we set this point to be far away from the other points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"ys\", ..., \"is_outlier\"].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the sampled curves against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_at_x(x, coefficients):\n",
    "    basis_values = jnp.array([1.0, x, x**2])\n",
    "    polynomial_value = jnp.sum(coefficients * basis_values)\n",
    "    return polynomial_value\n",
    "\n",
    "\n",
    "jitted = jit(vmap(polynomial_at_x, in_axes=(None, 0)))\n",
    "\n",
    "\n",
    "def plot_polynomial_values(ax, x, coefficients, **kwargs):\n",
    "    v = jitted(x, coefficients)\n",
    "    ax.scatter(jnp.repeat(x, len(v)), v, **kwargs)\n",
    "\n",
    "\n",
    "fig_data, ax_data = plt.subplots(figsize=(6, 6))\n",
    "viz(ax_data, x, y, color=\"blue\")\n",
    "coefficients = posterior_samples[\"alpha\"]\n",
    "evaluation_points = jnp.arange(0, 5, 0.01)\n",
    "for data in evaluation_points:\n",
    "    plot_polynomial_values(\n",
    "        ax_data, data, coefficients, marker=\".\", color=\"gold\", alpha=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We’ve covered a lot of ground in this notebook. Please reflect, re-read, and post issues!\n",
    "\n",
    "* We discussed generative functions - the main computational object of Gen, and how these objects represent probability distributions.\n",
    "* We showed how to create generative functions.\n",
    "* We showed how to use interfaces on generative functions to compute with common operations on distributions.\n",
    "* We created a generative function to model a data-generating process based on sampling and evaluating random polynomials on input data - representing regression task.\n",
    "* We showed how to create _inference problems_ from generative functions.\n",
    "* We created an inference problem from our regression model.\n",
    "* We showed how to create approximate inference solutions to inference problems, and sample from them.\n",
    "* We investigated the approximate posterior samples, and visually inspected that they match the inferences that we might draw - both for the polynomials we expected to produce the data, as well as what data points might be outliers.\n",
    "\n",
    "This is just the beginning! There’s a lot more to learn, but this is plenty to chew (for now).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
