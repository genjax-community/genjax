{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b923c964-5666-42c8-aada-48971300f519",
   "metadata": {},
   "source": [
    "---\n",
    "title: Introduction to Gen and GenJAX\n",
    "date: \"December 3, 2023\"\n",
    "abstract: This notebook is designed to give the reader an accelerated introduction to concepts native to trace-based probabilistic programming, Gen, and GenJAX (an implementation of Gen on top of JAX). For pre-requisites, the notebook assumes some familiarity with trace-based probabilistic programming systems, and Monte Carlo inference - especially importance sampling (although we describe concepts when we need them).\n",
    "callout-appearance: simple\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa8f2c-3038-4030-ba78-6d0f28c2b5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ea0d6-22d6-4a93-8951-7ed7bd91297b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from genjax import ChoiceMap as C\n",
    "from genjax import gen\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Reproducibility.\n",
    "key = jax.random.PRNGKey(314159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f72209-dddd-45fe-a1f5-81d55c0530ac",
   "metadata": {},
   "source": [
    "## What is GenJAX?\n",
    "\n",
    "Here are a few high-level ways to think about GenJAX:\n",
    "\n",
    "* A probabilistic programming^[New to probabilistic programming? [Don't fret, read on!](#what-is-probabilistic-programming)] system based on the concepts of [Gen](https://www.gen.dev/).\n",
    "\n",
    "* A Bayesian modelling and inference compiler with support for device acceleration (courtesy of JAX).\n",
    "\n",
    "* A base layer for experiments in model and inference DSL design.\n",
    "\n",
    "There's already a well-supported implementation of [Gen in Julia](https://github.com/probcomp/Gen.jl). Why is a JAX port interesting?\n",
    "\n",
    "There are a number of compelling technical and social reasons to explore Gen's probabilistic programming paradigm on top of JAX, here are a few:\n",
    "\n",
    "* JAX's excellent accelerator support - our implementation natively supports several common accelerator idioms - like automatic struct-of-array representations, and the ability to automatically batch model/inference programs onto accelerators.\n",
    "\n",
    "* JAX's excellent support for compositional AD removes implementation and maintenance complexity for Gen's gradient interfaces - previously a difficult challenge in other implementations. In addition, JAX's support for convenient, higher-order AD opens up new opportunities to explore during inference design with gradient interfaces.\n",
    "\n",
    "* JAX exposes compositional code transformations to library authors, and, as library authors, we can utilize code transformations to implement state-of-the-art optimizations for models and inference expressed in our system.\n",
    "\n",
    "* A lot of domain experts and modelers are working in Python! Some of them even use JAX (hopefully more each year). Presenting an interface to Gen which is familiar, and takes advantage of JAX's native ecosystem is a compelling social reason.\n",
    "\n",
    "Let's truncate the list there for now.\n",
    "\n",
    "For the JAX literati, one final (hopefully tantalizing) takeaway: <u>by construction, all GenJAX modeling + inference code is JAX traceable</u> - and thus, `vmap`able, `jit`able, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843a431-5554-419a-bdb7-3df9fc6e9dab",
   "metadata": {},
   "source": [
    "## What is probabilistic programming?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293349b-0d3e-4b4f-b27d-74208a52752c",
   "metadata": {},
   "source": [
    "Perhaps you may be coming to this notebook without any prior knowledge in probabilistic programming...\n",
    "\n",
    "That's okay! Ideally, the ideas in this notebook should be self-contained^[You may miss _why generative functions (see below) are designed the way they are_ on a first read - but you'll still get the punchline if you follow the notebook to the end.].\n",
    "\n",
    "### A Bayesian viewpoint\n",
    "\n",
    "Here's one practical take on what probabilistic programming is all about: programming language design for expressing and solving Bayesian inference problems^[In the [Probabilistic Computing lab at MIT](http://probcomp.csail.mit.edu/), we also consider differentiable programming to be contained within the set of concerns of probabilistic programming. We won't cover differentiable programming interfaces in this notebook.]. \n",
    "\n",
    "Probabilistic programming is a broad field, and there are corners which may not be covered by this practical take. We'll just assume that people are interested in Bayes, and how to represent Bayes on computers in nice ways. For our purposes in this notebook, we'll stick as much as we can to the basics.\n",
    "\n",
    "### What are we actually computing with?\n",
    "\n",
    "The objects which we program with expose a mixture of generative and differentiable interfaces - the interfaces are designed to support common (as well as quite advanced) classes of Bayesian inference algorithms. Gen provides automation for the tricky math which these algorithms require.\n",
    "\n",
    "We separate the design of inference (whose implementation uses the interfaces), from the implementation of the interfaces on computational objects. This allows us to build languages of objects which satisfy the interfaces - and allows their compositional usage and interoperability.\n",
    "\n",
    "In Gen, the objects which implement the interfaces are called **generative functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cc347-c669-4c55-9818-33083a38725b",
   "metadata": {},
   "source": [
    "## What is a generative function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315530d-5be7-45c3-9b84-64f22980d667",
   "metadata": {},
   "source": [
    "Generative functions are the key concept of Gen's probabilistic programming paradigm. Generative functions are computational objects defined by a set of associated data types and methods. These types and methods describe compositional interfaces that are useful for Bayesian inference computations. \n",
    "\n",
    "Gen's formal description of generative functions consist of two objects:\n",
    "\n",
    "* $P(\\tau, r; x)$ - a normalized measure over tree-like data (*choice maps*) and untraced randomness^[More on this later. It's safe to say \"I have no idea what that is\" for now, and expect us to explain later or in another notebook.] $r$, parametrized by arguments $x$.\n",
    "\n",
    "* $f(\\tau; x)$ - a deterministic function from the above measure's sample space to a space of data types.\n",
    "\n",
    "We can informally think of the sampling semantics of these objects as consisting of two steps:\n",
    "\n",
    "1. First, sample a  choice map from $P$.\n",
    "2. Then, compute the return value using $f$.\n",
    "\n",
    "In many of the generative function interfaces, we won't just be interested in the final sampled return value. We'll also be interested in what happened along the way: we'll record the intermediate and final results of these steps in `Trace` objects - data structures which contain the recordings of values, along with probabilistic metadata like the score of random choices selected along the way.\n",
    "\n",
    "Below, we provide an example of an (admittedly, not too interesting) GenJAX generative function^[There's not just one generative function class - users can and are encouraged to design new types of generative functions which capture repeated modeling patterns. An excellent example of this modularity in Gen's design is [generative function combinators](/introduction/intro_to_combinators/intro_to_combinators).]. \n",
    "\n",
    "This generative function is part of a function-like language (the `BuiltinGenerativeFunction` language) - pay close attention to the hierarchical compositionality of generative functions in this language under an abstraction (`genjax.trace`) similar to a function call. We'll discuss the addresses (`\"sub\"` and `\"m0\"`) a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfac5fa-39da-4f60-9b5b-0454cd50da7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@gen\n",
    "def g(x):\n",
    "    m0 = genjax.bernoulli(x) @ \"m0\"  # unsweetened\n",
    "    return m0\n",
    "\n",
    "\n",
    "@gen\n",
    "def h(x):\n",
    "    m0 = g(x) @ \"sub\"  # sweetened\n",
    "    return m0\n",
    "\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73438a-9e9d-40ea-b193-829dab36cd6a",
   "metadata": {},
   "source": [
    "This generative function holds a Python `Callable` object. For this generative function language, the interface methods (see the list under **Generative function interface** below) which are useful for modeling and inference are given semantics via JAX's tracing and program transformation infrastructure.\n",
    "\n",
    "Let's examine some of these operations now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf8a3e-d095-4b89-9151-b5fb3f43b12a",
   "metadata": {},
   "source": [
    "This is our first glimpse of the **generative function interface** (GFI), the secret sauce which Gen is based around.\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "## JAX interfaces\n",
    "\n",
    "There's a few methods which you might see which are not explicitly part of Gen's GFI, but are worth mentioning because they deal with data interfaces to JAX:\n",
    "\n",
    "* `flatten` - which allows us to treat generative functions as [Pytree](https://jax.readthedocs.io/en/latest/pytrees.html) implementors.\n",
    "* `unflatten` - same as above.\n",
    "\n",
    "These are used to register the implementor type as a `Pytree`, which is roughly a tree-like Python structure which JAX can zip/unzip at runtime API boundaries.\n",
    "\n",
    ":::\n",
    "\n",
    "Let's study the `simulate` method first: we'll explore its semantics, and see the types of data it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278e990-e168-4bf1-9dfc-13e8c33d3745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "tr = h.simulate(sub_key, (0.3,))\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df36a11-ead4-4cb1-a632-d71cf3f4fb6a",
   "metadata": {},
   "source": [
    "If you're familiar with other \"trace-based\" probabilistic systems - this should look familiar. \n",
    "\n",
    "This object instance is a piece of data which has captured information about the execution of the function. Specifically, the subtraces of _other generative function calls_ which occur in `genjax.trace` statements.\n",
    "\n",
    "It also captures the `score` - the log probability of the normalized measure which the model program represents, evaluated at the random choices which the generative call execution produced. \n",
    "\n",
    "If you were paying attention above, the score is $\\log P(\\tau, r; x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6466a02a-bb3e-4753-b83c-d503ed51a4d5",
   "metadata": {},
   "source": [
    "### How is `simulate` implemented for this language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5168c-080d-4111-83c6-b557c625e83c",
   "metadata": {},
   "source": [
    "For this generative function language, we implement `simulate` using a code transformation! Here's the transformed code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a323c57-f69d-4e91-b72a-3d1a6bf8cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaxpr = jax.make_jaxpr(h.simulate)(key, (0.3,))\n",
    "jaxpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5bc97a-3704-43a0-a827-c202270a0b72",
   "metadata": {},
   "source": [
    "That's a lot of code! This code is pure, numerical, and ready for acceleration. By utilizing JAX and a staging transformation, we've stripped out all Python overhead.\n",
    "\n",
    "This is how we've implemented `simulate` for this particular generative function language.^[In general, Gen doesn't require that we follow the same \"JAX code transformation\" implementation for other generative function languages. The relationship with JAX in GenJAX, however, is a bit special - often, we assume that the user is working within the JAX traceable subset of Python - hence, generative function interface implementations which are JAX traceable benefit from composition with other JAX compatible modeling structures.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1c4fa-6a37-46ee-aa10-dabde03d31af",
   "metadata": {},
   "source": [
    "## Generative function interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3d518-1ca8-43f4-b4c6-b4994801af2c",
   "metadata": {},
   "source": [
    "There are a few more generative function interface methods worth discussing.\n",
    "\n",
    "In this notebook, instead of carefully walking through the math which these interface methods compute, we'll defer that discussion to another notebook. Below, we give an informal discussion of what each of the interface methods computes, and roughly describe what algorithm families are supported by their usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ddad3-eb9f-48ba-a2bd-0c766c4eb9c7",
   "metadata": {},
   "source": [
    "### The generative function interface in GenJAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2cc0c8-ea7c-438c-aa3d-ad48831888e9",
   "metadata": {},
   "source": [
    "GenJAX's generative functions define an interface which support compositional usage of generative functions within other generative functions. The interface functions here closely mirror [the interfaces defined in Gen.jl](https://www.gen.dev/docs/stable/ref/gfi/#Generative-function-interface-1).\n",
    "\n",
    "In the following, we use the following abbreviations:\n",
    "\n",
    "* **IS** - importance sampling\n",
    "* **SMC** - sequential Monte Carlo\n",
    "* **MCMC** - Markov chain Monte Carlo\n",
    "* **VI** - variational inference\n",
    "\n",
    "| Interface | Type | Inference algorithm support |\n",
    "| --- | --- | --- |\n",
    "| `simulate` | Generative | IS, SMC |\n",
    "| `importance` | Generative | IS, SMC, VI |\n",
    "| `update` | Generative and incremental | MCMC, SMC |\n",
    "| `assess` | Generative and differentiable | MCMC, IS, SMC |\n",
    "\n",
    "This interface supports several methods - I've roughly described them and split them into the two categories **Generative** and **Differentiable** below:\n",
    "\n",
    "#### Generative\n",
    "\n",
    "* `simulate` - sample from normalized trace measure, and return the score.\n",
    "* `importance` - given constraints for some addresses, sample from unnormalized trace measure and return an importance weight.\n",
    "* `update` - given an existing trace, and a set of constraints and argument change values, update the trace to be consistent with the set of constraints under execution with the new arguments, and return an incremental importance weight.\n",
    "* `assess` - given a complete choice map and arguments, return the normalized log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657faf34-b67d-46a4-8f24-fdc79cecfcf3",
   "metadata": {},
   "source": [
    "## More about generative functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db80f2-ac3e-4cd4-b1e0-590fb63c7e37",
   "metadata": {},
   "source": [
    "Here are a few more bits of information which should help you gain context with these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841abef-c0a6-43f4-970d-b8b8626b53b3",
   "metadata": {},
   "source": [
    "### Distributions are generative functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d061e8-9db9-4645-8ae5-fe91b7627da7",
   "metadata": {},
   "source": [
    "In GenJAX, distributions are generative functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200ee17-1c9d-48b9-a978-e71f5b308b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "tr = genjax.normal.simulate(sub_key, (0.0, 1.0))\n",
    "tr.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15ac63-bc1f-45d1-b91d-fa8bec006f9f",
   "metadata": {},
   "source": [
    "This should bring a sigh of relief! Ah, distributions are generative functions - *the concepts can't be too exotic*.\n",
    "\n",
    "Distributions implement the interface in a conceptually simple way. They don't have internal compositional choice structure (like the function-like `BuiltinGenerativeFunction` language above).\n",
    "\n",
    "Distributions themselves expose two interfaces:\n",
    "\n",
    "* `logpdf` - exact density evaluation.\n",
    "* `sample` - exact sampling.\n",
    "\n",
    "We can use these two interfaces to implement all the generative function interfaces for distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2897f6-a5b3-4acc-92bf-1c3365897012",
   "metadata": {},
   "source": [
    "### Associated data types\n",
    "\n",
    "* **Choice maps** are the tree-like representations of the values sampled at random choices inside of generative functions.\n",
    "* **Selections** are objects which allows querying a trace/choice map - filtering certain choices, projecting joint log score computations onto address contributions, even manipulating choice maps. For those familiar with functional programming - they present a lens-like interface on choice maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80644e14-30af-4b95-8299-22e8851852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def h(x):\n",
    "    m1 = genjax.bernoulli(x) @ \"m0\"\n",
    "    m2 = genjax.bernoulli(x) @ \"m1\"\n",
    "    return m1 + m2\n",
    "\n",
    "\n",
    "key, sub_key = jax.random.split(key)\n",
    "tr = h.simulate(sub_key, (0.3,))\n",
    "tr.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34981530-4df7-469d-9511-308dabb3a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = genjax.Selection.at[\"m1\"]\n",
    "selected = tr.get_choices().filter(selection)\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd63dc6-5a10-4f3c-a0f9-0c7f5ea08f9d",
   "metadata": {},
   "source": [
    "## What can I do with them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca246689-45e5-4db0-9792-76bf668b41d5",
   "metadata": {},
   "source": [
    "Now, we've informally seen the interfaces and datatypes associated with generative functions.\n",
    "\n",
    "Studying the interfaces (and improvements thereof), as well as the computational objects which satisfy them can be an entire PhD's worth of effort. \n",
    "\n",
    "In the remainder of this notebook, let's see how we can do machine learning with them.\n",
    "\n",
    "Let's consider a modeling problem where we wish to perform generalized regression with outliers between two variates, taking a family of polynomials as potential curves.\n",
    "\n",
    "One such model for this data generating process is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd85b36-3c0d-4c8f-ac8a-7c42b31db964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two branches for a branching submodel.\n",
    "@gen\n",
    "def model_y(x, coefficients):\n",
    "    basis_value = jnp.array([1.0, x, x**2])\n",
    "    polynomial_value = jnp.sum(basis_value * coefficients)\n",
    "    y = genjax.normal(polynomial_value, 0.3) @ \"value\"\n",
    "    return y\n",
    "\n",
    "\n",
    "@gen\n",
    "def outlier_model(x, coefficients):\n",
    "    basis_value = jnp.array([1.0, x, x**2])\n",
    "    polynomial_value = jnp.sum(basis_value * coefficients)\n",
    "    y = genjax.normal(polynomial_value, 30.0) @ \"value\"\n",
    "    return y\n",
    "\n",
    "\n",
    "# The branching submodel.\n",
    "switch = genjax.switch_combinator(model_y, outlier_model)\n",
    "\n",
    "# A mapped kernel function which calls the branching submodel.\n",
    "\n",
    "\n",
    "@genjax.vmap_combinator(in_axes=(0, None))\n",
    "@gen\n",
    "def kernel(x, coefficients):\n",
    "    is_outlier = genjax.flip(0.1) @ \"outlier\"\n",
    "    is_outlier = jnp.array(is_outlier, dtype=int)\n",
    "    y = switch(is_outlier, (x, coefficients), (x, coefficients)) @ \"y\"\n",
    "    return y\n",
    "\n",
    "\n",
    "@gen\n",
    "def model(xs):\n",
    "    coefficients = (\n",
    "        genjax.mv_normal(np.zeros(3, dtype=float), 2.0 * np.identity(3)) @ \"alpha\"\n",
    "    )\n",
    "    ys = kernel(xs, coefficients) @ \"ys\"\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c238a-e986-4f81-94e9-39ab5192e8f9",
   "metadata": {},
   "source": [
    "There's a few implementation patterns which you might pick up on by studying this model.\n",
    "\n",
    "1. To implement control flow, we use higher-order functions called combinators. These accept generative functions as input, and return generative functions as output.\n",
    "\n",
    "2. Any JAX compatible code is allowed in the body of a generative function.\n",
    "\n",
    "Courtesy of the interface, we get to design our `model` generative function in pieces.\n",
    "\n",
    "Now, let's examine the sampled observation address `(\"ys\", \"y\")` from a sample trace from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974bbe1-e255-4e44-a3fa-225f832745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.arange(0, 10, 0.5)\n",
    "key, sub_key = jax.random.split(key)\n",
    "tr = jax.jit(model.simulate)(sub_key, (data,))\n",
    "tr.get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8797b-5ae0-4c63-bc74-8d35e98dabcc",
   "metadata": {},
   "source": [
    "Here, I'm just showing a concise, pretty printed representation of the choice map -- but it doesn't tell us much about the values we truly care about here - the sampled values.\n",
    "\n",
    "From this model, we can get these in two ways. \n",
    "\n",
    "The first way: we can just look at the trace return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d5c96-acdf-40e9-b45b-e31dafab25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.get_retval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da0278-07b0-4907-8f9d-21e05e287f00",
   "metadata": {},
   "source": [
    "The second way: we can get them out of the choice map of the trace directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0a783-78f1-47f3-ba99-6286df4b7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = tr.get_choices()\n",
    "values = [chm[\"ys\", i, \"y\", \"value\"] for i in range(len(data))]\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b4d41-d78e-45f2-a64e-00b23ebb1903",
   "metadata": {},
   "source": [
    "Now, let's construct a small visualization function to show us the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180204fe-fed6-4dce-9de2-08226aa89ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ys(chm):\n",
    "    return jax.vmap(lambda v: chm[\"ys\", v, \"y\", \"value\"])(jnp.arange(0, len(data)))\n",
    "\n",
    "\n",
    "def viz(ax, x, y, **kwargs):\n",
    "    chm = tr.get_choices()\n",
    "    get_ys(chm)\n",
    "    sns.scatterplot(x=x, y=y, ax=ax, **kwargs)\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(3, 3, figsize=(8, 8), sharex=True, sharey=True)\n",
    "jitted = jax.jit(model.simulate)\n",
    "for ax in axes.flatten():\n",
    "    key, sub_key = jax.random.split(key)\n",
    "    tr = jitted(key, (data,))\n",
    "    x = data\n",
    "    y = tr.get_retval()\n",
    "    viz(ax, x, y, marker=\".\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e177c-3d07-40eb-aaed-f8e56d1d0718",
   "metadata": {},
   "source": [
    "These are the `(\"ys\", \"y\", \"value\")` samples for 9 traces from our model, against the points from the data we passed.\n",
    "\n",
    "We just walked through one of the main elements of probabilistic programming: setting up a program, which represents a joint distribution over random variates, some of which we'll identify with data we expect to see in the world.\n",
    "\n",
    "We can adjust the noise settings of our model to produce wider priors over possible sets of points - and we may want to do this if our data is noisy!\n",
    "\n",
    "For now, let's keep the settings as is, and explore inference in GenJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc83c5d-e59e-4bf4-9c90-12718c137c6d",
   "metadata": {},
   "source": [
    "## Your first inference program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15977ab4-0ef4-4927-87f9-02b41667fe88",
   "metadata": {},
   "source": [
    "Now, let's say we have some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8eaa5-2504-4a33-b994-0d627d2c2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\n",
    "y = 2.0 * x + 1.5 + x**2\n",
    "y[2] = 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8afa74-ecda-48b9-b808-cc9761311b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_data, ax_data = plt.subplots(figsize=(6, 6))\n",
    "viz(ax_data, x, y, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5253774-7395-45d8-98d3-7c3eef18a313",
   "metadata": {},
   "source": [
    "In Bayesian inference, if we wish to consider the conditional distribution $P(\\tau, r; x | \\text{data})$ induced from a model $P(\\tau, \\text{data}, r; x)$ - Bayes' rule gives us a way to compute it.\n",
    "\n",
    "$$\n",
    "P(\\tau, r; x | \\text{data}) = \\frac{P(\\tau, \\text{data}, r; x)}{\\int P(\\tau, \\text{data}, r; x) \\ d\\tau}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f2f04-b8b9-4f33-a15a-c31fa504c9dd",
   "metadata": {},
   "source": [
    "The problem is that we often cannot compute the denominator (_the evidence integral_) easily. Instead, we turn to _approximate_ Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1337b4-23db-4b00-87d6-d532cfdaad8e",
   "metadata": {},
   "source": [
    "Depending on how we wish to use the LHS conditional (which is called _the posterior_ in Bayesian inference) - we have different options available to us.\n",
    "\n",
    "If we wish to approximately sample from the posterior, to get an empirical sense of its shape and properties, we will often utilize techniques which provide exact samplers for another distribution which gets asymptotically close to the target posterior if we increase certain hyperparameters.\n",
    "\n",
    "One such algorithm is _importance sampling_, and that's what we'll write today.\n",
    "\n",
    "Here's importance sampling (with a single sample) without a custom proposal in GenJAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f569f-33db-402c-8a73-a7498deb8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = jax.vmap(lambda idx, v: C.n.at[\"ys\", idx, \"y\", \"value\"].set(v))(\n",
    "    jnp.arange(len(y)), y\n",
    ")\n",
    "key, sub_key = jax.random.split(key)\n",
    "(tr, w) = model.importance(sub_key, obs, (x,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa22dc-1c96-4e8f-afe6-bf3bc76ef823",
   "metadata": {},
   "source": [
    "We're introduced to another interface method! \n",
    "\n",
    "`importance` accepts a PRNG key, a choice map representing observations (sometimes called constraints), and model arguments. It returns a new evolved PRNG key, and a tuple contained a log _importance weight_ and a trace.\n",
    "\n",
    "The trace is consistent with the arguments and constraints passed into the invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b753a9-b3c4-4297-83c5-02e92964bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "[observations[\"ys\", i, \"y\", \"value\"] for i in range(0, len(y))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c345b-b633-44e0-96b2-43b9906ed31e",
   "metadata": {},
   "source": [
    "Let's examine the weight now, and compare it to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8728a-fc74-46c5-b416-8bf42bb35bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(w, tr.get_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db14785-1caa-4887-967c-cac1a79b62e3",
   "metadata": {},
   "source": [
    "Notice that these two quantities are different. \n",
    "\n",
    "Remember: the score is the normalized log density of the choice map measure evaluated at the complete set of trace constraints. We'll refer to complete traces by $\\tau$.\n",
    "\n",
    "The log importance weight `w` is slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be43479-64b6-4721-afe5-3376cb71196c",
   "metadata": {},
   "source": [
    "### Importance sampling, informally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740577c-d87f-489c-a768-da9384db2e4e",
   "metadata": {},
   "source": [
    "Let's discuss how importance sampling works first^[In this notebook, I'll defer discussing formal proofs concerning the asymptotic consistence of posterior estimators derived from importance sampling.].\n",
    "\n",
    "This will provide us with an understanding as to why `w` is different from the `score`. \n",
    "\n",
    "More importantly, we'll understand how we can use `importance` to solve the inference task of approximately sampling from the posterior over coefficients (and ultimately, over curves) from our generative function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9c683-f3cb-4645-873e-86e4088797af",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Importance sampling is typically presented by focusing on posterior expectations $E_{x \\sim P(x | y)}[f(x)]$.\n",
    "\n",
    "In our case, we want to sample $x \\sim P(x | y)$. To do this, we'll actually be considering a different procedure called *sampling importance resampling* or SIR for short.\n",
    "\n",
    "Importantly, importance sampling is a subroutine in SIR. \n",
    "\n",
    "We'll discuss why importance sampling works here, and provide references to why SIR works to solve our problem.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4d0f0-6589-4466-a28c-6d4b2dd270fa",
   "metadata": {},
   "source": [
    "Let's start by considering two distributions which we can sample from, and evaluate densities.\n",
    "\n",
    "Below, I'm plotting the densities of two distributions - a 1D Gaussian mixture and a 1D Gaussian^[GenJAX allows usage of TensorFlow Distributions as generative functions. Here, we're just using the `logpdf` interface from distributions which expose exact `logpdf` evaluation - but `genjax` exports a wrapper which implements the complete generative function interface.]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66494510-a80a-4628-a7a5-6967ad14af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = genjax.mixture(genjax.categorical, [genjax.normal, genjax.normal])\n",
    "mix_args = ([0.5, 0.5], [(-3.0, 0.8), (1.0, 0.3)])\n",
    "d = genjax.normal\n",
    "d_args = (0.0, 1.0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "evaluation_points = np.arange(-5, 5, 0.01)\n",
    "\n",
    "\n",
    "def plot_logpdf(ax, logpdf_fn, evaluation_points, **kwargs):\n",
    "    logpdfs = jax.vmap(logpdf_fn)(evaluation_points)\n",
    "    ax.scatter(evaluation_points, jnp.exp(logpdfs), marker=\".\", **kwargs)\n",
    "\n",
    "\n",
    "def d_logpdf(v):\n",
    "    return d.logpdf(v, *d_args)\n",
    "\n",
    "\n",
    "def mix_logpdf(v):\n",
    "    return mix.logpdf(v, *mix_args)\n",
    "\n",
    "\n",
    "plot_logpdf(ax, d_logpdf, evaluation_points, color=\"red\", label=\"1D Gaussian PDF\")\n",
    "plot_logpdf(\n",
    "    ax,\n",
    "    mix_logpdf,\n",
    "    evaluation_points,\n",
    "    color=\"blue\",\n",
    "    label=\"1D Gaussian mixture PDF\",\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b0f6f-20ac-4de9-899a-3931bc5b296b",
   "metadata": {},
   "source": [
    "To gain context on importance sampling, imagine that the distribution which produces the blue curve is difficult to sample from - but it exposes a `logpdf` interface which we can use to evaluate the density at any point on the support of the distribution.\n",
    "\n",
    "Now, suppose you hand me the distribution which made the red curve - and it is easy to sample from, and it also exposes a `logpdf` interface. \n",
    "\n",
    "One thing we could do is sample from the red curve and then \"correct\" for the fact that we're sampling from the wrong distribution.\n",
    "\n",
    "This is the key intuition behind importance sampling.\n",
    "\n",
    "Now, I'm going to write a procedure and ask you to just go with it ... for a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96389704-6539-457a-9ece-70e3ec8d0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sample(hard, easy):\n",
    "    def _inner(key, hard_args, easy_args):\n",
    "        sample = easy.sample(key, *easy_args)\n",
    "        easy_logpdf = easy.logpdf(sample, *easy_args)\n",
    "        hard_logpdf = hard.logpdf(sample, *hard_args)\n",
    "        importance_weight = hard_logpdf - easy_logpdf\n",
    "        return (importance_weight, sample)\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9f80f-057a-43e3-8082-16b83c3efd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = genjax.mixture(genjax.categorical, [genjax.normal, genjax.normal])\n",
    "easy = genjax.normal\n",
    "jitted = jax.jit(importance_sample(hard, easy))\n",
    "key, sub_key = jax.random.split(key)\n",
    "(importance_weight, sample) = jitted(sub_key, mix_args, d_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a441921-0545-4258-b378-61f6cf4582d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(importance_weight, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0c7f8-f4b9-4405-b8d1-afc029bc5b72",
   "metadata": {},
   "source": [
    "Now, we can easily run this procedure many times in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ddb26-80a2-49a1-881a-e1174706203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted = jax.jit(jax.vmap(importance_sample(hard, easy), in_axes=(0, None, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b845d-2704-4c43-ba04-e7d98cc687cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, *sub_keys = jax.random.split(key, 100 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "(importance_weight, sample) = jitted(sub_keys, mix_args, d_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38987f4-e272-4ea8-8060-6534355e3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1eefd-02aa-459a-a436-72e1b8befd44",
   "metadata": {},
   "source": [
    "We're just sampling from `easy`, then scoring the samples with `importance_weight` according to the log ratio `easy_logpdf(sample) - hard_logpdf(sample)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7eb8b-d80d-4b70-8c8a-d59c4779ea39",
   "metadata": {},
   "source": [
    "Here's the trick - from our collection of samples and weights, let's normalize the weights into a distribution and sample a single sample to return using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e5228-1026-46e5-8530-f0c58c06a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_importance_resampling(hard, easy, n_samples):\n",
    "    def _inner(key, hard_args, easy_args):\n",
    "        fn = importance_sample(hard, easy)\n",
    "        resample_key, _sub_key = jax.random.split(key)\n",
    "        sub_keys = jax.random.split(key, n_samples)\n",
    "        vmapped = jax.vmap(fn, in_axes=(0, None, None))\n",
    "        (ws, samples) = vmapped(sub_keys, hard_args, easy_args)\n",
    "        logits = ws\n",
    "        index = genjax.categorical.sample(resample_key, logits)\n",
    "        final_sample = samples[index]\n",
    "        return final_sample\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412e2ff-2e69-4c37-8794-f46747fc3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = genjax.mixture(genjax.categorical, [genjax.normal, genjax.normal])\n",
    "easy = genjax.normal\n",
    "jitted = jax.jit(sampling_importance_resampling(hard, easy, 100))\n",
    "key, sub_key = jax.random.split(key)\n",
    "sample = jitted(sub_key, mix_args, d_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a1cbe-fa77-44a8-bccf-bbd323362907",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9e01a-7897-4e12-910b-16c4750c7840",
   "metadata": {},
   "source": [
    "Let's run this procedure a bunch of times and plot the points on the x-axis of our plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b520e12-be8b-4883-88c9-8b524e004c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_on_x(ax, x, **kwargs):\n",
    "    ax.scatter(x, np.zeros_like(x), **kwargs)\n",
    "\n",
    "\n",
    "key, *sub_keys = jax.random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "fn = sampling_importance_resampling(hard, easy, 1000)\n",
    "jitted = jax.jit(jax.vmap(fn, in_axes=(0, None, None)))\n",
    "samples = jitted(sub_keys, mix_args, d_args)\n",
    "plot_on_x(ax, samples, color=\"gold\", marker=\".\", alpha=0.05)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f04b4-f038-4d28-ad53-8722e5d5a2b3",
   "metadata": {},
   "source": [
    "Notice what happens with the SIR samples (in gold)?\n",
    "\n",
    "They accumulate around the places you'd expect to see if you were sampling from the `hard` distribution!\n",
    "\n",
    "That's what importance sampling and sampling importance sampling give us - we provide a \"hard\" distribution with a `logpdf` interface, and another \"easy\" distribution with `sample` and `logpdf` interface^[There are more constraints. The second distribution must be _absolutely continuous_ in measure with respect to the first. Let's defer this discussion to a formal treatment of importance sampling.], and SIR returns an exact sampler for a distribution which approximates the hard distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e158d-a731-4d54-8849-fe076a4410f4",
   "metadata": {},
   "source": [
    "### Back to our generative function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a46940-2f11-4bd8-b94c-b3b2ddcfc623",
   "metadata": {},
   "source": [
    "Now that we've seen the ingredients and implementation of importance sampling and sampling importance resampling - let's return to our original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4ee3a-c134-4021-9f88-dd0b7151c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6543cdf-9201-4891-848c-037c07199b28",
   "metadata": {},
   "source": [
    "If you studied the previous section careful - one question might jump out at you: what is the \"easy\" distribution for `model.importance`?\n",
    "\n",
    "#### Builtin proposals\n",
    "\n",
    "Generative functions defined using the `BuiltinGenerativeFunction` language come with builtin proposals - it's a distribution (which we'll refer to as $Q$) induced from the prior, with sampling and `score` defined ancestrally.\n",
    "\n",
    "Give observation constraints $u$, the importance weight which `model.importance` computes is^[This definition again considers \"untraced randomness\" $r$. If you wish to ignore this in the math, just remove the $Q(r; x, \\tau)$ term. Even in the presence of untraced randomness, the weights which Gen computes are asymptotically consistent in expectation over $Q(r; x, \\tau)$]:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log w &= \\log P(\\tau, r; x) - \\log Q(\\tau; u, x)Q(r; x, \\tau) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the `BuiltinGenerativeFunction` language, we implement $Q$ by invoking the generative function - when we arrive at a constrained address, we recursively called `submodel.importance` - accumulate the log weight, as well as the log score.\n",
    "\n",
    "Now, if an address has no constraints - we get `0.0` for the weight (think about why this is by looking at the above equation and asking what happens when $Q$ has to generate a full $\\tau$). However, we still get a score.\n",
    "\n",
    "#### Sequential importance resampling in GenJAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af0ba8-21cf-4504-9a55-e66b04c727fe",
   "metadata": {},
   "source": [
    "Here's SIR using builtin proposals (just a single call to `model.importance`) in GenJAX^[To implement a variant with custom proposals, all we need to do is first `proposal.simulate`, merge the proposal choice map with the constraints, then `model.importance` followed by a final weight adjustment `w = w - proposal_tr.get_score()` - easy peasy.]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3de7de-dd56-48e1-a16e-f7a3acec188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_importance_resampling(model, n_samples):\n",
    "    def _inner(key, observations, model_args):\n",
    "        resample_key, sub_key = jax.random.split(key)\n",
    "        sub_keys = jax.random.split(sub_key, n_samples)\n",
    "        vmapped = jax.vmap(model.importance, in_axes=(0, None, None))\n",
    "        (trs, lws) = vmapped(sub_keys, observations, model_args)\n",
    "        index = genjax.categorical.sample(resample_key, lws)\n",
    "        final_tr = jtu.tree_map(lambda v: v[index], trs)\n",
    "        return final_tr\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9e49c-4d45-4d02-a0f5-67b08e12cf5b",
   "metadata": {},
   "source": [
    "One difference between our first implementation (on just distributions) above and this one is that `Trace` instances are structured objects (but all of them are `Pytree` implementors) - meaning we need to index into the leaves when we wish to return a single sampled trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b52e2-4274-4780-a519-333022208b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = (x,)\n",
    "jitted = jax.jit(\n",
    "    jax.vmap(sampling_importance_resampling(model, 100), in_axes=(0, None, None))\n",
    ")\n",
    "key, *sub_keys = jax.random.split(key, 100 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "samples = jitted(sub_keys, observations, model_args)\n",
    "coefficients = samples[\"alpha\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42d08a-2323-433b-9120-b29e477a6001",
   "metadata": {},
   "source": [
    "So now we have an approximate sampler for the posterior and we can use it to look at properties of the posterior - like what sort of curves are likely given our data and our model prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6ef26-4241-412c-a266-f470bb0e0292",
   "metadata": {},
   "source": [
    "... and by the way, to get a representative set of samples from the posterior for this model, on an Apple M2 device - only takes about 0.05 seconds^[Just remember: we're running this notebook on CPU - but the resulting specialized inference code can easily be moved to accelerators, courtesy of the fact that all our code is JAX traceable.]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163a4af-1654-41e7-bf5e-5fda15546836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "samples = jitted(sub_keys, observations, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db2520-16f9-41e6-963b-f1d84b203ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_at_x(x, coefficients):\n",
    "    basis_values = jnp.array([1.0, x, x**2])\n",
    "    polynomial_value = jnp.sum(coefficients * basis_values)\n",
    "    return polynomial_value\n",
    "\n",
    "\n",
    "jitted = jax.jit(jax.vmap(polynomial_at_x, in_axes=(None, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b44c82-a457-4049-b3fe-3eb9bba0518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_values(ax, x, coefficients, **kwargs):\n",
    "    v = jitted(x, coefficients)\n",
    "    ax.scatter(np.repeat(x, len(v)), v, **kwargs)\n",
    "\n",
    "\n",
    "coefficients = samples[\"alpha\"]\n",
    "evaluation_points = np.arange(0, 5, 0.01)\n",
    "for data in evaluation_points:\n",
    "    plot_polynomial_values(\n",
    "        ax_data, data, coefficients, marker=\".\", color=\"gold\", alpha=0.01\n",
    "    )\n",
    "fig_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562597b9-1adb-4173-a173-1fd14bd39d55",
   "metadata": {},
   "source": [
    "Intuitively, this makes a lot of sense. Our prior over polynomials considers a wide range of curves - but, if our approximate sampling process is trusted, we're correctly seeing what we should expect to happen if we observed this data - polynomials with the coefficients shown above tend to be sampled more under the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354bd27-a299-4da1-b396-1c475cc321e8",
   "metadata": {},
   "source": [
    "We can also ask for an estimate of the posterior probability that any particle point was an outlier. \n",
    "\n",
    "For example, below is the set of samples projected onto the `(\"ys\", \"outlier\")` address for the point which we manually set to be quite far from the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03311e-58cb-409b-a783-9ca1527a1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chs = jax.vmap(lambda idx: samples.get_choices()[\"ys\", idx, \"outlier\"])(jnp.arange(100))\n",
    "outlier_at_2 = chs[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86e829-51cc-491b-8197-2fd2f3b30be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(outlier_at_2) / len(outlier_at_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac4fdd-8245-4b92-9fc1-2c0a92da7e0c",
   "metadata": {},
   "source": [
    "That seems to make sense! We pulled that point quite far away from ground truth curve - so we'd expect that point 2 is considered an outlier under the true posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da558133-8cf2-43c2-8b01-f443a6b4afb5",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86832b13-a156-4a20-8fc4-fa6b476280c4",
   "metadata": {},
   "source": [
    "We've covered a lot of ground in this notebook. Please reflect, re-read, and post issues!\n",
    "\n",
    "* We discussed the Gen probabilistic programming framework, and discussed GenJAX - an implementation of Gen on top of JAX. \n",
    "* We discussed _generative functions_ - the main computational object of Gen.\n",
    "* We discussed how to create generative functions using _generative function languages_, and several of GenJAX's builtin capabilities for constructing generative functions.\n",
    "* We discussed how to use generative functions to represent joint probability distributions, which can be used to construct models of phenomena.\n",
    "* We created a generative function to model a data-generating process based on sampling and evaluating random polynomials at input data - to represent a typical regression task.\n",
    "* We discussed how to formulate questions about induced conditional distributions under a probabilistic model as a Bayesian inference problem.\n",
    "* We discussed importance sampling and sampling importance resampling, two central techniques in approximate Bayesian inference.\n",
    "* We created a sampling importance resampling routine and applied it to produce approximate posterior samples from the posterior in the our polynomial generating model.\n",
    "* We investigated the approximate posterior samples, and visually inspected that they match the inferences that we might draw - both for the polynomials we expected to produce the data, as well as what data points might be outliers.\n",
    "\n",
    "This is just the beginning! There's a lot more to learn, but plenty to bite off with this initial notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
